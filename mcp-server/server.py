from fastmcp import FastMCP
import asyncio
from playwright.async_api import async_playwright
import base64
import json
from typing import Dict, List, Any, Optional
import httpx
import logging
from urllib.parse import urljoin, urlparse
import re

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ë¶ˆí•„ìš”í•œ ë””ë²„ê·¸ ë¡œê·¸ ìˆ¨ê¸°ê¸°
logging.getLogger("mcp.server").setLevel(logging.INFO)
logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
logging.getLogger("sse_starlette").setLevel(logging.WARNING)
logging.getLogger("mcp.server.lowlevel").setLevel(logging.WARNING)

mcp = FastMCP(name="CrawlerMindServer")

# Health check endpoint (MCP tool)
@mcp.tool
def health_check() -> Dict[str, Any]:
    """
    ëŸ°íƒ€ì„ ì˜ì¡´ì„±ì„ ì‹¤ì œ ì ê²€í•˜ëŠ” í—¬ìŠ¤ì²´í¬
    - crawl4ai ì„í¬íŠ¸ ê°€ëŠ¥ ì—¬ë¶€
    - Playwright ë¸Œë¼ìš°ì € ê¸°ë™ ê°€ëŠ¥ ì—¬ë¶€(ê°„ë‹¨ ì²´í¬)
    """
    logger.info("[MCP] health_check called")
    crawl4ai_ok = False
    playwright_ok = False

    # 1) crawl4ai import í™•ì¸
    try:
        import importlib
        importlib.import_module("crawl4ai")
        crawl4ai_ok = True
    except Exception as e:
        logger.warning(f"crawl4ai import failed: {e}")

    # 2) Playwright import í™•ì¸ (ì´ë²¤íŠ¸ ë£¨í”„ ì¤‘ì²© ë¬¸ì œ ë°©ì§€ ìœ„í•´ ëŸ°íƒ€ì„ ê¸°ë™ì€ ìƒëµ)
    try:
        import importlib
        importlib.import_module("playwright.async_api")
        playwright_ok = True
    except Exception as e:
        logger.warning(f"playwright import failed: {e}")

    status = "healthy" if (crawl4ai_ok and playwright_ok) else "degraded" if (crawl4ai_ok or playwright_ok) else "unhealthy"
    return {
        "success": crawl4ai_ok and playwright_ok,
        "status": status,
        "service": "mcp-server",
        "dependencies": {
            "crawl4ai": crawl4ai_ok,
            "playwright": playwright_ok,
        }
    }


async def _crawl_with_playwright(url: str) -> Dict[str, Any]:
    """
    Playwrightë¥¼ ì‚¬ìš©í•œ í´ë°± í¬ë¡¤ë§ í•¨ìˆ˜
    """
    logger.info(f"[MCP] Playwright í´ë°±ìœ¼ë¡œ {url} í¬ë¡¤ë§ ì‹œì‘")
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            try:
                page = await browser.new_page()
                await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = await page.title()
                html_content = await page.content()
                
                # markdownifyë¥¼ ì‚¬ìš©í•œ ë§ˆí¬ë‹¤ìš´ ë³€í™˜
                markdown_text = ""
                try:
                    from bs4 import BeautifulSoup
                    from markdownify import markdownify as md
                    soup = BeautifulSoup(html_content, 'html.parser')
                    
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    for sel in [
                        "#cfmClHeader", "#cfmClFooter", "#cfmClSkip", ".header", ".footer",
                        ".navigation", ".sidebar", ".banner", ".popup", ".overlay", ".sns-area",
                    ]:
                        for el in soup.select(sel):
                            el.decompose()
                    for t in soup(["script", "style", "noscript"]):
                        t.decompose()
                    
                    cleaned_html = str(soup)
                    markdown_text = md(cleaned_html, heading_style="ATX")
                except Exception as me:
                    logger.warning(f"Playwright markdown ë³€í™˜ ì‹¤íŒ¨: {me}")
                
                logger.info(f"[MCP] Playwright í¬ë¡¤ë§ ì™„ë£Œ: html={len(html_content)} chars, markdown={len(markdown_text)} chars")
                return {
                    "success": True,
                    "url": url,
                    "title": title,
                    "html_content": html_content,
                    "markdown": markdown_text,
                    "status_code": 200,
                }
            finally:
                await browser.close()
                
    except Exception as e:
        logger.error(f"Playwright í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "url": url,
            "error": f"Playwright í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}"
        }

# ============================================================================
# RAG CRAWLING TOOLS (ì¼ë°˜ ì›¹í˜ì´ì§€ í¬ë¡¤ë§ ë° ì •ì œ)
# ============================================================================

@mcp.tool
async def crawl4ai_scrape(url: str, include_selector: Optional[str] = None) -> Dict[str, Any]:
    """
    RAGìš© ì›¹ í¬ë¡¤ë§: ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±° ë° ë§ˆí¬ë‹¤ìš´ ë³€í™˜
    - í—¤ë”/í‘¸í„°/ë„¤ë¹„ê²Œì´ì…˜ ë“± ì œê±°í•˜ì—¬ ë³¸ë¬¸ë§Œ ì¶”ì¶œ
    - markdownifyë¡œ ê¹”ë”í•œ í…ìŠ¤íŠ¸ ë³€í™˜
    - ì„±ê³µ ì‹œ: { success, url, title, html_content, markdown, status_code }
    - ì‹¤íŒ¨ ì‹œ: { success: False, url, error }
    """
    logger.info(f"[MCP] crawl4ai_scrape called for URL: {url}")
    try:
        try:
            from crawl4ai import AsyncWebCrawler
            from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig, CacheMode
        except Exception as e:
            logger.warning(f"crawl4ai ë¯¸ì„¤ì¹˜ ë˜ëŠ” ì„í¬íŠ¸ ì‹¤íŒ¨: {e} â€” Playwrightë¡œ í´ë°±í•©ë‹ˆë‹¤")
            return await _crawl_with_playwright(url)

        browser_config = BrowserConfig(
            headless=True,
            verbose=False,
            browser_type="chromium",
            ignore_https_errors=True,
            java_script_enabled=True,
        )

        crawler = AsyncWebCrawler(config=browser_config)
        await crawler.start()
        try:
            is_js_heavy = any(d in url.lower() for d in [
                "google.com", "youtube.com", "facebook.com", "instagram.com", "twitter.com",
            ])
            run_config = CrawlerRunConfig(
                verbose=False,
                word_count_threshold=10,
                exclude_external_links=True,
                remove_overlay_elements=False,
                process_iframes=True,
                ignore_body_visibility=True,
                js_only=False,
                cache_mode=CacheMode.BYPASS,
                excluded_tags=['form', 'header', 'footer', 'nav'],
                excluded_selector="#cfmClHeader, #cfmClFooter, #cfmClSkip, .location, .sns-area",
                wait_until="networkidle" if is_js_heavy else "domcontentloaded",
                delay_before_return_html=12 if is_js_heavy else 6,
                simulate_user=is_js_heavy,
                override_navigator=is_js_heavy,
                page_timeout=120000,
            )

            result = await crawler.arun(url=url, config=run_config)
            if not result.success:
                logger.error(f"[MCP] crawl4ai ì‹¤íŒ¨: {result.error_message}")
                # í´ë°±: Playwright ì‹œë„
                try:
                    return await _crawl_with_playwright(url)
                except Exception:
                    return {
                        "success": False,
                        "url": url,
                        "error": result.error_message or "crawl4ai ì‹¤íŒ¨",
                    }

            html_content = result.html or ""
            status_code = getattr(result, 'status_code', None)
            title = None
            meta = getattr(result, 'metadata', None)
            if isinstance(meta, list):
                meta = meta[0] if meta else None
            if meta is not None:
                title = getattr(meta, 'title', None)

            markdown_text = ""
            try:
                from bs4 import BeautifulSoup
                from markdownify import markdownify as md
                soup = BeautifulSoup(html_content, 'html.parser')
                # include_selectorê°€ ì£¼ì–´ì§€ë©´ í•´ë‹¹ ì˜ì—­ë§Œ ë³€í™˜ ëŒ€ìƒìœ¼ë¡œ ì œí•œ
                if include_selector:
                    sel = include_selector if include_selector.startswith(('#', '.', '[', ':')) else f"#{include_selector}"
                    selected = soup.select_one(sel)
                    if selected:
                        soup = BeautifulSoup(str(selected), 'html.parser')
                for sel in [
                    "#cfmClHeader", "#cfmClFooter", "#cfmClSkip", ".header", ".footer",
                    ".navigation", ".sidebar", ".banner", ".popup", ".overlay", ".sns-area",
                ]:
                    for el in soup.select(sel):
                        el.decompose()
                for t in soup(["script", "style", "noscript"]):
                    t.decompose()
                cleaned_html = str(soup)
                markdown_text = md(cleaned_html, heading_style="ATX")
            except Exception as me:
                logger.warning(f"markdown ë³€í™˜ ì‹¤íŒ¨(ë¬´ì‹œ): {me}")

            payload = {
                "success": True,
                "url": url,
                "title": title,
                "html_content": html_content,
                "markdown": markdown_text,
                "status_code": status_code,
            }
            logger.info(f"[MCP] crawl4ai_scrape completed: html={len(html_content)} chars, markdown={len(markdown_text)} chars, title='{title}'")
            return payload
        finally:
            try:
                await crawler.close()
            except Exception:
                pass
    except Exception as e:
        logger.error(f"crawl4ai_scrape ì‹¤íŒ¨ {url}: {str(e)}")
        return {
            "success": False,
            "url": url,
            "error": str(e)
        }

@mcp.tool
async def crawl_urls_sequential(urls: List[str], selector: Optional[str] = None) -> Dict[str, Any]:
    """
    ì—¬ëŸ¬ URLì„ ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    rag-scrapingì˜ scrape_urls_sequential ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    logger.info(f"[MCP] crawl_urls_sequential called with {len(urls)} URLs")
    results = []
    
    for i, url in enumerate(urls):
        try:
            logger.info(f"[MCP] í¬ë¡¤ë§ ì§„í–‰: {i+1}/{len(urls)} - {url}")
            result = await crawl4ai_scrape(url, selector)
            results.append(result)
        except Exception as e:
            logger.error(f"[MCP] URL í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {e}")
            results.append({
                "success": False,
                "url": url,
                "error": str(e)
            })
    
    return {
        "success": True,
        "total_urls": len(urls),
        "results": results,
        "successful_count": len([r for r in results if r.get("success", False)]),
        "failed_count": len([r for r in results if not r.get("success", False)])
    }

@mcp.tool
def extract_headings_from_html(
    html_content: str,
    heading_tags: Optional[List[str]] = None
) -> Dict[str, Any]:
    """HTMLì—ì„œ heading íƒœê·¸(H1~H6 ë“±)ë¥¼ ì¶”ì¶œ"""
    try:
        from bs4 import BeautifulSoup

        soup = BeautifulSoup(html_content or "", "html.parser")
        tags = heading_tags or ["h1", "h2", "h3", "h4", "h5", "h6"]
        headings: List[str] = []
        for tag in tags:
            for element in soup.find_all(tag):
                text = element.get_text(strip=True)
                if text:
                    headings.append(text)
        return {
            "success": True,
            "headings": headings,
            "count": len(headings),
        }
    except Exception as exc:
        logger.error(f"extract_headings_from_html ì‹¤íŒ¨: {exc}")
        return {"success": False, "error": str(exc)}


@mcp.tool
async def extract_image_metadata(
    html_content: str,
    base_url: Optional[str] = None,
    min_alt_length: int = 2
) -> Dict[str, Any]:
    """HTMLì—ì„œ ì´ë¯¸ì§€ ì •ë³´(src, alt ë“±)ë¥¼ ì¶”ì¶œ"""
    try:
        from bs4 import BeautifulSoup

        soup = BeautifulSoup(html_content or "", "html.parser")
        images: List[Dict[str, str]] = []
        for element in soup.find_all("img"):
            parent = element.find_parent(id=["cfmClHeader", "cfmClFooter"])
            if parent:
                continue

            alt_text = (element.get("alt") or "").strip()
            if len(alt_text) < min_alt_length:
                continue

            src = element.get("src") or ""
            if base_url and src and not src.startswith("http"):
                src = urljoin(base_url, src)

            images.append({"alt": alt_text, "src": src})

        return {
            "success": True,
            "images": images,
            "count": len(images),
        }
    except Exception as exc:
        logger.error(f"extract_image_metadata ì‹¤íŒ¨: {exc}")
        return {"success": False, "error": str(exc)}


@mcp.tool
async def extract_links(
    html_content: str,
    base_url: Optional[str] = None,
    min_text_length: int = 2
) -> Dict[str, Any]:
    """HTMLì—ì„œ ì˜ë¯¸ ìˆëŠ” ì•µì»¤ ë§í¬ë¥¼ ì¶”ì¶œ"""
    try:
        from bs4 import BeautifulSoup

        soup = BeautifulSoup(html_content or "", "html.parser")
        links: List[Dict[str, str]] = []
        for element in soup.find_all("a", href=True):
            parent = element.find_parent(id=["cfmClHeader", "cfmClFooter"])
            if parent:
                continue

            text = element.get_text(strip=True)
            if len(text) < min_text_length:
                continue

            href = element.get("href")
            if not href:
                continue

            if base_url and href.startswith("/"):
                href = urljoin(base_url, href)

            if href.startswith(("http://", "https://")):
                links.append({"text": text, "url": href})

        return {
            "success": True,
            "links": links,
            "count": len(links),
        }
    except Exception as exc:
        logger.error(f"extract_links ì‹¤íŒ¨: {exc}")
        return {"success": False, "error": str(exc)}


@mcp.tool
def extract_meta_title(html_content: str) -> Dict[str, Any]:
    """HTMLì˜ meta og:title ë˜ëŠ” title íƒœê·¸ë¥¼ ì¶”ì¶œ"""
    try:
        title = extract_meta_title_from_html(html_content)
        return {"success": bool(title), "title": title or ""}
    except Exception as exc:
        logger.error(f"extract_meta_title ì‹¤íŒ¨: {exc}")
        return {"success": False, "error": str(exc)}


@mcp.tool
def convert_to_json_format(
    url: str,
    title: Optional[str],
    markdown_content: str,
    html_content: str,
    hierarchy: Optional[List[str]] = None,
    murl: Optional[str] = None,
    startdate: str = "1900-01-01",
    enddate: str = "2999-12-31"
) -> Dict[str, Any]:
    """
    RAGìš© JSON í¬ë§· ë³€í™˜: í¬ë¡¤ë§ ê²°ê³¼ë¥¼ rag-scrapingì˜ JSON í¬ë§·ìœ¼ë¡œ ë³€í™˜
    - í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    - ì´ë¯¸ì§€/ë§í¬ ì •ë³´ í¬í•¨
    - to_json.pyì˜ ê¸°ëŠ¥ì„ ì œê³µ
    """
    logger.info(f"[MCP] convert_to_json_format called for URL: {url}")
    try:
        import unicodedata

        title = unicodedata.normalize('NFC', (title or "ì œëª© ì—†ìŒ"))
        url = unicodedata.normalize('NFC', url)

        final_content = (markdown_content or "").strip().replace("\n", "\\n")
        final_content = unicodedata.normalize('NFC', final_content)

        normalized_hierarchy: Optional[List[str]] = None
        if hierarchy:
            normalized_hierarchy = [unicodedata.normalize('NFC', item) for item in hierarchy if item]

        metadata: Dict[str, Any] = {}
        try:
            from bs4 import BeautifulSoup

            soup = BeautifulSoup(html_content or "", 'html.parser')

            images = []
            for img in soup.find_all('img'):
                if img.find_parent(id=['cfmClHeader', 'cfmClFooter']):
                    continue
                alt_text = (img.get('alt') or '').strip()
                if len(alt_text) > 2:
                    images.append({'alt': alt_text, 'src': img.get('src', '')})
            if images:
                metadata['images'] = images

            urls_data = []
            for link in soup.find_all('a', href=True):
                if link.find_parent(id=['cfmClHeader', 'cfmClFooter']):
                    continue
                link_text = link.get_text().strip()
                if len(link_text) < 2:
                    continue
                href = link.get('href')
                if href.startswith('http') or href.startswith('/'):
                    urls_data.append({'desc': link_text, 'url': href})
            if urls_data:
                metadata['urls'] = _deduplicate_by_key(urls_data, 'url')
        except Exception as e:
            logger.warning(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        json_data = {
            "url": url,
            "murl": murl or "",
            "hierarchy": normalized_hierarchy or [],
            "title": title,
            "text": final_content,
            "startdate": startdate,
            "enddate": enddate,
            "metadata": metadata,
        }
        if normalized_hierarchy:
            json_data["hierarchy"] = normalized_hierarchy

        return {
            "success": True,
            "json_data": json_data,
            "text_length": len(final_content),
            "metadata_count": len(metadata),
        }
    except Exception as e:
        logger.error(f"JSON í¬ë§· ë³€í™˜ ì‹¤íŒ¨: {e}")
        return {
            "success": False,
            "error": str(e)
        }


def _deduplicate_by_key(items: List[Dict[str, Any]], key: str) -> List[Dict[str, Any]]:
    seen: set[str] = set()
    unique_items: List[Dict[str, Any]] = []
    for item in items:
        value = item.get(key)
        if value and value not in seen:
            seen.add(value)
            unique_items.append(item)
    return unique_items

def extract_meta_title_from_html(html_content: str) -> Optional[str]:
    from bs4 import BeautifulSoup

    soup = BeautifulSoup(html_content or "", "html.parser")
    prop = soup.find("meta", attrs={"property": "og:title"})
    if prop and prop.get("content"):
        content = prop.get("content").strip()
        if content:
            return content
    name_meta = soup.find("meta", attrs={"name": "title"})
    if name_meta and name_meta.get("content"):
        content = name_meta.get("content").strip()
        if content:
            return content
    head_title = soup.find("title")
    if head_title:
        text = head_title.get_text(strip=True)
        if text:
            return text
    return None

# ============================================================================
# MENU SEARCH TOOLS (ë©”ë‰´ ê²€ìƒ‰ ë° ì¡°íšŒ)
# ============================================================================

@mcp.tool
async def menu_search(user_query: str, page: int = 1, size: int = 50, with_managers: bool = True) -> Dict[str, Any]:
    """
    ì‚¬ìš©ì ì§ˆì˜ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ mcp-clientì˜ ë©”ë‰´ APIë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
    - í‚¤ì›Œë“œ: í•œê¸€/ì˜ë¬¸/ìˆ«ì 2ì ì´ìƒ í† í°ë§Œ ì‚¬ìš©í•˜ì—¬ search íŒŒë¼ë¯¸í„° êµ¬ì„±
    - with_managers=Trueì´ë©´ /menu-links/with-managersë„ í•¨ê»˜ í˜¸ì¶œ
    """
    try:
        tokens = re.findall(r"[A-Za-z0-9ê°€-í£]{2,}", user_query)
        search = " ".join(tokens[:5]) if tokens else ""

        base_url = "http://127.0.0.1:8000"
        async with httpx.AsyncClient(timeout=30) as client:
            params = {"page": page, "size": size}
            if search:
                params["search"] = search
            resp_links = await client.get(f"{base_url}/menu-links", params=params)
            resp_links.raise_for_status()
            menu_links = resp_links.json()

            data = {"menu_links": menu_links, "search": search}
            if with_managers:
                resp_with = await client.get(f"{base_url}/menu-links/with-managers", params=params)
                resp_with.raise_for_status()
                data["with_managers"] = resp_with.json()

            return {"success": True, "query": user_query, "data": data}
    except Exception as e:
        logger.error(f"menu_search failed: {e}")
        return {"success": False, "error": str(e)}

# ============================================================================
# ARI CONTENT PROCESSING TOOLS (HTML êµ¬ì¡°í™” ë° ì „ìš© íŒŒì‹±)
# ============================================================================

@mcp.tool  
def ari_parse_html(html_content: str) -> Dict[str, Any]:
    """
    ARI ì „ìš© HTML íŒŒì‹±: ìˆœìˆ˜ HTML íŒŒì‹± ë° êµ¬ì¡°í™”ëœ JSON ë°˜í™˜
    - í•„í„°ë§ ì—†ì´ ëª¨ë“  í…ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€ ì¶”ì¶œ
    - ARI ëª¨ë¸ ìŠ¤í‚¤ë§ˆì— ë§ì¶˜ êµ¬ì¡°í™”ëœ ê²°ê³¼ ë°˜í™˜
    - RAG í¬ë¡¤ë§ê³¼ëŠ” ë‹¤ë¥¸ ëª©ì ì˜ ì „ìš© íŒŒì„œ
    """
    try:
        from bs4 import BeautifulSoup
        from datetime import datetime
        
        soup = BeautifulSoup(html_content or "", 'html.parser')
        title_el = soup.find('title')
        title_text = title_el.get_text(strip=True) if title_el else ""
        text = soup.get_text(separator=' ', strip=True)

        images = []
        for img in soup.find_all('img', src=True):
            images.append({'alt': (img.get('alt') or '').strip(), 'src': img['src']})

        return {
            'success': True,
            'result': {
                'content': {'text': text},
                'metadata': {
                    'title': title_text,
                    'extracted_at': datetime.now().isoformat(),
                    'content_length': len(text),
                    'images': images
                }
            }
        }
    except Exception as e:
        logger.error(f"ARI íŒŒì‹± ì‹¤íŒ¨: {e}")
        return {'success': False, 'error': str(e)}

async def main():
    # Start MCP server
    logger.info("ğŸš€ Starting MCP Server on 0.0.0.0:4200")
    await mcp.run_async(
        transport="http",
        host="0.0.0.0",
        port=4200,
        path="/my-custom-path",
        log_level="debug",
    )

if __name__ == "__main__":
    asyncio.run(main())