"""LLM Service - Handles OpenAI API interactions"""
from typing import List, Dict, Any, AsyncGenerator
import json
import logging
import tiktoken
from openai import AsyncOpenAI
from app.config import settings
from app.infrastructure.mcp.mcp_service import mcp_service
from app.shared.exceptions.base import LLMQueryError

logger = logging.getLogger(__name__)

class LLMService:
    """Service class for managing LLM interactions"""
    
    def __init__(self):
        self._client = AsyncOpenAI(api_key=settings.openai_api_key)
        try:
            self.tokenizer = tiktoken.encoding_for_model("gpt-4o")
        except Exception:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
    
    def _format_tools_for_openai(self, available_tools: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Convert MCP tools format to OpenAI tools format
        
        MCP format might be: {"name": "tool_name", "description": "...", "parameters": {...}}
        OpenAI expects: {"type": "function", "function": {"name": "...", "description": "...", "parameters": {...}}}
        """
        formatted_tools = []
        for tool in available_tools:
            # Check if already in OpenAI format
            if "type" in tool and tool["type"] == "function" and "function" in tool:
                formatted_tools.append(tool)
            else:
                # Convert from MCP format to OpenAI format
                formatted_tool = {
                    "type": "function",
                    "function": {
                        "name": tool.get("name", ""),
                        "description": tool.get("description", ""),
                        "parameters": tool.get("parameters", {
                            "type": "object",
                            "properties": {},
                            "required": []
                        })
                    }
                }
                formatted_tools.append(formatted_tool)
        return formatted_tools
    
    # (ì‚­ì œë¨) ìƒë‹¨ ì¤‘ë³µ ì •ì˜ëœ _filter_tools_by_intent â€” í´ë˜ìŠ¤ì˜ í•˜ë‹¨ ì •ì˜ë§Œ ì‚¬ìš©
    
    async def query(self, question: str, available_tools: List[Dict[str, Any]]) -> str:
        """Execute a query against the LLM with available tools"""
        try:
            # 1ë‹¨ê³„: ì˜ë„ ë¶„ë¥˜ ë° ë„êµ¬ í•„í„°ë§ (í† í° ì œí•œ í•´ê²°)
            filtered_tools = self._filter_tools_by_intent(question, available_tools)
            
            # Format tools for OpenAI
            formatted_tools = self._format_tools_for_openai(filtered_tools)
            
            # Log for debugging
            logger.info(f"ğŸ” ì˜ë„ ë¶„ë¥˜ ì™„ë£Œ: {len(filtered_tools)}ê°œ ë„êµ¬ ì„ íƒ (ì „ì²´ {len(available_tools)}ê°œ ì¤‘)")
            logger.debug(f"Formatted tools: {json.dumps(formatted_tools[:1], indent=2)}")  # Log first tool as example
            
            tool_catalog = self._get_tool_categories_description(formatted_tools)
            system_prompt = f"""You are an AI assistant specialized in web analysis and data processing.

CRITICAL: Always use tools when available. Execute the appropriate tools to perform real work; avoid generic explanations without using tools.

Available tools (dynamic):
{tool_catalog}

Tool usage principles:
1) If a URL is present â†’ use crawl4ai_scrape or crawl_urls_sequential
2) For system status â†’ use health_check
3) For text summarization/clean-up â†’ use summarize_content
4) For menu/DB queries â†’ use menu_search
5) For HTML/Confluence content â†’ use ari_extract_main_blocks and/or ari_markdown_to_json and/or convert_to_json_format

Response policy:
- After using tools, produce a concise, high-signal answer based on the results.
- IMPORTANT: Write all final answers to the user in Korean.
"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ]
            
            # First API call with tools
            response = await self._client.chat.completions.create(
                model=settings.openai_model,
                messages=messages,
                tools=formatted_tools if formatted_tools else None,  # Only pass tools if available
                tool_choice="auto" if formatted_tools else None
            )
            
            # Check if tools were called
            message = response.choices[0].message
            
            if not message.tool_calls:
                return message.content or ""
            
            # Process tool calls
            messages.append(message.model_dump())  # Add assistant's message with tool calls
            
            # Execute each tool call
            for tool_call in message.tool_calls:
                function_name = tool_call.function.name
                function_args = json.loads(tool_call.function.arguments)
                
                logger.info(f"Executing tool: {function_name} with args: {function_args}")
                
                try:
                    # Call your MCP service
                    tool_result = await mcp_service.call_tool(function_name, function_args)
                    
                    # Format the result
                    if hasattr(tool_result, 'structured_content'):
                        result_content = json.dumps(tool_result.structured_content, ensure_ascii=False)
                    elif hasattr(tool_result, 'data'):
                        result_content = json.dumps(tool_result.data, ensure_ascii=False)
                    elif isinstance(tool_result, dict):
                        result_content = json.dumps(tool_result, ensure_ascii=False)
                    else:
                        result_content = str(tool_result)
                    
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": result_content
                    })
                except Exception as e:
                    logger.error(f"Tool execution failed for {function_name}: {e}")
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": json.dumps({"error": str(e)}, ensure_ascii=False)
                    })
            
            # Second API call for final response
            final_response = await self._client.chat.completions.create(
                model=settings.openai_model,
                messages=messages
            )
            
            return final_response.choices[0].message.content or ""
            
        except Exception as e:
            logger.error(f"LLM query failed: {e}")
            raise LLMQueryError(f"LLM ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def _filter_tools_by_intent(self, question: str, available_tools: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ê´€ë ¨ëœ ë„êµ¬ë§Œ í•„í„°ë§
        """
        question_lower = question.lower()
        
        # ì˜ë„ë³„ í‚¤ì›Œë“œ ë§¤í•‘
        intent_keywords = {
            'web_crawling': ['í¬ë¡¤ë§', 'crawl', 'url', 'ì›¹ì‚¬ì´íŠ¸', 'ì›¹í˜ì´ì§€', 'ìŠ¤í¬ë˜í•‘', 'scrape'],
            'system_status': ['ìƒíƒœ', 'status', 'health', 'ì„œë²„', 'í†µê³„', 'statistics'],
            'text_analysis': ['ìš”ì•½', 'summarize', 'ë¶„ì„', 'analyze', 'ì¶”ì¶œ', 'extract', 'í…ìŠ¤íŠ¸'],
            'html_processing': ['html', 'íŒŒì¼', 'file', 'confluence', 'ì—…ë¡œë“œ', 'upload', 'ari'],
            'db_query': ['ë©”ë‰´', 'menu', 'ë§í¬', 'link', 'ë§¤ë‹ˆì €', 'manager', 'ë‹´ë‹¹ì', 'ì¡°íšŒ', 'query'],
            'rag_query': ['rag', 'ê²€ìƒ‰', 'search', 'ë¬¸ì„œ', 'document', 'ì§€ì‹', 'knowledge']
        }
        
        # ì˜ë„ ê°ì§€
        detected_intents = []
        for intent, keywords in intent_keywords.items():
            if any(keyword in question_lower for keyword in keywords):
                detected_intents.append(intent)
        
        # ì˜ë„ë³„ ë„êµ¬ ë§¤í•‘
        tool_mapping = {
            'web_crawling': ['crawl4ai_scrape', 'crawl_urls_sequential'],
            'system_status': ['health_check'],
            'text_analysis': ['summarize_content'],
            'html_processing': ['ari_extract_main_blocks', 'ari_markdown_to_json', 'convert_to_json_format'],
            'db_query': ['menu_search'],
            'rag_query': []
        }
        
        # ê´€ë ¨ ë„êµ¬ë“¤ ìˆ˜ì§‘
        relevant_tools = []
        for intent in detected_intents:
            relevant_tools.extend(tool_mapping.get(intent, []))
        
        # ê¸°ë³¸ ë„êµ¬ë“¤ (í•­ìƒ í¬í•¨)
        essential_tools = ['health_check', 'crawl4ai_scrape', 'summarize_content']
        relevant_tools.extend(essential_tools)
        
        # ì¤‘ë³µ ì œê±°
        relevant_tools = list(set(relevant_tools))
        
        # ë„êµ¬ í•„í„°ë§
        filtered_tools = []
        for tool in available_tools:
            tool_name = tool.get('function', {}).get('name', '')
            if tool_name in relevant_tools:
                filtered_tools.append(tool)
        
        logger.info(f"ğŸ” ì˜ë„ ë¶„ë¥˜: {detected_intents}")
        logger.info(f"ğŸ¯ í•„í„°ë§ëœ ë„êµ¬: {[t.get('function', {}).get('name', '') for t in filtered_tools]}")
        
        return filtered_tools if filtered_tools else available_tools[:10]  # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
    
    def _get_tool_categories_description(self, tools: List[Dict[str, Any]]) -> str:
        """
        ë„êµ¬ ì¹´í…Œê³ ë¦¬ë³„ ì„¤ëª… ìƒì„±
        """
        categories = {
            'web': [],
            'system': [],
            'text': [],
            'html': [],
            'other': []
        }
        
        for tool in tools:
            tool_name = tool.get('function', {}).get('name', '')
            tool_desc = tool.get('function', {}).get('description', '')
            
            if 'crawl' in tool_name or 'scrape' in tool_name:
                categories['web'].append(f"- {tool_name}: {tool_desc}")
            elif 'health' in tool_name or 'status' in tool_name:
                categories['system'].append(f"- {tool_name}: {tool_desc}")
            elif 'summarize' in tool_name or 'extract' in tool_name:
                categories['text'].append(f"- {tool_name}: {tool_desc}")
            elif 'html' in tool_name or 'table' in tool_name:
                categories['html'].append(f"- {tool_name}: {tool_desc}")
            else:
                categories['other'].append(f"- {tool_name}: {tool_desc}")
        
        description = ""
        if categories['web']:
            description += "**ì›¹ í¬ë¡¤ë§ ë„êµ¬:**\n" + "\n".join(categories['web']) + "\n\n"
        if categories['system']:
            description += "**ì‹œìŠ¤í…œ ìƒíƒœ ë„êµ¬:**\n" + "\n".join(categories['system']) + "\n\n"
        if categories['text']:
            description += "**í…ìŠ¤íŠ¸ ë¶„ì„ ë„êµ¬:**\n" + "\n".join(categories['text']) + "\n\n"
        if categories['html']:
            description += "**HTML ì²˜ë¦¬ ë„êµ¬:**\n" + "\n".join(categories['html']) + "\n\n"
        if categories['other']:
            description += "**ê¸°íƒ€ ë„êµ¬:**\n" + "\n".join(categories['other']) + "\n\n"
        
        return description
    
    async def generate_response(self, prompt: str) -> str:
        """
        Generate a simple response using OpenAI Chat Completions API
        
        Args:
            prompt: The prompt to send to the LLM
            
        Returns:
            Generated response as string
        """
        try:
            # í† í° ìˆ˜ ê³„ì‚° ë° ì œí•œ í™•ì¸
            tokens = self.tokenizer.encode(prompt)
            token_count = len(tokens)
            
            # RAGìš©ìœ¼ë¡œëŠ” gpt-4o-mini ì‚¬ìš© (ë” ì €ë ´í•˜ê³  í† í° ì œí•œì´ í¼)
            model = "gpt-4o-mini"  # RAG ì‘ë‹µ ìƒì„±ìš©
            
            # í† í° ì œí•œ í™•ì¸ (gpt-4o-miniëŠ” 128k context)
            max_tokens = 20000  # ë” ì•ˆì „í•œ ì œí•œ
            if token_count > max_tokens:
                logger.warning(f"Prompt too long ({token_count} tokens), truncating to {max_tokens}")
                tokens = tokens[:max_tokens]
                prompt = self.tokenizer.decode(tokens)
                token_count = max_tokens
            
            logger.info(f"LLM request: {token_count} tokens, model: {model}")
            
            response = await self._client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,  # ì‘ë‹µ í† í° ìˆ˜ ì¦ê°€
                temperature=0.7
            )
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Failed to generate response: {e}")
            raise LLMQueryError(f"Failed to generate response: {str(e)}")
    
    async def query_stream(self, question: str, available_tools: List[Dict[str, Any]]) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Execute a streaming query against the LLM with available tools
        
        Args:
            question: User's question
            available_tools: List of available MCP tools in OpenAI format
            
        Yields:
            Dict with event type and data (not SSE formatted strings)
        """
        try:
            # 1ë‹¨ê³„: ì˜ë„ ë¶„ë¥˜ ë° ë„êµ¬ í•„í„°ë§
            filtered_tools = self._filter_tools_by_intent(question, available_tools)
            
            # 2ì°¨ ìš”ì²­ - ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
            logger.warning(f"ğŸ”§ LLM tools available: {len(filtered_tools)} tools (filtered from {len(available_tools)})")
            logger.warning(f"ğŸ”§ LLM question: {question[:100]}...")
            
            attempt = 0
            stream = None
            last_error: Exception | None = None
            while attempt < 2 and stream is None:
                try:
                    logger.warning(f"ğŸ”§ Creating OpenAI stream with {len(filtered_tools)} filtered tools")
                    stream = await self._client.responses.create(
                        model=settings.openai_model,
                        input=[
                            {"role": "system", "content": f"""You are an AI assistant specialized in web analysis and data processing.

CRITICAL: Always use tools when available. Execute the appropriate tools to perform real work; avoid generic explanations without using tools.

Available tools: {len(filtered_tools)} (filtered from {len(available_tools)} total)

Tool usage principles:
1) If a URL is present â†’ use crawl4ai_scrape or crawl_urls_sequential
2) For system status â†’ use health_check
3) For text summarization/clean-up â†’ use summarize_content
4) For menu/DB queries â†’ use menu_search
5) For HTML/Confluence content â†’ use ari_extract_main_blocks and/or ari_markdown_to_json and/or convert_to_json_format

Response policy:
- After using tools, produce a concise, high-signal answer based on the results.
- IMPORTANT: Write all final answers to the user in Korean.
"""},
                            {"role": "user", "content": question}
                        ],
                        tools=filtered_tools,
                        stream=True
                    )
                    logger.warning(f"ğŸ”§ OpenAI stream created successfully")
                except Exception as e:
                    last_error = e
                    attempt += 1
                    logger.warning(f"Streaming create failed (attempt {attempt}): {e}")
            if stream is None and last_error is not None:
                raise last_error

            tool_calls = []
            current_call = None
            
            async for chunk in stream:
                # OpenAI Responses API ì´ë²¤íŠ¸ ì²˜ë¦¬
                event_type = type(chunk).__name__
                logger.warning(f"ğŸ”§ Event: {event_type}")
                
                # Function call ì‹œì‘
                if event_type == 'ResponseOutputItemAddedEvent':
                    if hasattr(chunk, 'item') and hasattr(chunk.item, 'type') and chunk.item.type == 'function_call':
                        # chunk.itemì´ ì§ì ‘ ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆìŒ
                        tool_call = {
                            'id': chunk.item.id,
                            'name': chunk.item.name,
                            'arguments': chunk.item.arguments or ''
                        }
                        tool_calls.append(tool_call)
                        current_call = tool_call
                        logger.warning(f"ğŸ”§ Function call started: {chunk.item.name}")
                
                # Function call arguments ìˆ˜ì§‘
                elif event_type == 'ResponseFunctionCallArgumentsDeltaEvent':
                    if current_call and hasattr(chunk, 'delta'):
                        current_call['arguments'] += chunk.delta
                        logger.warning(f"ğŸ”§ Arguments delta: {chunk.delta[:20]}...")
                
                # Function call ì™„ë£Œ
                elif event_type == 'ResponseFunctionCallArgumentsDoneEvent':
                    if current_call:
                        logger.warning(f"ğŸ”§ Function call completed: {current_call['name']}")
                        yield f"data: {json.dumps({'type': 'tool_call_delta', 'data': {'tool_calls': tool_calls}})}\n\n"
                
                # ì „ì²´ ì‘ë‹µ ì™„ë£Œ
                elif event_type == 'ResponseCompletedEvent':
                    logger.warning(f"ğŸ”§ Response completed with {len(tool_calls)} tool calls")
                    break

            # Tool í˜¸ì¶œì´ ìˆëŠ” ê²½ìš° ì™„ì„±ëœ tool_calls ë°˜í™˜
            if tool_calls:
                yield f"data: {json.dumps({'type': 'tool_calls_ready', 'data': {'tool_calls': tool_calls}})}\n\n"
                async for event in self._process_tool_calls_stream(question, tool_calls):
                    if isinstance(event, dict):
                        yield f"data: {json.dumps(event)}\n\n"
                    else:
                        yield event
            
            yield f"data: {json.dumps({'type': 'done', 'data': {}})}\n\n"
            
        except Exception as e:
            # ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ: ìƒíƒœë¡œ ì•Œë¦¬ê³  ìƒìœ„ì—ì„œ í´ë°± ì§„í–‰
            logger.warning(f"Streaming LLM query failed, will fallback: {e}")
            yield f"data: {json.dumps({'type': 'error', 'data': {'message': f'LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}'}})}\n\n"
            yield f"data: {json.dumps({'type': 'done', 'data': {}})}\n\n"
    
    async def _process_tool_calls(self, question: str, tool_calls: List[Any]) -> List[Any]:
        """Process tool calls and return next input for LLM"""
        next_input: List[Any] = [{"role": "user", "content": question}]
        
        for call in tool_calls:
            args = call.arguments
            if isinstance(args, str):
                args = json.loads(args)
            
            result = await mcp_service.call_tool(call.name, args)

            # í˜¸ì¶œ ìì²´ë¥¼ ë©”ì‹œì§€ ë°°ì—´ì— ì¶”ê°€
            next_input.append(call)
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° ë„êµ¬ë“¤ì€ LLMì— ìš”ì•½ëœ ì •ë³´ë§Œ ì „ë‹¬
            if call.name == 'take_screenshot':
                result_str = str({
                    "success": True,
                    "message": "ìŠ¤í¬ë¦°ìƒ·ì´ ì„±ê³µì ìœ¼ë¡œ ì´¬ì˜ë˜ì—ˆìŠµë‹ˆë‹¤",
                    "screenshot_captured": True
                })
                logger.info(f"Screenshot tool result simplified for LLM: original {len(str(result)):,} chars -> {len(result_str)} chars")
            elif call.name == 'crawl_webpage':
                # HTML ì „ì²´ ëŒ€ì‹  ë©”íƒ€ ì •ë³´ë§Œ ì „ë‹¬
                try:
                    original_result = result.structured_content if hasattr(result, 'structured_content') else result.data
                    result_str = str({
                        "success": original_result.get("success", True),
                        "url": original_result.get("url", ""),
                        "title": original_result.get("title", ""),
                        "meta_description": original_result.get("meta_description", ""),
                        "content_length": original_result.get("content_length", 0),
                        "message": "ì›¹í˜ì´ì§€ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ (HTML ë°ì´í„°ëŠ” LLM ë¶„ì„ì— ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ìƒëµ)"
                    })
                    logger.info(f"Crawl webpage result simplified for LLM: original {len(str(result)):,} chars -> {len(result_str)} chars")
                except Exception as e:
                    logger.warning(f"Failed to simplify crawl result: {e}")
                    result_str = str(result)  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ì‚¬ìš©
            else:
                # ë‹¤ë¥¸ ë„êµ¬ë“¤ì˜ ê²°ê³¼ ë°ì´í„° í¬ê¸° ì œí•œ (OpenAI API ì œí•œ: 10MB)
                result_str = str(result)
                max_output_size = 8 * 1024 * 1024  # 8MBë¡œ ì•ˆì „í•˜ê²Œ ì œí•œ
                
                if len(result_str) > max_output_size:
                    # ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ìš”ì•½í•´ì„œ ì „ë‹¬
                    truncated_result = {
                        "success": True,
                        "message": f"ê²°ê³¼ê°€ ë„ˆë¬´ ì»¤ì„œ ìš”ì•½ë¨ (ì›ë³¸ í¬ê¸°: {len(result_str):,} ë¬¸ì)",
                        "truncated": True,
                        "sample": result_str[:1000] + "..." if len(result_str) > 1000 else result_str
                    }
                    result_str = str(truncated_result)
                    logger.warning(f"Tool result truncated for {call.name}: {len(str(result)):,} -> {len(result_str):,} chars")
            
            # ì‹¤í–‰ ê²°ê³¼ë¥¼ function_call_output í˜•ì‹ìœ¼ë¡œ ì¶”ê°€
            next_input.append({
                "type": "function_call_output",
                "call_id": call.call_id,
                "output": result_str,
            })
        
        return next_input
    
    async def _handle_streaming_tool_calls(self, delta_tool_calls, tool_calls, current_call):
        """Handle streaming tool calls from delta"""
        for tool_call in delta_tool_calls:
            if tool_call.index == len(tool_calls):
                tool_calls.append({
                    'id': tool_call.id,
                    'name': tool_call.function.name if tool_call.function else '',
                    'arguments': tool_call.function.arguments if tool_call.function else ''
                })
                current_call = tool_calls[-1]
            elif current_call:
                if tool_call.function and tool_call.function.arguments:
                    current_call['arguments'] += tool_call.function.arguments
        return current_call
    
    async def _process_tool_calls_stream(self, question: str, tool_calls: List[Dict]) -> AsyncGenerator[str, None]:
        """Process tool calls in streaming mode"""
        yield f"data: {json.dumps({'type': 'tool_start', 'data': {'message': 'Tool ì‹¤í–‰ ì¤‘...'}})}\n\n"
        
        next_input: List[Any] = [{"role": "user", "content": question}]
        
        # ê° íˆ´ í˜¸ì¶œ ì²˜ë¦¬
        for call in tool_calls:
            try:
                args = json.loads(call['arguments']) if call['arguments'] else {}
                result = await mcp_service.call_tool(call['name'], args)
                
                # Tool ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ì†¡ (ì›ë³¸ ê²°ê³¼ ì „ë‹¬)
                tool_message = f"Tool '{call['name']}' ì‹¤í–‰ ì™„ë£Œ"
                yield f"data: {json.dumps({'type': 'tool_result', 'data': {'tool_name': call['name'], 'message': tool_message, 'result': result}})}\n\n"
                
                # ë‹¤ìŒ ìš”ì²­ì„ ìœ„í•œ ë©”ì‹œì§€ êµ¬ì„±
                next_input.append({
                    "type": "function_call",
                    "call_id": call['id'],
                    "name": call['name'],
                    "arguments": call['arguments']
                })
                                # ëŒ€ìš©ëŸ‰ ë°ì´í„° ë„êµ¬ë“¤ì€ LLMì— ìš”ì•½ëœ ì •ë³´ë§Œ ì „ë‹¬
                if call['name'] == 'take_screenshot':
                    result_str = str({
                        "success": True,
                        "message": "ìŠ¤í¬ë¦°ìƒ·ì´ ì„±ê³µì ìœ¼ë¡œ ì´¬ì˜ë˜ì—ˆìŠµë‹ˆë‹¤",
                        "screenshot_captured": True
                    })
                    logger.info(f"Screenshot tool result simplified for LLM: original {len(str(result)):,} chars -> {len(result_str)} chars")
                elif call['name'] == 'crawl_webpage':
                    # HTML ì „ì²´ ëŒ€ì‹  ë©”íƒ€ ì •ë³´ë§Œ ì „ë‹¬
                    try:
                        original_result = result.structured_content if hasattr(result, 'structured_content') else result.data
                        result_str = str({
                            "success": original_result.get("success", True),
                            "url": original_result.get("url", ""),
                            "title": original_result.get("title", ""),
                            "meta_description": original_result.get("meta_description", ""),
                            "content_length": original_result.get("content_length", 0),
                            "message": "ì›¹í˜ì´ì§€ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ (HTML ë°ì´í„°ëŠ” LLM ë¶„ì„ì— ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ìƒëµ)"
                        })
                        logger.info(f"Crawl webpage result simplified for LLM: original {len(str(result)):,} chars -> {len(result_str)} chars")
                    except Exception as e:
                        logger.warning(f"Failed to simplify crawl result: {e}")
                        result_str = str(result)  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ì‚¬ìš©
                else:
                    # ë‹¤ë¥¸ ë„êµ¬ë“¤ì˜ ê²°ê³¼ ë°ì´í„° í¬ê¸° ì œí•œ (OpenAI API ì œí•œ: 10MB)
                    result_str = str(result)
                    max_output_size = 8 * 1024 * 1024  # 8MBë¡œ ì•ˆì „í•˜ê²Œ ì œí•œ
                    
                    if len(result_str) > max_output_size:
                        # ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ìš”ì•½í•´ì„œ ì „ë‹¬
                        truncated_result = {
                            "success": True,
                            "message": f"ê²°ê³¼ê°€ ë„ˆë¬´ ì»¤ì„œ ìš”ì•½ë¨ (ì›ë³¸ í¬ê¸°: {len(result_str):,} ë¬¸ì)",
                            "truncated": True,
                            "sample": result_str[:1000] + "..." if len(result_str) > 1000 else result_str
                        }
                        result_str = str(truncated_result)
                        logger.warning(f"Tool result truncated for {call['name']}: {len(str(result)):,} -> {len(result_str):,} chars")
                
                next_input.append({
                    "type": "function_call_output", 
                    "call_id": call['id'],
                    "output": result_str
                })
                
            except Exception as tool_error:
                error_message = f'Tool ì‹¤í–‰ ì˜¤ë¥˜: {str(tool_error)}'
                yield f"data: {json.dumps({'type': 'tool_error', 'data': {'tool_name': call['name'], 'error': error_message}})}\n\n"

        # 2ì°¨ í˜¸ì¶œ - ìµœì¢… ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë°
        final_stream = await self._client.responses.create(
            model=settings.openai_model,
            input=next_input,
            stream=True
        )
        
        async for chunk in final_stream:
            if hasattr(chunk, 'choices') and chunk.choices:
                choice = chunk.choices[0]
                if hasattr(choice, 'delta') and choice.delta and hasattr(choice.delta, 'content') and choice.delta.content:
                    yield f"data: {json.dumps({'type': 'final_content', 'data': {'content': choice.delta.content}})}\n\n"

# Global service instance
llm_service = LLMService()
