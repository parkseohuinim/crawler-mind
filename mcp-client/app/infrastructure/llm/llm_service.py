"""LLM Service - Handles OpenAI API interactions"""
from typing import List, Dict, Any, AsyncGenerator
import json
import logging
import tiktoken
from openai import AsyncOpenAI
from app.config import settings
from app.infrastructure.mcp.mcp_service import mcp_service
from app.shared.exceptions.base import LLMQueryError

logger = logging.getLogger(__name__)

class LLMService:
    """Service class for managing LLM interactions"""
    
    def __init__(self):
        self._client = AsyncOpenAI(api_key=settings.openai_api_key)
        
        # Initialize tokenizer for token counting
        try:
            self.tokenizer = tiktoken.encoding_for_model("gpt-4o")
        except Exception:
            # Fallback to cl100k_base encoding
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
    
    async def query(self, question: str, available_tools: List[Dict[str, Any]]) -> str:
        """
        Execute a query against the LLM with available tools
        
        Args:
            question: User's question
            available_tools: List of available MCP tools in OpenAI format
            
        Returns:
            LLM response as string
        """
        try:
            # 1ì°¨ ìš”ì²­
            resp = await self._client.responses.create(
                model=settings.openai_model,
                input=[{"role": "user", "content": question}],
                tools=available_tools,
            )

            # Tool í˜¸ì¶œì´ ì—†ì„ ë•Œ
            tool_calls = [o for o in resp.output if getattr(o, "type", "") == "function_call"]
            if not tool_calls:
                return resp.output_text

            # Tool í˜¸ì¶œ ì²˜ë¦¬
            next_input = await self._process_tool_calls(question, tool_calls)

            # 2ì°¨ í˜¸ì¶œ -> ìµœì¢… ë‹µë³€
            final = await self._client.responses.create(
                model=settings.openai_model,
                input=next_input,
            )
            return final.output_text
        
        except Exception as e:
            logger.error(f"LLM query failed: {e}")
            raise LLMQueryError(f"LLM ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    async def generate_response(self, prompt: str) -> str:
        """
        Generate a simple response using OpenAI Chat Completions API
        
        Args:
            prompt: The prompt to send to the LLM
            
        Returns:
            Generated response as string
        """
        try:
            # í† í° ìˆ˜ ê³„ì‚° ë° ì œí•œ í™•ì¸
            tokens = self.tokenizer.encode(prompt)
            token_count = len(tokens)
            
            # RAGìš©ìœ¼ë¡œëŠ” gpt-4o-mini ì‚¬ìš© (ë” ì €ë ´í•˜ê³  í† í° ì œí•œì´ í¼)
            model = "gpt-4o-mini"  # RAG ì‘ë‹µ ìƒì„±ìš©
            
            # í† í° ì œí•œ í™•ì¸ (gpt-4o-miniëŠ” 128k context)
            max_tokens = 20000  # ë” ì•ˆì „í•œ ì œí•œ
            if token_count > max_tokens:
                logger.warning(f"Prompt too long ({token_count} tokens), truncating to {max_tokens}")
                tokens = tokens[:max_tokens]
                prompt = self.tokenizer.decode(tokens)
                token_count = max_tokens
            
            logger.info(f"LLM request: {token_count} tokens, model: {model}")
            
            response = await self._client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,  # ì‘ë‹µ í† í° ìˆ˜ ì¦ê°€
                temperature=0.7
            )
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Failed to generate response: {e}")
            raise LLMQueryError(f"Failed to generate response: {str(e)}")
    
    async def query_stream(self, question: str, available_tools: List[Dict[str, Any]]) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Execute a streaming query against the LLM with available tools
        
        Args:
            question: User's question
            available_tools: List of available MCP tools in OpenAI format
            
        Yields:
            Dict with event type and data (not SSE formatted strings)
        """
        try:
            # 1ì°¨ ìš”ì²­ - ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ (ê°„ë‹¨ ì¬ì‹œë„)
            logger.warning(f"ğŸ”§ LLM tools available: {len(available_tools)} tools")
            logger.warning(f"ğŸ”§ LLM question: {question[:100]}...")
            
            attempt = 0
            stream = None
            last_error: Exception | None = None
            while attempt < 2 and stream is None:
                try:
                    logger.warning(f"ğŸ”§ Creating OpenAI stream with {len(available_tools)} tools")
                    stream = await self._client.responses.create(
                        model=settings.openai_model,
                        input=[
                            {"role": "system", "content": "You are a web analysis assistant. You MUST use ALL tools that are explicitly mentioned in the user's request. Complete each step in the exact order specified. Do not stop until you have used every requested tool. If a tool fails, still continue with the remaining tools."},
                            {"role": "user", "content": question}
                        ],
                        tools=available_tools,
                        stream=True
                    )
                    logger.warning(f"ğŸ”§ OpenAI stream created successfully")
                except Exception as e:
                    last_error = e
                    attempt += 1
                    logger.warning(f"Streaming create failed (attempt {attempt}): {e}")
            if stream is None and last_error is not None:
                raise last_error

            tool_calls = []
            current_call = None
            
            async for chunk in stream:
                # OpenAI Responses API ì´ë²¤íŠ¸ ì²˜ë¦¬
                event_type = type(chunk).__name__
                logger.warning(f"ğŸ”§ Event: {event_type}")
                
                # Function call ì‹œì‘
                if event_type == 'ResponseOutputItemAddedEvent':
                    if hasattr(chunk, 'item') and hasattr(chunk.item, 'type') and chunk.item.type == 'function_call':
                        # chunk.itemì´ ì§ì ‘ ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆìŒ
                        tool_call = {
                            'id': chunk.item.id,
                            'name': chunk.item.name,
                            'arguments': chunk.item.arguments or ''
                        }
                        tool_calls.append(tool_call)
                        current_call = tool_call
                        logger.warning(f"ğŸ”§ Function call started: {chunk.item.name}")
                
                # Function call arguments ìˆ˜ì§‘
                elif event_type == 'ResponseFunctionCallArgumentsDeltaEvent':
                    if current_call and hasattr(chunk, 'delta'):
                        current_call['arguments'] += chunk.delta
                        logger.warning(f"ğŸ”§ Arguments delta: {chunk.delta[:20]}...")
                
                # Function call ì™„ë£Œ
                elif event_type == 'ResponseFunctionCallArgumentsDoneEvent':
                    if current_call:
                        logger.warning(f"ğŸ”§ Function call completed: {current_call['name']}")
                        yield {'type': 'tool_call_delta', 'data': {'tool_calls': tool_calls}}
                
                # ì „ì²´ ì‘ë‹µ ì™„ë£Œ
                elif event_type == 'ResponseCompletedEvent':
                    logger.warning(f"ğŸ”§ Response completed with {len(tool_calls)} tool calls")
                    break

            # Tool í˜¸ì¶œì´ ìˆëŠ” ê²½ìš° ì™„ì„±ëœ tool_calls ë°˜í™˜
            if tool_calls:
                yield {'type': 'tool_calls_ready', 'data': {'tool_calls': tool_calls}}
                async for event in self._process_tool_calls_stream(question, tool_calls):
                    yield event
            
            yield {'type': 'done', 'data': {}}
            
        except Exception as e:
            # ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ: ìƒíƒœë¡œ ì•Œë¦¬ê³  ìƒìœ„ì—ì„œ í´ë°± ì§„í–‰
            logger.warning(f"Streaming LLM query failed, will fallback: {e}")
            yield {'type': 'error', 'data': {'message': f'LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}'}}
            yield {'type': 'done', 'data': {}}
    
    async def _process_tool_calls(self, question: str, tool_calls: List[Any]) -> List[Any]:
        """Process tool calls and return next input for LLM"""
        next_input: List[Any] = [{"role": "user", "content": question}]
        
        for call in tool_calls:
            args = call.arguments
            if isinstance(args, str):
                args = json.loads(args)
            
            result = await mcp_service.call_tool(call.name, args)

            # í˜¸ì¶œ ìì²´ë¥¼ ë©”ì‹œì§€ ë°°ì—´ì— ì¶”ê°€
            next_input.append(call)
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° ë„êµ¬ë“¤ì€ LLMì— ìš”ì•½ëœ ì •ë³´ë§Œ ì „ë‹¬
            if call.name == 'take_screenshot':
                result_str = str({
                    "success": True,
                    "message": "ìŠ¤í¬ë¦°ìƒ·ì´ ì„±ê³µì ìœ¼ë¡œ ì´¬ì˜ë˜ì—ˆìŠµë‹ˆë‹¤",
                    "screenshot_captured": True
                })
                logger.info(f"Screenshot tool result simplified for LLM: original {len(str(result)):,} chars -> {len(result_str)} chars")
            elif call.name == 'crawl_webpage':
                # HTML ì „ì²´ ëŒ€ì‹  ë©”íƒ€ ì •ë³´ë§Œ ì „ë‹¬
                try:
                    original_result = result.structured_content if hasattr(result, 'structured_content') else result.data
                    result_str = str({
                        "success": original_result.get("success", True),
                        "url": original_result.get("url", ""),
                        "title": original_result.get("title", ""),
                        "meta_description": original_result.get("meta_description", ""),
                        "content_length": original_result.get("content_length", 0),
                        "message": "ì›¹í˜ì´ì§€ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ (HTML ë°ì´í„°ëŠ” LLM ë¶„ì„ì— ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ìƒëµ)"
                    })
                    logger.info(f"Crawl webpage result simplified for LLM: original {len(str(result)):,} chars -> {len(result_str)} chars")
                except Exception as e:
                    logger.warning(f"Failed to simplify crawl result: {e}")
                    result_str = str(result)  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ì‚¬ìš©
            else:
                # ë‹¤ë¥¸ ë„êµ¬ë“¤ì˜ ê²°ê³¼ ë°ì´í„° í¬ê¸° ì œí•œ (OpenAI API ì œí•œ: 10MB)
                result_str = str(result)
                max_output_size = 8 * 1024 * 1024  # 8MBë¡œ ì•ˆì „í•˜ê²Œ ì œí•œ
                
                if len(result_str) > max_output_size:
                    # ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ìš”ì•½í•´ì„œ ì „ë‹¬
                    truncated_result = {
                        "success": True,
                        "message": f"ê²°ê³¼ê°€ ë„ˆë¬´ ì»¤ì„œ ìš”ì•½ë¨ (ì›ë³¸ í¬ê¸°: {len(result_str):,} ë¬¸ì)",
                        "truncated": True,
                        "sample": result_str[:1000] + "..." if len(result_str) > 1000 else result_str
                    }
                    result_str = str(truncated_result)
                    logger.warning(f"Tool result truncated for {call.name}: {len(str(result)):,} -> {len(result_str):,} chars")
            
            # ì‹¤í–‰ ê²°ê³¼ë¥¼ function_call_output í˜•ì‹ìœ¼ë¡œ ì¶”ê°€
            next_input.append({
                "type": "function_call_output",
                "call_id": call.call_id,
                "output": result_str,
            })
        
        return next_input
    
    async def _handle_streaming_tool_calls(self, delta_tool_calls, tool_calls, current_call):
        """Handle streaming tool calls from delta"""
        for tool_call in delta_tool_calls:
            if tool_call.index == len(tool_calls):
                tool_calls.append({
                    'id': tool_call.id,
                    'name': tool_call.function.name if tool_call.function else '',
                    'arguments': tool_call.function.arguments if tool_call.function else ''
                })
                current_call = tool_calls[-1]
            elif current_call:
                if tool_call.function and tool_call.function.arguments:
                    current_call['arguments'] += tool_call.function.arguments
        return current_call
    
    async def _process_tool_calls_stream(self, question: str, tool_calls: List[Dict]) -> AsyncGenerator[Dict[str, Any], None]:
        """Process tool calls in streaming mode"""
        yield {'type': 'tool_start', 'data': {'message': 'Tool ì‹¤í–‰ ì¤‘...'}}
        
        next_input: List[Any] = [{"role": "user", "content": question}]
        
        # ê° íˆ´ í˜¸ì¶œ ì²˜ë¦¬
        for call in tool_calls:
            try:
                args = json.loads(call['arguments']) if call['arguments'] else {}
                result = await mcp_service.call_tool(call['name'], args)
                
                # Tool ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ì†¡ (ì›ë³¸ ê²°ê³¼ ì „ë‹¬)
                tool_message = f"Tool '{call['name']}' ì‹¤í–‰ ì™„ë£Œ"
                yield {'type': 'tool_result', 'data': {'tool_name': call['name'], 'message': tool_message, 'result': result}}
                
                # ë‹¤ìŒ ìš”ì²­ì„ ìœ„í•œ ë©”ì‹œì§€ êµ¬ì„±
                next_input.append({
                    "type": "function_call",
                    "call_id": call['id'],
                    "name": call['name'],
                    "arguments": call['arguments']
                })
                                # ëŒ€ìš©ëŸ‰ ë°ì´í„° ë„êµ¬ë“¤ì€ LLMì— ìš”ì•½ëœ ì •ë³´ë§Œ ì „ë‹¬
                if call['name'] == 'take_screenshot':
                    result_str = str({
                        "success": True,
                        "message": "ìŠ¤í¬ë¦°ìƒ·ì´ ì„±ê³µì ìœ¼ë¡œ ì´¬ì˜ë˜ì—ˆìŠµë‹ˆë‹¤",
                        "screenshot_captured": True
                    })
                    logger.info(f"Screenshot tool result simplified for LLM: original {len(str(result)):,} chars -> {len(result_str)} chars")
                elif call['name'] == 'crawl_webpage':
                    # HTML ì „ì²´ ëŒ€ì‹  ë©”íƒ€ ì •ë³´ë§Œ ì „ë‹¬
                    try:
                        original_result = result.structured_content if hasattr(result, 'structured_content') else result.data
                        result_str = str({
                            "success": original_result.get("success", True),
                            "url": original_result.get("url", ""),
                            "title": original_result.get("title", ""),
                            "meta_description": original_result.get("meta_description", ""),
                            "content_length": original_result.get("content_length", 0),
                            "message": "ì›¹í˜ì´ì§€ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ (HTML ë°ì´í„°ëŠ” LLM ë¶„ì„ì— ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ìƒëµ)"
                        })
                        logger.info(f"Crawl webpage result simplified for LLM: original {len(str(result)):,} chars -> {len(result_str)} chars")
                    except Exception as e:
                        logger.warning(f"Failed to simplify crawl result: {e}")
                        result_str = str(result)  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ì‚¬ìš©
                else:
                    # ë‹¤ë¥¸ ë„êµ¬ë“¤ì˜ ê²°ê³¼ ë°ì´í„° í¬ê¸° ì œí•œ (OpenAI API ì œí•œ: 10MB)
                    result_str = str(result)
                    max_output_size = 8 * 1024 * 1024  # 8MBë¡œ ì•ˆì „í•˜ê²Œ ì œí•œ
                    
                    if len(result_str) > max_output_size:
                        # ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ìš”ì•½í•´ì„œ ì „ë‹¬
                        truncated_result = {
                            "success": True,
                            "message": f"ê²°ê³¼ê°€ ë„ˆë¬´ ì»¤ì„œ ìš”ì•½ë¨ (ì›ë³¸ í¬ê¸°: {len(result_str):,} ë¬¸ì)",
                            "truncated": True,
                            "sample": result_str[:1000] + "..." if len(result_str) > 1000 else result_str
                        }
                        result_str = str(truncated_result)
                        logger.warning(f"Tool result truncated for {call['name']}: {len(str(result)):,} -> {len(result_str):,} chars")
                
                next_input.append({
                    "type": "function_call_output", 
                    "call_id": call['id'],
                    "output": result_str
                })
                
            except Exception as tool_error:
                error_message = f'Tool ì‹¤í–‰ ì˜¤ë¥˜: {str(tool_error)}'
                yield {'type': 'tool_error', 'data': {'tool_name': call['name'], 'error': error_message}}

        # 2ì°¨ í˜¸ì¶œ - ìµœì¢… ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë°
        final_stream = await self._client.responses.create(
            model=settings.openai_model,
            input=next_input,
            stream=True
        )
        
        async for chunk in final_stream:
            if hasattr(chunk, 'choices') and chunk.choices:
                choice = chunk.choices[0]
                if hasattr(choice, 'delta') and choice.delta and hasattr(choice.delta, 'content') and choice.delta.content:
                    yield {'type': 'final_content', 'data': {'content': choice.delta.content}}

# Global service instance
llm_service = LLMService()
