"""LLM Service - Handles OpenAI API interactions"""
from typing import List, Dict, Any, AsyncGenerator, Optional
import json
import logging
import tiktoken
import numpy as np
from openai import AsyncOpenAI
from app.config import settings
from app.infrastructure.mcp.mcp_service import mcp_service
from app.shared.exceptions.base import LLMQueryError

logger = logging.getLogger(__name__)

# ì„ë² ë”© ëª¨ë¸ (ì§€ì—° ë¡œë”©)
try:
    from sentence_transformers import SentenceTransformer
    EMBEDDING_AVAILABLE = True
except ImportError:
    EMBEDDING_AVAILABLE = False
    logger.warning("sentence-transformers not available, falling back to keyword-based intent detection")

class LLMService:
    """Service class for managing LLM interactions"""
    
    def __init__(self):
        self._client = AsyncOpenAI(api_key=settings.openai_api_key)
        try:
            self.tokenizer = tiktoken.encoding_for_model("gpt-4o")
        except Exception:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
        self._embedding_model: Optional[SentenceTransformer] = None
        self._intent_examples = self._get_intent_examples()
        
    def _get_embedding_model(self) -> Optional[SentenceTransformer]:
        """ì„ë² ë”© ëª¨ë¸ ì§€ì—° ë¡œë”©"""
        if not EMBEDDING_AVAILABLE:
            return None
            
        if self._embedding_model is None:
            try:
                # ë‹¤êµ­ì–´ ì§€ì› ê²½ëŸ‰ ëª¨ë¸ ì‚¬ìš©
                self._embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                logger.info("ğŸ¯ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: paraphrase-multilingual-MiniLM-L12-v2")
            except Exception as e:
                logger.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return None
        
        return self._embedding_model
    
    def _get_intent_examples(self) -> Dict[str, List[str]]:
        """ì˜ë„ë³„ ì˜ˆì‹œ ë¬¸ì¥ë“¤"""
        return {
            'web_crawling': [
                "ì›¹ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•´ì£¼ì„¸ìš”",
                "ì´ URLì˜ ë‚´ìš©ì„ ê°€ì ¸ì™€ì£¼ì„¸ìš”",
                "í˜ì´ì§€ë¥¼ ìŠ¤í¬ë˜í•‘í•´ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”",
                "ì‚¬ì´íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”",
                "ì›¹í˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”"
            ],
            'system_status': [
                "ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”",
                "ì„œë²„ê°€ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ ì²´í¬í•´ì£¼ì„¸ìš”",
                "í—¬ìŠ¤ì²´í¬ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”",
                "í˜„ì¬ ì‹œìŠ¤í…œ í†µê³„ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”",
                "ì„œë¹„ìŠ¤ ìƒíƒœë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
            ],
            'text_analysis': [
                "ì´ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”",
                "ë‚´ìš©ì„ ë¶„ì„í•´ì„œ ì •ë¦¬í•´ì£¼ì„¸ìš”",
                "ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”",
                "í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ì •ë³´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ë‚´ìš©ì„ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”"
            ],
            'html_processing': [
                "HTML íŒŒì¼ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”",
                "ì»¨í”Œë£¨ì–¸ìŠ¤ ë¬¸ì„œë¥¼ ë³€í™˜í•´ì£¼ì„¸ìš”",
                "ì—…ë¡œë“œí•œ íŒŒì¼ì„ ë¶„ì„í•´ì£¼ì„¸ìš”",
                "HTMLì—ì„œ ë©”ì¸ ì»¨í…ì¸ ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”",
                "ARI íŒŒì¼ì„ íŒŒì‹±í•´ì£¼ì„¸ìš”"
            ],
            'db_query': [
                "ë©”ë‰´ ì •ë³´ë¥¼ ì¡°íšŒí•´ì£¼ì„¸ìš”",
                "ë§í¬ ë°ì´í„°ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ë‹´ë‹¹ì ì •ë³´ë¥¼ ê²€ìƒ‰í•´ì£¼ì„¸ìš”",
                "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì£¼ì„¸ìš”",
                "ë§¤ë‹ˆì € ëª©ë¡ì„ ë³´ì—¬ì£¼ì„¸ìš”"
            ],
            'database_schema': [
                "ë©”ë‰´ í…Œì´ë¸” êµ¬ì¡°ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”",
                "ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ì»¬ëŸ¼ëª…ì„ ë³´ì—¬ì£¼ì„¸ìš”",
                "í…Œì´ë¸” í•„ë“œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ë©”ë‰´ ì»¬ëŸ¼ ì •ë³´ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”",
                "ë°ì´í„° êµ¬ì¡°ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"
            ],
            'rag_query': [
                "ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”",
                "ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ì°¾ì•„ì£¼ì„¸ìš”",
                "RAGë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”",
                "ë“±ë¡ëœ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ë²¡í„° ê²€ìƒ‰ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”"
            ]
        }
    
    async def _classify_intent_with_embedding(self, question: str) -> List[str]:
        """ì„ë² ë”© ê¸°ë°˜ ì˜ë„ ë¶„ë¥˜"""
        model = self._get_embedding_model()
        if not model:
            return []
        
        try:
            # ì§ˆë¬¸ ì„ë² ë”©
            question_embedding = model.encode([question])
            
            detected_intents = []
            similarity_threshold = 0.6  # ìœ ì‚¬ë„ ì„ê³„ê°’
            
            # ê° ì˜ë„ë³„ë¡œ ìœ ì‚¬ë„ ê³„ì‚°
            for intent, examples in self._intent_examples.items():
                example_embeddings = model.encode(examples)
                
                # ì§ˆë¬¸ê³¼ ê° ì˜ˆì‹œ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarities = np.dot(question_embedding, example_embeddings.T).flatten()
                max_similarity = float(np.max(similarities))
                
                logger.debug(f"ğŸ“Š ì˜ë„ '{intent}' ìœ ì‚¬ë„: {max_similarity:.3f}")
                
                if max_similarity > similarity_threshold:
                    detected_intents.append((intent, max_similarity))
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 3ê°œë§Œ ì„ íƒ
            detected_intents.sort(key=lambda x: x[1], reverse=True)
            result = [intent for intent, _ in detected_intents[:3]]
            
            logger.info(f"ğŸ¯ ì„ë² ë”© ê¸°ë°˜ ì˜ë„ ë¶„ë¥˜: {result}")
            return result
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ê¸°ë°˜ ì˜ë„ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return []
    
    async def _classify_intent_with_llm(self, question: str) -> List[str]:
        """LLM í”„ë¡¬í”„íŒ… ê¸°ë°˜ ì˜ë„ ë¶„ë¥˜"""
        try:
            prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ í•´ë‹¹í•˜ëŠ” ì˜ë„ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”.

ì¹´í…Œê³ ë¦¬ ì„¤ëª…:
- web_crawling: ì›¹í˜ì´ì§€ í¬ë¡¤ë§, ìŠ¤í¬ë˜í•‘, URL ë°ì´í„° ìˆ˜ì§‘
- system_status: ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸, í—¬ìŠ¤ì²´í¬, ì„œë²„ ëª¨ë‹ˆí„°ë§  
- text_analysis: í…ìŠ¤íŠ¸ ìš”ì•½, ë¶„ì„, ì •ë³´ ì¶”ì¶œ
- html_processing: HTML íŒŒì¼ ì²˜ë¦¬, ì»¨í”Œë£¨ì–¸ìŠ¤ ë¬¸ì„œ ë³€í™˜, ARI íŒŒì¼ íŒŒì‹±
- db_query: ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ, ë©”ë‰´/ë§í¬/ë‹´ë‹¹ì ì •ë³´ ê²€ìƒ‰ (ì‹¤ì œ ë°ì´í„°)
- database_schema: ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ, í…Œì´ë¸” êµ¬ì¡°, ì»¬ëŸ¼ ì •ë³´ í™•ì¸
- rag_query: ë¬¸ì„œ ê²€ìƒ‰, ì§€ì‹ë² ì´ìŠ¤ ì§ˆì˜, RAG ê¸°ë°˜ ì§ˆë¬¸ë‹µë³€

ì‚¬ìš©ì ì§ˆë¬¸: "{question}"

ìœ„ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ì¹´í…Œê³ ë¦¬ 1-3ê°œë¥¼ JSON ë°°ì—´ í˜•íƒœë¡œ ì‘ë‹µí•˜ì„¸ìš”.
ì˜ˆì‹œ: ["web_crawling", "text_analysis"]

ì‘ë‹µ:"""

            response = await self._client.chat.completions.create(
                model="gpt-4o-mini",  # ë¹ ë¥´ê³  ì €ë ´í•œ ëª¨ë¸ ì‚¬ìš©
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì‚¬ìš©ì ì˜ë„ë¥¼ ì •í™•íˆ ë¶„ë¥˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100,
                temperature=0.1  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                if result_text.startswith('[') and result_text.endswith(']'):
                    intents = json.loads(result_text)
                else:
                    # í…ìŠ¤íŠ¸ì—ì„œ JSON ë¶€ë¶„ ì¶”ì¶œ
                    import re
                    json_match = re.search(r'\[.*?\]', result_text)
                    if json_match:
                        intents = json.loads(json_match.group())
                    else:
                        intents = []
                
                # ìœ íš¨í•œ ì˜ë„ë§Œ í•„í„°ë§
                valid_intents = [intent for intent in intents if intent in self._intent_examples.keys()]
                
                logger.info(f"ğŸ¤– LLM ê¸°ë°˜ ì˜ë„ ë¶„ë¥˜: {valid_intents}")
                return valid_intents
                
            except json.JSONDecodeError as e:
                logger.warning(f"LLM ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {result_text}, ì˜¤ë¥˜: {e}")
                return []
                
        except Exception as e:
            logger.error(f"LLM ê¸°ë°˜ ì˜ë„ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return []
    
    async def _classify_intent_hybrid(self, question: str) -> List[str]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì˜ë„ ë¶„ë¥˜ (ì„ë² ë”© + LLM)"""
        embedding_intents = await self._classify_intent_with_embedding(question)
        llm_intents = await self._classify_intent_with_llm(question)
        
        # ë‘ ê²°ê³¼ë¥¼ ì¡°í•©í•˜ì—¬ ì‹ ë¢°ë„ ë†’ì€ ì˜ë„ ì„ íƒ
        combined_intents = []
        
        # 1. ë‘ ë°©ë²• ëª¨ë‘ì—ì„œ ê°ì§€ëœ ì˜ë„ (ë†’ì€ ì‹ ë¢°ë„)
        common_intents = list(set(embedding_intents) & set(llm_intents))
        combined_intents.extend(common_intents)
        
        # 2. ì„ë² ë”©ì—ì„œë§Œ ê°ì§€ëœ ì˜ë„ (ì¤‘ê°„ ì‹ ë¢°ë„)
        embedding_only = [i for i in embedding_intents if i not in common_intents]
        combined_intents.extend(embedding_only[:2])  # ìƒìœ„ 2ê°œë§Œ
        
        # 3. LLMì—ì„œë§Œ ê°ì§€ëœ ì˜ë„ (ì¤‘ê°„ ì‹ ë¢°ë„)
        llm_only = [i for i in llm_intents if i not in common_intents]
        combined_intents.extend(llm_only[:1])  # ìƒìœ„ 1ê°œë§Œ
        
        # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 3ê°œë¡œ ì œí•œ
        final_intents = list(dict.fromkeys(combined_intents))[:3]
        
        logger.info(f"ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ ì˜ë„ ë¶„ë¥˜:")
        logger.info(f"   ì„ë² ë”©: {embedding_intents}")
        logger.info(f"   LLM: {llm_intents}")
        logger.info(f"   ìµœì¢…: {final_intents}")
        
        return final_intents
    
    def _format_tools_for_openai(self, available_tools: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Convert MCP tools format to OpenAI tools format
        
        MCP format might be: {"name": "tool_name", "description": "...", "parameters": {...}}
        OpenAI expects: {"type": "function", "function": {"name": "...", "description": "...", "parameters": {...}}}
        """
        formatted_tools = []
        for tool in available_tools:
            # Check if already in OpenAI format
            if "type" in tool and tool["type"] == "function" and "function" in tool:
                formatted_tools.append(tool)
            else:
                # Convert from MCP format to OpenAI format
                formatted_tool = {
                    "type": "function",
                    "function": {
                        "name": tool.get("name", ""),
                        "description": tool.get("description", ""),
                        "parameters": tool.get("parameters", {
                            "type": "object",
                            "properties": {},
                            "required": []
                        })
                    }
                }
                formatted_tools.append(formatted_tool)
        return formatted_tools
    
    # (ì‚­ì œë¨) ìƒë‹¨ ì¤‘ë³µ ì •ì˜ëœ _filter_tools_by_intent â€” í´ë˜ìŠ¤ì˜ í•˜ë‹¨ ì •ì˜ë§Œ ì‚¬ìš©
    
    async def query(self, question: str, available_tools: List[Dict[str, Any]]) -> str:
        """Execute a query against the LLM with available tools"""
        try:
            # 1ë‹¨ê³„: ì˜ë„ ë¶„ë¥˜ ë° ë„êµ¬ í•„í„°ë§ (í† í° ì œí•œ í•´ê²°)
            filtered_tools = await self._filter_tools_by_intent(question, available_tools)
            
            # Format tools for OpenAI
            formatted_tools = self._format_tools_for_openai(filtered_tools)
            
            # Log for debugging
            logger.info(f"ğŸ” ì˜ë„ ë¶„ë¥˜ ì™„ë£Œ: {len(filtered_tools)}ê°œ ë„êµ¬ ì„ íƒ (ì „ì²´ {len(available_tools)}ê°œ ì¤‘)")
            logger.debug(f"Formatted tools: {json.dumps(formatted_tools[:1], indent=2)}")  # Log first tool as example
            
            tool_catalog = self._get_tool_categories_description(formatted_tools)
            system_prompt = f"""You are an AI assistant specialized in web analysis and data processing.

CRITICAL: Always use tools when available. Execute the appropriate tools to perform real work; avoid generic explanations without using tools.

Available tools (dynamic):
{tool_catalog}

Tool usage principles:
1) If a URL is present â†’ use crawl4ai_scrape or crawl_urls_sequential
2) For system status â†’ use health_check
3) For structured data conversion â†’ use convert_to_json_format
4) For menu/DB queries â†’ use menu_search
5) For HTML/Confluence content â†’ use ari_extract_main_blocks and/or ari_markdown_to_json and/or convert_to_json_format

Response policy:
- After using tools, produce a concise, high-signal answer based on the results.
- IMPORTANT: Write all final answers to the user in Korean.
"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ]
            
            # First API call with tools
            response = await self._client.chat.completions.create(
                model=settings.openai_model,
                messages=messages,
                tools=formatted_tools if formatted_tools else None,  # Only pass tools if available
                tool_choice="auto" if formatted_tools else None
            )
            
            # Check if tools were called
            message = response.choices[0].message
            
            if not message.tool_calls:
                return message.content or ""
            
            # Process tool calls
            messages.append(message.model_dump())  # Add assistant's message with tool calls
            
            # Execute each tool call
            for tool_call in message.tool_calls:
                function_name = tool_call.function.name
                function_args = json.loads(tool_call.function.arguments)
                
                logger.info(f"Executing tool: {function_name} with args: {function_args}")
                
                try:
                    # Call your MCP service
                    tool_result = await mcp_service.call_tool(function_name, function_args)
                    
                    # Format the result
                    if hasattr(tool_result, 'structured_content'):
                        result_content = json.dumps(tool_result.structured_content, ensure_ascii=False)
                    elif hasattr(tool_result, 'data'):
                        result_content = json.dumps(tool_result.data, ensure_ascii=False)
                    elif isinstance(tool_result, dict):
                        result_content = json.dumps(tool_result, ensure_ascii=False)
                    else:
                        result_content = str(tool_result)
                    
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": result_content
                    })
                except Exception as e:
                    logger.error(f"Tool execution failed for {function_name}: {e}")
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": json.dumps({"error": str(e)}, ensure_ascii=False)
                    })
            
            # Second API call for final response
            final_response = await self._client.chat.completions.create(
                model=settings.openai_model,
                messages=messages
            )
            
            return final_response.choices[0].message.content or ""
            
        except Exception as e:
            logger.error(f"LLM query failed: {e}")
            raise LLMQueryError(f"LLM ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    async def _filter_tools_by_intent(self, question: str, available_tools: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ì„ë² ë”© + LLM í”„ë¡¬í”„íŒ… ê¸°ë°˜ ì˜ë„ ë¶„ì„ìœ¼ë¡œ ê´€ë ¨ ë„êµ¬ í•„í„°ë§
        """
        try:
            # í•˜ì´ë¸Œë¦¬ë“œ ì˜ë„ ë¶„ë¥˜ ì‹¤í–‰
            detected_intents = await self._classify_intent_hybrid(question)
            
            # í´ë°±: ì˜ë„ê°€ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ ì‚¬ìš©
            if not detected_intents:
                logger.warning("ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜ ì‹¤íŒ¨, í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ì‚¬ìš©")
                detected_intents = self._fallback_keyword_intent(question)
            
            # ì˜ë„ë³„ ë„êµ¬ ë§¤í•‘ (í™•ì¥ëœ ë²„ì „)
            tool_mapping = {
                'web_crawling': [
                    'crawl4ai_scrape', 'crawl_urls_sequential', 'playwright_scrape',
                    'scrape_webpage', 'take_screenshot'
                ],
                'system_status': [
                    'health_check', 'get_system_stats', 'monitor_services'
                ],
                'text_analysis': [
                    'extract_keywords', 'analyze_sentiment',
                    'text_classification', 'content_analysis'
                ],
                'html_processing': [
                    'ari_extract_main_blocks', 'ari_markdown_to_json', 'convert_to_json_format',
                    'parse_html', 'extract_tables', 'convert_markdown'
                ],
                'db_query': [
                    'menu_search', 'search_database', 'query_menu_links',
                    'get_manager_info', 'search_records'
                ],
                'database_schema': [
                    'get_database_schema'
                ],
                'rag_query': [
                    'rag_search', 'vector_search', 'document_query',
                    'knowledge_base_search', 'semantic_search'
                ]
            }
            
            # ê´€ë ¨ ë„êµ¬ë“¤ ìˆ˜ì§‘
            relevant_tools = []
            for intent in detected_intents:
                relevant_tools.extend(tool_mapping.get(intent, []))
            
            # ìŠ¤ë§ˆíŠ¸ ê¸°ë³¸ ë„êµ¬ ì„ íƒ (ì˜ë„ì— ë”°ë¼ ë‹¤ë¥´ê²Œ)
            smart_essential_tools = []
            if 'web_crawling' in detected_intents or any('http' in question.lower() for protocol in ['http', 'www', '.com', '.kr']):
                smart_essential_tools.extend(['crawl4ai_scrape'])
            if 'system_status' in detected_intents or not detected_intents:
                smart_essential_tools.extend(['health_check'])
            if 'text_analysis' in detected_intents:
                smart_essential_tools.extend(['convert_to_json_format'])
                
            relevant_tools.extend(smart_essential_tools)
            
            # ì¤‘ë³µ ì œê±°
            relevant_tools = list(set(relevant_tools))
            
            # ë„êµ¬ í•„í„°ë§
            filtered_tools = []
            for tool in available_tools:
                tool_name = tool.get('function', {}).get('name', '')
                if tool_name in relevant_tools:
                    filtered_tools.append(tool)
            
            logger.info(f"ğŸ¯ ìµœì¢… í•„í„°ë§ ê²°ê³¼: {len(filtered_tools)}ê°œ ë„êµ¬ ì„ íƒ (ì „ì²´ {len(available_tools)}ê°œ ì¤‘)")
            logger.info(f"   ì„ íƒëœ ë„êµ¬: {[t.get('function', {}).get('name', '') for t in filtered_tools]}")
            
            return filtered_tools if filtered_tools else available_tools[:10]  # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
            
        except Exception as e:
            logger.error(f"ì˜ë„ ê¸°ë°˜ í•„í„°ë§ ì‹¤íŒ¨, ì „ì²´ ë„êµ¬ ì‚¬ìš©: {e}")
            return available_tools[:15]  # ì˜¤ë¥˜ ì‹œ ìƒìœ„ 15ê°œ ë„êµ¬ ì‚¬ìš©
    
    def _fallback_keyword_intent(self, question: str) -> List[str]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ì˜ë„ ë¶„ì„ (ê¸°ì¡´ ë°©ì‹)"""
        question_lower = question.lower()
        
        intent_keywords = {
            'web_crawling': ['í¬ë¡¤ë§', 'crawl', 'url', 'ì›¹ì‚¬ì´íŠ¸', 'ì›¹í˜ì´ì§€', 'ìŠ¤í¬ë˜í•‘', 'scrape', 'http'],
            'system_status': ['ìƒíƒœ', 'status', 'health', 'ì„œë²„', 'í†µê³„', 'statistics'],
            'text_analysis': ['ìš”ì•½', 'summarize', 'ë¶„ì„', 'analyze', 'ì¶”ì¶œ', 'extract', 'í…ìŠ¤íŠ¸'],
            'html_processing': ['html', 'íŒŒì¼', 'file', 'confluence', 'ì—…ë¡œë“œ', 'upload', 'ari'],
            'db_query': ['ë©”ë‰´', 'menu', 'ë§í¬', 'link', 'ë§¤ë‹ˆì €', 'manager', 'ë‹´ë‹¹ì', 'ì¡°íšŒ', 'query'],
            'database_schema': ['ìŠ¤í‚¤ë§ˆ', 'schema', 'êµ¬ì¡°', 'ì»¬ëŸ¼', 'column', 'í…Œì´ë¸”', 'table', 'í•„ë“œ', 'field', 'ë°ì´í„°êµ¬ì¡°'],
            'rag_query': ['rag', 'ê²€ìƒ‰', 'search', 'ë¬¸ì„œ', 'document', 'ì§€ì‹', 'knowledge']
        }
        
        detected_intents = []
        for intent, keywords in intent_keywords.items():
            if any(keyword in question_lower for keyword in keywords):
                detected_intents.append(intent)
        
        logger.info(f"ğŸ”„ í‚¤ì›Œë“œ ê¸°ë°˜ ì˜ë„ ë¶„ë¥˜: {detected_intents}")
        return detected_intents
    
    def _get_tool_categories_description(self, tools: List[Dict[str, Any]]) -> str:
        """
        ë„êµ¬ ì¹´í…Œê³ ë¦¬ë³„ ì„¤ëª… ìƒì„±
        """
        categories = {
            'web': [],
            'system': [],
            'text': [],
            'html': [],
            'other': []
        }
        
        for tool in tools:
            tool_name = tool.get('function', {}).get('name', '')
            tool_desc = tool.get('function', {}).get('description', '')
            
            if 'crawl' in tool_name or 'scrape' in tool_name:
                categories['web'].append(f"- {tool_name}: {tool_desc}")
            elif 'health' in tool_name or 'status' in tool_name:
                categories['system'].append(f"- {tool_name}: {tool_desc}")
            elif 'summarize' in tool_name or 'extract' in tool_name:
                categories['text'].append(f"- {tool_name}: {tool_desc}")
            elif 'html' in tool_name or 'table' in tool_name:
                categories['html'].append(f"- {tool_name}: {tool_desc}")
            else:
                categories['other'].append(f"- {tool_name}: {tool_desc}")
        
        description = ""
        if categories['web']:
            description += "**ì›¹ í¬ë¡¤ë§ ë„êµ¬:**\n" + "\n".join(categories['web']) + "\n\n"
        if categories['system']:
            description += "**ì‹œìŠ¤í…œ ìƒíƒœ ë„êµ¬:**\n" + "\n".join(categories['system']) + "\n\n"
        if categories['text']:
            description += "**í…ìŠ¤íŠ¸ ë¶„ì„ ë„êµ¬:**\n" + "\n".join(categories['text']) + "\n\n"
        if categories['html']:
            description += "**HTML ì²˜ë¦¬ ë„êµ¬:**\n" + "\n".join(categories['html']) + "\n\n"
        if categories['other']:
            description += "**ê¸°íƒ€ ë„êµ¬:**\n" + "\n".join(categories['other']) + "\n\n"
        
        return description
    
    async def generate_response(self, prompt: str) -> str:
        """
        Generate a simple response using OpenAI Chat Completions API
        
        Args:
            prompt: The prompt to send to the LLM
            
        Returns:
            Generated response as string
        """
        try:
            # í† í° ìˆ˜ ê³„ì‚° ë° ì œí•œ í™•ì¸
            tokens = self.tokenizer.encode(prompt)
            token_count = len(tokens)
            
            # RAGìš©ìœ¼ë¡œëŠ” gpt-4o-mini ì‚¬ìš© (ë” ì €ë ´í•˜ê³  í† í° ì œí•œì´ í¼)
            model = "gpt-4o-mini"  # RAG ì‘ë‹µ ìƒì„±ìš©
            
            # í† í° ì œí•œ í™•ì¸ (gpt-4o-miniëŠ” 128k context)
            max_tokens = 20000  # ë” ì•ˆì „í•œ ì œí•œ
            if token_count > max_tokens:
                logger.warning(f"Prompt too long ({token_count} tokens), truncating to {max_tokens}")
                tokens = tokens[:max_tokens]
                prompt = self.tokenizer.decode(tokens)
                token_count = max_tokens
            
            logger.info(f"LLM request: {token_count} tokens, model: {model}")
            
            response = await self._client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,  # ì‘ë‹µ í† í° ìˆ˜ ì¦ê°€
                temperature=0.7
            )
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Failed to generate response: {e}")
            raise LLMQueryError(f"Failed to generate response: {str(e)}")
    
    async def query_stream(self, question: str, available_tools: List[Dict[str, Any]]) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Execute a streaming query against the LLM with available tools
        
        Args:
            question: User's question
            available_tools: List of available MCP tools in OpenAI format
            
        Yields:
            Dict with event type and data (not SSE formatted strings)
        """
        try:
            # 1ë‹¨ê³„: ì˜ë„ ë¶„ë¥˜ ë° ë„êµ¬ í•„í„°ë§
            filtered_tools = await self._filter_tools_by_intent(question, available_tools)
            
            # 2ì°¨ ìš”ì²­ - ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
            logger.warning(f"ğŸ”§ LLM tools available: {len(filtered_tools)} tools (filtered from {len(available_tools)})")
            logger.warning(f"ğŸ”§ LLM question: {question[:100]}...")
            
            attempt = 0
            stream = None
            last_error: Exception | None = None
            while attempt < 2 and stream is None:
                try:
                    logger.warning(f"ğŸ”§ Creating OpenAI stream with {len(filtered_tools)} filtered tools")
                    stream = await self._client.responses.create(
                        model=settings.openai_model,
                        input=[
                            {"role": "system", "content": f"""You are an AI assistant specialized in web analysis and data processing.

CRITICAL: Always use tools when available. Execute the appropriate tools to perform real work; avoid generic explanations without using tools.

Available tools: {len(filtered_tools)} (filtered from {len(available_tools)} total)

Tool usage principles:
1) If a URL is present â†’ use crawl4ai_scrape or crawl_urls_sequential
2) For system status â†’ use health_check
3) For structured data conversion â†’ use convert_to_json_format
4) For menu/DB queries â†’ use menu_search
5) For HTML/Confluence content â†’ use ari_extract_main_blocks and/or ari_markdown_to_json and/or convert_to_json_format

Response policy:
- After using tools, produce a concise, high-signal answer based on the results.
- IMPORTANT: Write all final answers to the user in Korean.
"""},
                            {"role": "user", "content": question}
                        ],
                        tools=filtered_tools,
                        stream=True
                    )
                    logger.warning(f"ğŸ”§ OpenAI stream created successfully")
                except Exception as e:
                    last_error = e
                    attempt += 1
                    logger.warning(f"Streaming create failed (attempt {attempt}): {e}")
            if stream is None and last_error is not None:
                raise last_error

            tool_calls = []
            current_call = None
            
            async for chunk in stream:
                # OpenAI Responses API ì´ë²¤íŠ¸ ì²˜ë¦¬
                event_type = type(chunk).__name__
                logger.warning(f"ğŸ”§ Event: {event_type}")
                
                # Function call ì‹œì‘
                if event_type == 'ResponseOutputItemAddedEvent':
                    if hasattr(chunk, 'item') and hasattr(chunk.item, 'type') and chunk.item.type == 'function_call':
                        # chunk.itemì´ ì§ì ‘ ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆìŒ
                        tool_call = {
                            'id': chunk.item.id,
                            'name': chunk.item.name,
                            'arguments': chunk.item.arguments or ''
                        }
                        tool_calls.append(tool_call)
                        current_call = tool_call
                        logger.warning(f"ğŸ”§ Function call started: {chunk.item.name}")
                
                # Function call arguments ìˆ˜ì§‘
                elif event_type == 'ResponseFunctionCallArgumentsDeltaEvent':
                    if current_call and hasattr(chunk, 'delta'):
                        current_call['arguments'] += chunk.delta
                        logger.warning(f"ğŸ”§ Arguments delta: {chunk.delta[:20]}...")
                
                # Function call ì™„ë£Œ
                elif event_type == 'ResponseFunctionCallArgumentsDoneEvent':
                    if current_call:
                        logger.warning(f"ğŸ”§ Function call completed: {current_call['name']}")
                        yield f"data: {json.dumps({'type': 'tool_call_delta', 'data': {'tool_calls': tool_calls}})}\n\n"
                
                # ì „ì²´ ì‘ë‹µ ì™„ë£Œ
                elif event_type == 'ResponseCompletedEvent':
                    logger.warning(f"ğŸ”§ Response completed with {len(tool_calls)} tool calls")
                    break

            # Tool í˜¸ì¶œì´ ìˆëŠ” ê²½ìš° ì™„ì„±ëœ tool_calls ë°˜í™˜
            if tool_calls:
                yield f"data: {json.dumps({'type': 'tool_calls_ready', 'data': {'tool_calls': tool_calls}})}\n\n"
                async for event in self._process_tool_calls_stream(question, tool_calls):
                    if isinstance(event, dict):
                        yield f"data: {json.dumps(event)}\n\n"
                    else:
                        yield event
            
            yield f"data: {json.dumps({'type': 'done', 'data': {}})}\n\n"
            
        except Exception as e:
            # ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ: ìƒíƒœë¡œ ì•Œë¦¬ê³  ìƒìœ„ì—ì„œ í´ë°± ì§„í–‰
            logger.warning(f"Streaming LLM query failed, will fallback: {e}")
            yield f"data: {json.dumps({'type': 'error', 'data': {'message': f'LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}'}})}\n\n"
            yield f"data: {json.dumps({'type': 'done', 'data': {}})}\n\n"
    
    async def _process_tool_calls(self, question: str, tool_calls: List[Any]) -> List[Any]:
        """Process tool calls and return next input for LLM"""
        next_input: List[Any] = [{"role": "user", "content": question}]
        
        for call in tool_calls:
            args = call.arguments
            if isinstance(args, str):
                args = json.loads(args)
            
            result = await mcp_service.call_tool(call.name, args)

            # í˜¸ì¶œ ìì²´ë¥¼ ë©”ì‹œì§€ ë°°ì—´ì— ì¶”ê°€
            next_input.append(call)
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° ë„êµ¬ë“¤ì€ LLMì— ìš”ì•½ëœ ì •ë³´ë§Œ ì „ë‹¬
            if call.name == 'take_screenshot':
                result_str = str({
                    "success": True,
                    "message": "ìŠ¤í¬ë¦°ìƒ·ì´ ì„±ê³µì ìœ¼ë¡œ ì´¬ì˜ë˜ì—ˆìŠµë‹ˆë‹¤",
                    "screenshot_captured": True
                })
                logger.info(f"Screenshot tool result simplified for LLM: original {len(str(result)):,} chars -> {len(result_str)} chars")
            elif call.name == 'crawl_webpage':
                # HTML ì „ì²´ ëŒ€ì‹  ë©”íƒ€ ì •ë³´ë§Œ ì „ë‹¬
                try:
                    original_result = result.structured_content if hasattr(result, 'structured_content') else result.data
                    result_str = str({
                        "success": original_result.get("success", True),
                        "url": original_result.get("url", ""),
                        "title": original_result.get("title", ""),
                        "meta_description": original_result.get("meta_description", ""),
                        "content_length": original_result.get("content_length", 0),
                        "message": "ì›¹í˜ì´ì§€ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ (HTML ë°ì´í„°ëŠ” LLM ë¶„ì„ì— ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ìƒëµ)"
                    })
                    logger.info(f"Crawl webpage result simplified for LLM: original {len(str(result)):,} chars -> {len(result_str)} chars")
                except Exception as e:
                    logger.warning(f"Failed to simplify crawl result: {e}")
                    result_str = str(result)  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ì‚¬ìš©
            else:
                # ë‹¤ë¥¸ ë„êµ¬ë“¤ì˜ ê²°ê³¼ ë°ì´í„° í¬ê¸° ì œí•œ (OpenAI API ì œí•œ: 10MB)
                result_str = str(result)
                max_output_size = 8 * 1024 * 1024  # 8MBë¡œ ì•ˆì „í•˜ê²Œ ì œí•œ
                
                if len(result_str) > max_output_size:
                    # ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ìš”ì•½í•´ì„œ ì „ë‹¬
                    truncated_result = {
                        "success": True,
                        "message": f"ê²°ê³¼ê°€ ë„ˆë¬´ ì»¤ì„œ ìš”ì•½ë¨ (ì›ë³¸ í¬ê¸°: {len(result_str):,} ë¬¸ì)",
                        "truncated": True,
                        "sample": result_str[:1000] + "..." if len(result_str) > 1000 else result_str
                    }
                    result_str = str(truncated_result)
                    logger.warning(f"Tool result truncated for {call.name}: {len(str(result)):,} -> {len(result_str):,} chars")
            
            # ì‹¤í–‰ ê²°ê³¼ë¥¼ function_call_output í˜•ì‹ìœ¼ë¡œ ì¶”ê°€
            next_input.append({
                "type": "function_call_output",
                "call_id": call.call_id,
                "output": result_str,
            })
        
        return next_input
    
    async def _handle_streaming_tool_calls(self, delta_tool_calls, tool_calls, current_call):
        """Handle streaming tool calls from delta"""
        for tool_call in delta_tool_calls:
            if tool_call.index == len(tool_calls):
                tool_calls.append({
                    'id': tool_call.id,
                    'name': tool_call.function.name if tool_call.function else '',
                    'arguments': tool_call.function.arguments if tool_call.function else ''
                })
                current_call = tool_calls[-1]
            elif current_call:
                if tool_call.function and tool_call.function.arguments:
                    current_call['arguments'] += tool_call.function.arguments
        return current_call
    
    async def _process_tool_calls_stream(self, question: str, tool_calls: List[Dict]) -> AsyncGenerator[str, None]:
        """Process tool calls in streaming mode"""
        yield f"data: {json.dumps({'type': 'tool_start', 'data': {'message': 'Tool ì‹¤í–‰ ì¤‘...'}})}\n\n"
        
        next_input: List[Any] = [{"role": "user", "content": question}]
        
        # ê° íˆ´ í˜¸ì¶œ ì²˜ë¦¬
        for call in tool_calls:
            try:
                args = json.loads(call['arguments']) if call['arguments'] else {}
                result = await mcp_service.call_tool(call['name'], args)
                
                # Tool ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ì†¡ (ì›ë³¸ ê²°ê³¼ ì „ë‹¬)
                tool_message = f"Tool '{call['name']}' ì‹¤í–‰ ì™„ë£Œ"
                yield f"data: {json.dumps({'type': 'tool_result', 'data': {'tool_name': call['name'], 'message': tool_message, 'result': result}})}\n\n"
                
                # ë‹¤ìŒ ìš”ì²­ì„ ìœ„í•œ ë©”ì‹œì§€ êµ¬ì„±
                next_input.append({
                    "type": "function_call",
                    "call_id": call['id'],
                    "name": call['name'],
                    "arguments": call['arguments']
                })
                                # ëŒ€ìš©ëŸ‰ ë°ì´í„° ë„êµ¬ë“¤ì€ LLMì— ìš”ì•½ëœ ì •ë³´ë§Œ ì „ë‹¬
                if call['name'] == 'take_screenshot':
                    result_str = str({
                        "success": True,
                        "message": "ìŠ¤í¬ë¦°ìƒ·ì´ ì„±ê³µì ìœ¼ë¡œ ì´¬ì˜ë˜ì—ˆìŠµë‹ˆë‹¤",
                        "screenshot_captured": True
                    })
                    logger.info(f"Screenshot tool result simplified for LLM: original {len(str(result)):,} chars -> {len(result_str)} chars")
                elif call['name'] == 'crawl_webpage':
                    # HTML ì „ì²´ ëŒ€ì‹  ë©”íƒ€ ì •ë³´ë§Œ ì „ë‹¬
                    try:
                        original_result = result.structured_content if hasattr(result, 'structured_content') else result.data
                        result_str = str({
                            "success": original_result.get("success", True),
                            "url": original_result.get("url", ""),
                            "title": original_result.get("title", ""),
                            "meta_description": original_result.get("meta_description", ""),
                            "content_length": original_result.get("content_length", 0),
                            "message": "ì›¹í˜ì´ì§€ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ (HTML ë°ì´í„°ëŠ” LLM ë¶„ì„ì— ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ìƒëµ)"
                        })
                        logger.info(f"Crawl webpage result simplified for LLM: original {len(str(result)):,} chars -> {len(result_str)} chars")
                    except Exception as e:
                        logger.warning(f"Failed to simplify crawl result: {e}")
                        result_str = str(result)  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ì‚¬ìš©
                else:
                    # ë‹¤ë¥¸ ë„êµ¬ë“¤ì˜ ê²°ê³¼ ë°ì´í„° í¬ê¸° ì œí•œ (OpenAI API ì œí•œ: 10MB)
                    result_str = str(result)
                    max_output_size = 8 * 1024 * 1024  # 8MBë¡œ ì•ˆì „í•˜ê²Œ ì œí•œ
                    
                    if len(result_str) > max_output_size:
                        # ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ìš”ì•½í•´ì„œ ì „ë‹¬
                        truncated_result = {
                            "success": True,
                            "message": f"ê²°ê³¼ê°€ ë„ˆë¬´ ì»¤ì„œ ìš”ì•½ë¨ (ì›ë³¸ í¬ê¸°: {len(result_str):,} ë¬¸ì)",
                            "truncated": True,
                            "sample": result_str[:1000] + "..." if len(result_str) > 1000 else result_str
                        }
                        result_str = str(truncated_result)
                        logger.warning(f"Tool result truncated for {call['name']}: {len(str(result)):,} -> {len(result_str):,} chars")
                
                next_input.append({
                    "type": "function_call_output", 
                    "call_id": call['id'],
                    "output": result_str
                })
                
            except Exception as tool_error:
                error_message = f'Tool ì‹¤í–‰ ì˜¤ë¥˜: {str(tool_error)}'
                yield f"data: {json.dumps({'type': 'tool_error', 'data': {'tool_name': call['name'], 'error': error_message}})}\n\n"

        # 2ì°¨ í˜¸ì¶œ - ìµœì¢… ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë°
        final_stream = await self._client.responses.create(
            model=settings.openai_model,
            input=next_input,
            stream=True
        )
        
        async for chunk in final_stream:
            if hasattr(chunk, 'choices') and chunk.choices:
                choice = chunk.choices[0]
                if hasattr(choice, 'delta') and choice.delta and hasattr(choice.delta, 'content') and choice.delta.content:
                    yield f"data: {json.dumps({'type': 'final_content', 'data': {'content': choice.delta.content}})}\n\n"

# Global service instance
llm_service = LLMService()
