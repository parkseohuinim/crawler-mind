"""RAG Application Service - orchestrates RAG operations"""
import json
import logging
import time
from typing import List, Dict, Any, Optional
from datetime import datetime

from app.domains.rag.entities.document import Document
from app.domains.rag.schemas.rag_schemas import (
    RagUploadResponse, RagQueryRequest, RagQueryResponse, DocumentChunk
)
from app.infrastructure.vectordb.qdrant_service import get_qdrant_service
from app.infrastructure.search.opensearch_service import get_opensearch_service
from app.infrastructure.llm.llm_service import llm_service

logger = logging.getLogger(__name__)


class RagApplicationService:
    """Application service for managing RAG operations"""
    
    def __init__(self):
        self.qdrant_service = None
        self.opensearch_service = None
        self.llm_service = llm_service
    
    def _get_services(self):
        """Lazy initialization of services"""
        if self.qdrant_service is None:
            self.qdrant_service = get_qdrant_service()
        if self.opensearch_service is None:
            self.opensearch_service = get_opensearch_service()
        return self.qdrant_service, self.opensearch_service
    
    async def initialize_services(self):
        """Initialize all RAG services"""
        try:
            qdrant_service, opensearch_service = self._get_services()
            await qdrant_service.initialize_collection()
            await opensearch_service.initialize_index()
            logger.info("RAG services initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize RAG services: {e}")
            raise
    
    async def upload_documents_from_json(self, json_data: List[Dict[str, Any]]) -> RagUploadResponse:
        """Upload documents from JSON data"""
        start_time = time.time()
        
        try:
            # Convert JSON data to Document entities
            documents = []
            for i, item in enumerate(json_data):
                try:
                    document = Document.from_json_data(item)
                    documents.append(document)
                    
                    # ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ë‚´ìš© í™•ì¸ (ë””ë²„ê¹…ìš©)
                    if i == 0:
                        logger.info(f"ğŸ” First document sample:")
                        logger.info(f"   ID: {document.id}")
                        logger.info(f"   Title: {document.title}")
                        logger.info(f"   Content length: {len(document.content)}")
                        logger.info(f"   Content preview: {document.content[:200]}...")
                    
                    if (i + 1) % 100 == 0:  # 100ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ë¡œê·¸
                        logger.info(f"Parsed {i + 1}/{len(json_data)} documents...")
                except Exception as e:
                    logger.warning(f"Failed to parse document {item.get('docId', 'unknown')}: {e}")
            
            logger.info(f"Successfully parsed {len(documents)} documents from JSON")
            
            # Store in both Qdrant and OpenSearch
            qdrant_service, opensearch_service = self._get_services()
            
            logger.info("Starting Qdrant storage...")
            qdrant_result = await qdrant_service.store_documents(documents)
            logger.info(f"Qdrant storage completed: {qdrant_result}")
            
            logger.info("Starting OpenSearch storage...")
            opensearch_result = await opensearch_service.store_documents(documents)
            logger.info(f"OpenSearch storage completed: {opensearch_result}")
            
            # Combine results
            total_processed = len(documents)
            total_failed = max(qdrant_result["failed_count"], opensearch_result["failed_count"])
            total_success = total_processed - total_failed
            
            failed_docs = list(set(
                qdrant_result["failed_documents"] + opensearch_result["failed_documents"]
            ))
            
            processing_time = time.time() - start_time
            logger.info(f"ğŸ“Š Document upload completed in {processing_time:.2f}s:")
            logger.info(f"   ğŸ“„ Original documents: {len(documents)}")
            logger.info(f"   âœ… Successfully processed: {total_success}")
            logger.info(f"   âŒ Failed: {total_failed}")
            logger.info(f"   ğŸ” Qdrant chunks: {qdrant_result.get('success_count', 0)}")
            logger.info(f"   ğŸ“š OpenSearch docs: {opensearch_result.get('success_count', 0)}")
            logger.info(f"   â±ï¸  Processing time: {processing_time:.2f}s")
            
            return RagUploadResponse(
                message=f"Documents uploaded successfully in {processing_time:.2f} seconds",
                processed_count=total_success,
                failed_count=total_failed,
                failed_documents=failed_docs
            )
            
        except Exception as e:
            logger.error(f"Failed to upload documents: {e}")
            raise
    
    async def query_documents(self, request: RagQueryRequest) -> RagQueryResponse:
        """Query documents using RAG approach"""
        start_time = time.time()
        
        try:
            qdrant_service, opensearch_service = self._get_services()
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
            search_queries = self._extract_search_queries(request.query)
            logger.info(f"ğŸ” Search queries: {search_queries}")
            
            # Step 1: Vector similarity search with Qdrant
            # ìœ ì‚¬ë„ ì„ê³„ê°’ì„ ë‚®ì¶°ì„œ ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê¸°
            vector_results = await qdrant_service.search_similar_documents(
                query=search_queries[0],  # ì²« ë²ˆì§¸ í‚¤ì›Œë“œ ì‚¬ìš©
                limit=request.max_results * 3,  # ë” ë§ì€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (3ë°°)
                score_threshold=0.0  # ì„ê³„ê°’ ì œê±°
            )
            
            # Step 2: Text search with OpenSearch
            text_results = await opensearch_service.search_documents(
                query=search_queries[0],  # ì²« ë²ˆì§¸ í‚¤ì›Œë“œ ì‚¬ìš©
                limit=request.max_results * 2  # ë” ë§ì€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            )
            
            # Step 3: Combine and deduplicate results
            combined_results = self._combine_search_results(vector_results, text_results)
            
            logger.info(f"ğŸ” Search results summary:")
            logger.info(f"   ğŸ“Š Vector results: {len(vector_results)}")
            logger.info(f"   ğŸ“Š Text results: {len(text_results)}")
            logger.info(f"   ğŸ“Š Combined results: {len(combined_results)}")
            
            # ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë¡œê·¸
            for i, result in enumerate(combined_results[:5]):  # ìƒìœ„ 5ê°œ ë¡œê·¸
                logger.info(f"   ğŸ“„ Result {i+1}: id={result.get('id', 'unknown')}, score={result.get('combined_score', 0):.2f}")
                if 'payload' in result:
                    payload = result['payload']
                    logger.info(f"      Title: {payload.get('title', 'No title')}")
                    logger.info(f"      Content length: {len(payload.get('content', ''))}")
                    # í™ˆì½”ë…¸ë¯¸ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
                    if 'í™ˆì½”ë…¸ë¯¸' in payload.get('title', '') or 'í™ˆì½”ë…¸ë¯¸' in payload.get('content', ''):
                        logger.info(f"      ğŸ¯ FOUND HOMECONOMY DOCUMENT!")
                elif 'source' in result:
                    source = result['source']
                    logger.info(f"      Title: {source.get('title', 'No title')}")
                    logger.info(f"      Content length: {len(source.get('content', ''))}")
                    # í™ˆì½”ë…¸ë¯¸ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
                    if 'í™ˆì½”ë…¸ë¯¸' in source.get('title', '') or 'í™ˆì½”ë…¸ë¯¸' in source.get('content', ''):
                        logger.info(f"      ğŸ¯ FOUND HOMECONOMY DOCUMENT!")
            
            # Step 4: Prepare context for LLM with token limit
            context_chunks = []
            sources = []
            max_context_tokens = 15000  # ë” ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ LLMì´ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡
            
            # í† í° ì¹´ìš´í„° ì´ˆê¸°í™”
            total_tokens = 0
            
            # Qdrant ê²°ê³¼ì—ì„œ í™ˆì½”ë…¸ë¯¸ ê´€ë ¨ ë¬¸ì„œ ìš°ì„  í¬í•¨
            homeco_results = [r for r in combined_results if 'í™ˆì½”ë…¸ë¯¸' in str(r.get('payload', {}).get('title', '')) or 'í™ˆì½”ë…¸ë¯¸' in str(r.get('source', {}).get('title', ''))]
            other_results = [r for r in combined_results if r not in homeco_results]
            
            # í™ˆì½”ë…¸ë¯¸ ë¬¸ì„œë¥¼ ìš°ì„ ì ìœ¼ë¡œ í¬í•¨
            prioritized_results = homeco_results + other_results
            logger.info(f"ğŸ¯ Prioritized results: {len(homeco_results)} homeco + {len(other_results)} others")
            
            # ë¨¼ì € ëª¨ë“  ì ìˆ˜ë¥¼ ìˆ˜ì§‘í•˜ì—¬ OpenSearch ì •ê·œí™” ë²”ìœ„ íŒŒì•…
            opensearch_scores = []
            
            for result in prioritized_results[:request.max_results * 2]:
                if 'source' in result:  # OpenSearch result
                    opensearch_scores.append(result['score'])
            
            # OpenSearch ì ìˆ˜ ë²”ìœ„ ê³„ì‚° (QdrantëŠ” ì´ë¯¸ 0~1 ë²”ìœ„ì´ë¯€ë¡œ ì •ê·œí™” ë¶ˆí•„ìš”)
            opensearch_max = max(opensearch_scores) if opensearch_scores else 1.0
            opensearch_min = min(opensearch_scores) if opensearch_scores else 0.0
            
            logger.info(f"ğŸ“Š Score ranges - OpenSearch: min={opensearch_min:.3f}, max={opensearch_max:.3f}")
            
            for result in prioritized_results[:request.max_results * 2]:  # ë” ë§ì€ ê²°ê³¼ í™•ì¸
                if 'payload' in result:  # Qdrant result
                    chunk_data = result['payload']
                    raw_score = result['score']
                    # Qdrant ì½”ì‚¬ì¸ ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (ì´ë¯¸ 0~1 ë²”ìœ„ì´ë¯€ë¡œ ì¶”ê°€ ì •ê·œí™” ë¶ˆí•„ìš”)
                    normalized_score = max(0.0, 1.0 - raw_score)
                elif 'source' in result:  # OpenSearch result
                    chunk_data = result['source']
                    raw_score = result['score']
                    # OpenSearch ì ìˆ˜ë¥¼ Min-Max ì •ê·œí™”ë¡œ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
                    if opensearch_max > opensearch_min:
                        normalized_score = (raw_score - opensearch_min) / (opensearch_max - opensearch_min + 1e-6)
                        normalized_score = max(0.0, min(1.0, normalized_score))
                    else:
                        normalized_score = 0.5  # fallback
                else:
                    continue
                
                # ì²­í¬ í…ìŠ¤íŠ¸ ìƒì„±
                title = chunk_data.get('title', '')
                content = chunk_data.get('content', '')
                chunk_text = f"Title: {title}\nContent: {content}"
                
                # ë¬¸ì„œ ë‚´ìš© ë””ë²„ê¹… ë¡œê·¸
                logger.info(f"ğŸ“„ Document chunk: title='{title}', content_length={len(content)}")
                if content:
                    logger.info(f"ğŸ“ Content preview: {content[:300]}...")
                else:
                    logger.warning(f"âš ï¸ Empty content for document: {chunk_data.get('id', 'unknown')}")
                
                # í† í° ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ ì¶”ì •)
                estimated_tokens = len(chunk_text.split()) * 1.3  # ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì •
                
                # í† í° ì œí•œ í™•ì¸
                if total_tokens + estimated_tokens > max_context_tokens:
                    logger.warning(f"Context token limit reached ({total_tokens:.0f} tokens), stopping at {len(context_chunks)} chunks")
                    break
                
                context_chunks.append(chunk_text)
                total_tokens += estimated_tokens
                
                # ê²€ìƒ‰ ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
                search_source = "vector" if 'payload' in result else "text"
                raw_score = result['score']
                
                # ì›ë³¸ ì ìˆ˜ ê·¸ëŒ€ë¡œ í‘œì‹œ (ì¼ê´€ì„± ìœ ì§€)
                if search_source == "vector":
                    # Qdrant: ì½”ì‚¬ì¸ ê±°ë¦¬ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    display_score = raw_score
                    score_label = "ì½”ì‚¬ì¸ ê±°ë¦¬"
                else:
                    # OpenSearch: BM25 ì ìˆ˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    display_score = raw_score
                    score_label = "BM25 ì ìˆ˜"
                
                sources.append({
                    "id": result.get('id', ''),
                    "title": chunk_data.get('title', ''),
                    "url": chunk_data.get('url', ''),
                    "hierarchy": chunk_data.get('hierarchy', []),
                    "similarity_score": display_score,
                    "score_label": score_label,
                    "search_source": search_source,
                    "raw_score": raw_score,
                    "search_method": "Qdrant (ë²¡í„° ê²€ìƒ‰)" if search_source == "vector" else "OpenSearch (í…ìŠ¤íŠ¸ ê²€ìƒ‰)"
                })
            
            logger.info(f"Prepared context: {len(context_chunks)} chunks, ~{total_tokens:.0f} tokens")
            
            # Contextì— í¬í•¨ëœ ë¬¸ì„œë“¤ ë””ë²„ê¹… ë¡œê·¸
            logger.info(f"ğŸ“‹ Context documents:")
            for i, (chunk, source) in enumerate(zip(context_chunks, sources)):
                logger.info(f"   {i+1}. {source['title']} (normalized_score: {source['similarity_score']:.3f})")
                if 'í™ˆì½”ë…¸ë¯¸' in source['title'] or 'í™ˆì½”ë…¸ë¯¸' in chunk:
                    logger.info(f"      ğŸ¯ HOMECONOMY FOUND IN CONTEXT!")
            
            # Step 5: Generate answer using LLM
            if context_chunks:
                context = "\n\n---\n\n".join(context_chunks)
                logger.info(f"ğŸ” Context prepared for LLM: {len(context_chunks)} chunks, {len(context)} characters")
                logger.info(f"ğŸ“ First chunk preview: {context_chunks[0][:200]}..." if context_chunks else "No chunks")
                answer = await self._generate_answer(request.query, context)
            else:
                logger.warning("âŒ No context chunks found for query")
                answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                sources = []
            
            processing_time = time.time() - start_time
            
            logger.info(f"RAG query completed in {processing_time:.2f}s: "
                       f"found {len(sources)} relevant sources")
            
            return RagQueryResponse(
                answer=answer,
                sources=sources,
                query=request.query,
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"Failed to process RAG query: {e}")
            raise
    
    def _combine_search_results(
        self, 
        vector_results: List[Dict[str, Any]], 
        text_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Combine and deduplicate search results from different sources"""
        combined = {}
        
        # Add vector results
        for result in vector_results:
            doc_id = result['id']
            combined[doc_id] = {
                **result,
                'source_type': 'vector',
                'combined_score': result['score']
            }
        
        # Add text results, boost score if already exists
        for result in text_results:
            doc_id = result['id']
            if doc_id in combined:
                # Document found in both searches, boost the score
                combined[doc_id]['combined_score'] = (
                    combined[doc_id]['score'] * 0.7 + result['score'] * 0.3
                )
                combined[doc_id]['source_type'] = 'both'
            else:
                combined[doc_id] = {
                    **result,
                    'source_type': 'text',
                    'combined_score': result['score']
                }
        
        # Sort by combined score
        sorted_results = sorted(
            combined.values(), 
            key=lambda x: x['combined_score'], 
            reverse=True
        )
        
        return sorted_results
    
    def _extract_search_queries(self, query: str) -> List[str]:
        """ìì—°ì–´ ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê¸°ë³¸ ì¿¼ë¦¬
        queries = [query]
        
        # íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° ë‹¨ìˆœí™”
        if "í™ˆì½”ë…¸ë¯¸" in query:
            queries.append("í™ˆì½”ë…¸ë¯¸")
            queries.append("í™ˆì½”ë…¸ë¯¸ìº í˜ì¸")
        
        if "eSIM" in query:
            queries.append("eSIM")
            queries.append("eSIMì´ë™")
        
        if "ë“€ì–¼ë²ˆí˜¸" in query:
            queries.append("ë“€ì–¼ë²ˆí˜¸")
            queries.append("ë“€ì–¼ë²ˆí˜¸ê°€ì…")
        
        if "ë°ì´í„°ì‰ì–´ë§" in query:
            queries.append("ë°ì´í„°ì‰ì–´ë§")
            queries.append("ë°ì´í„°ì‰ì–´ë§ê°€ì…")
        
        # ì¤‘ë³µ ì œê±°
        return list(dict.fromkeys(queries))
    
    async def _generate_answer(self, query: str, context: str) -> str:
        """Generate answer using LLM with retrieved context"""
        try:
            prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query}

ê´€ë ¨ ì •ë³´:
{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ êµ¬ì²´ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”:"""

            logger.info(f"ğŸ¤– Sending prompt to LLM: {len(prompt)} characters")
            logger.info(f"ğŸ“‹ Query: {query}")
            logger.info(f"ğŸ“„ Context length: {len(context)} characters")
            
            # Use the existing LLM service
            response = await self.llm_service.generate_response(prompt)
            logger.info(f"âœ… LLM response received: {len(response)} characters")
            logger.info(f"ğŸ’¬ Response preview: {response[:200]}...")
            return response
            
        except Exception as e:
            logger.error(f"Failed to generate LLM answer: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    async def delete_all_data(self) -> Dict[str, Any]:
        """Delete all RAG data from both Qdrant and OpenSearch"""
        start_time = time.time()
        
        try:
            qdrant_service, opensearch_service = self._get_services()
            
            # Delete from both services
            qdrant_result = await qdrant_service.delete_all_documents()
            opensearch_result = await opensearch_service.delete_all_documents()
            
            processing_time = time.time() - start_time
            
            # Check if both operations were successful
            both_success = qdrant_result.get("success", False) and opensearch_result.get("success", False)
            
            if both_success:
                message = "ëª¨ë“  RAG ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
                logger.info(f"All RAG data deleted successfully in {processing_time:.2f}s")
            else:
                message = f"ì¼ë¶€ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: Qdrant({qdrant_result.get('message', '')}), OpenSearch({opensearch_result.get('message', '')})"
                logger.warning(f"Partial deletion failure in {processing_time:.2f}s")
            
            return {
                "success": both_success,
                "message": message,
                "processing_time": processing_time,
                "details": {
                    "qdrant": qdrant_result,
                    "opensearch": opensearch_result
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to delete RAG data: {e}")
            return {
                "success": False,
                "message": f"ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "processing_time": time.time() - start_time
            }
    
    async def get_data_info(self) -> Dict[str, Any]:
        """Get information about stored RAG data"""
        try:
            qdrant_service, opensearch_service = self._get_services()
            
            # Get info from both services
            qdrant_info = await qdrant_service.get_collection_info()
            opensearch_info = await opensearch_service.get_index_info()
            
            qdrant_count = qdrant_info.get("points_count", 0) if qdrant_info.get("success") else 0
            opensearch_count = opensearch_info.get("document_count", 0) if opensearch_info.get("success") else 0
            
            return {
                "success": True,
                "qdrant": qdrant_info,
                "opensearch": opensearch_info,
                "summary": {
                    "qdrant_documents": qdrant_count,
                    "opensearch_documents": opensearch_count,
                    "chunk_ratio": round(qdrant_count / opensearch_count, 2) if opensearch_count > 0 else 0,
                    "status": "âœ… Both services healthy" if qdrant_info.get("success") and opensearch_info.get("success") else "âš ï¸ Some services have issues"
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to get data info: {e}")
            return {
                "success": False,
                "message": f"ë°ì´í„° ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }


# Global instance
rag_service = RagApplicationService()
