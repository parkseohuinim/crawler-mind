"""
Daily Crawling Service

input_urls í…Œì´ë¸”ì—ì„œ URLì„ ì¡°íšŒí•˜ì—¬ í¬ë¡¤ë§í•˜ê³ ,
ì „ì²˜ë¦¬ í›„ menu_links í…Œì´ë¸”ì— ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
ìµœì¢… ê²°ê³¼ëŠ” data_*.json í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.
"""
import asyncio
import json
import logging
import re
import unicodedata
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any, AsyncGenerator, Dict, List, Optional

from sqlalchemy import select, or_

from app.application.crawler.tools_client import crawler_tools
from app.application.crawler.page_handlers import (
    route_url,
    get_handler_for_url,
    page_handler_client,
)
from app.application.crawler.preprocess import preprocess_content
from app.domains.crawler.entities.input_url import InputUrl
from app.domains.crawler.repositories.input_url_repository import input_url_repository
from app.domains.menu.entities.menu_link import MenuLink
from app.models import TaskResult, TaskStatus, CrawlingResult, FailedItem
from app.shared.database.base import get_database_session

logger = logging.getLogger(__name__)

# ê²°ê³¼ ì €ì¥ ê²½ë¡œ
RESULT_DIR = Path(__file__).parent / "result"
JSON_START_DATE = "1900-01-01"
JSON_END_DATE = "2999-12-31"


def _setup_asyncio_exception_handler():
    """
    asyncio ì´ë²¤íŠ¸ ë£¨í”„ì˜ exception handler ì„¤ì •
    Playwright TargetClosedError ë“± íƒ€ì„ì•„ì›ƒ ì‹œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸ë¥¼ ë¬´ì‹œ
    """
    def handle_exception(loop, context):
        exception = context.get("exception")
        message = context.get("message", "")
        
        # TargetClosedErrorëŠ” íƒ€ì„ì•„ì›ƒ ì‹œ ì •ìƒì ìœ¼ë¡œ ë°œìƒí•˜ë¯€ë¡œ ë¬´ì‹œ
        if exception:
            exc_name = type(exception).__name__
            if exc_name in ("TargetClosedError", "CancelledError"):
                logger.debug(f"Ignored async exception: {exc_name}")
                return
        
        # ê·¸ ì™¸ ì˜ˆì™¸ëŠ” ê¸°ë³¸ í•¸ë“¤ëŸ¬ë¡œ ì²˜ë¦¬
        logger.warning(f"âš ï¸ Async exception: {message} - {exception}")
    
    try:
        loop = asyncio.get_running_loop()
        loop.set_exception_handler(handle_exception)
    except RuntimeError:
        # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ
        pass


class DailyCrawlingService:
    """
    Daily Crawling ì„œë¹„ìŠ¤
    
    input_urls í…Œì´ë¸”ì—ì„œ í™œì„±í™”ëœ URLì„ ì¡°íšŒí•˜ì—¬ í¬ë¡¤ë§í•˜ê³ ,
    ì „ì²˜ë¦¬ í›„ menu_links í…Œì´ë¸”ì— ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    ìµœì¢… ê²°ê³¼ëŠ” data_*.json í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.
    """
    
    def __init__(self) -> None:
        self.tasks: Dict[str, TaskResult] = {}
        self.task_streams: Dict[str, asyncio.Queue] = {}
        self._collected_results: Dict[str, List[Dict[str, Any]]] = {}  # taskë³„ ê²°ê³¼ ìˆ˜ì§‘
        self._failed_items: Dict[str, List[FailedItem]] = {}  # taskë³„ ì‹¤íŒ¨ ë‚´ì—­ ìˆ˜ì§‘
    
    # ----------------------------------------------------------------------------------
    # Public APIs
    # ----------------------------------------------------------------------------------
    def create_task(
        self, 
        force_recrawl: bool = False, 
        limit: Optional[int] = None,
        url_ids: Optional[List[int]] = None,
        mode: str = "sequential",
        concurrency: int = 3,
        update_menu_links: bool = True
    ) -> str:
        """
        Daily Crawling íƒœìŠ¤í¬ ìƒì„±
        
        Args:
            force_recrawl: ì´ë¯¸ ì„±ê³µí•œ URLë„ ì¬í¬ë¡¤ë§
            limit: ìµœëŒ€ URL ìˆ˜ (url_idsê°€ ìˆìœ¼ë©´ ë¬´ì‹œ)
            url_ids: íŠ¹ì • input_urls ID ëª©ë¡ (í…ŒìŠ¤íŠ¸ìš©)
            mode: ì‹¤í–‰ ëª¨ë“œ ("sequential" ë˜ëŠ” "parallel")
            concurrency: ë³‘ë ¬ ì‹¤í–‰ ì‹œ ë™ì‹œ ì²˜ë¦¬ ìˆ˜ (1~50, ê¸°ë³¸ê°’: 3)
            update_menu_links: menu_links DB ì—…ë°ì´íŠ¸ ì—¬ë¶€ (ê¸°ë³¸ê°’ True)
            
        Returns:
            task_id
        """
        task_id = str(uuid.uuid4())
        task_result = TaskResult(
            taskId=task_id,
            status=TaskStatus.PENDING,
            createdAt=datetime.now().isoformat(),
        )
        self.tasks[task_id] = task_result
        self.task_streams[task_id] = asyncio.Queue()
        self._collected_results[task_id] = []
        self._failed_items[task_id] = []
        
        # concurrency ë²”ìœ„ ì œí•œ
        concurrency = max(1, min(10, concurrency))
        
        if url_ids:
            logger.info(f"âœ… Task created: {task_id} (url_ids={url_ids}, mode={mode})")
        else:
            logger.info(f"âœ… Task created: {task_id} (mode={mode}, concurrency={concurrency}, update_menu_links={update_menu_links})")
        asyncio.create_task(self._process_daily_task(task_id, force_recrawl, limit, url_ids, mode, concurrency, update_menu_links))
        
        return task_id
    
    def get_task(self, task_id: str) -> Optional[TaskResult]:
        """íƒœìŠ¤í¬ ì¡°íšŒ"""
        return self.tasks.get(task_id)
    
    def get_tasks(self, limit: int = 10) -> List[TaskResult]:
        """ìµœê·¼ íƒœìŠ¤í¬ ëª©ë¡ ì¡°íšŒ"""
        # ìƒì„± ì‹œê°„ ì—­ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
        sorted_tasks = sorted(
            self.tasks.values(), 
            key=lambda x: x.createdAt, 
            reverse=True
        )
        return sorted_tasks[:limit]
    
    async def get_task_stream(self, task_id: str) -> AsyncGenerator[str, None]:
        """SSE ìŠ¤íŠ¸ë¦¼ ìƒì„±"""
        logger.info(f"ğŸ” SSE stream requested: {task_id}")
        
        if task_id not in self.task_streams:
            # íƒœìŠ¤í¬ê°€ ì•„ì§ ì‹¤í–‰ ì¤‘ì´ë¼ë©´ íë¥¼ ë‹¤ì‹œ ìƒì„± (ë³µêµ¬/ì¬ì—°ê²° ëŒ€ì‘)
            task = self.tasks.get(task_id)
            if task and task.status in {TaskStatus.PENDING, TaskStatus.RUNNING}:
                logger.info(f"ğŸ”„ Re-creating stream queue for active task: {task_id}")
                self.task_streams[task_id] = asyncio.Queue()
            else:
                logger.error(f"âŒ Stream not found: {task_id}")
                yield f"data: {json.dumps({'type': 'error', 'data': {'message': 'Task not found or already finished'}})}\n\n"
                return
        
        yield f"data: {json.dumps({'type': 'connected', 'data': {'message': 'Daily Crawling Stream connected'}})}\n\n"
        
        queue = self.task_streams[task_id]
        try:
            while True:
                try:
                    message = await asyncio.wait_for(queue.get(), timeout=30.0)
                    yield f"data: {message}\n\n"
                    
                    try:
                        payload = json.loads(message)
                        if payload.get("type") in {"final", "complete", "error"}:
                            # í´ë¼ì´ì–¸íŠ¸ê°€ ë©”ì‹œì§€ë¥¼ ë°›ì„ ìˆ˜ ìˆë„ë¡ ì¶©ë¶„íˆ ëŒ€ê¸°
                            await asyncio.sleep(2.0)
                            break
                    except json.JSONDecodeError:
                        pass
                    
                    task = self.tasks.get(task_id)
                    if task and task.status in {TaskStatus.COMPLETED, TaskStatus.FAILED}:
                        break
                        
                except asyncio.TimeoutError:
                    await self._send_update(task_id, "heartbeat", {})
                    task = self.tasks.get(task_id)
                    if task and task.status in {TaskStatus.COMPLETED, TaskStatus.FAILED}:
                        break
                        
        except Exception as exc:
            logger.error(f"âŒ Stream error {task_id}: {exc}")
            # ì´ë¯¸ ë‹«íŒ ìŠ¤íŠ¸ë¦¼ì— ì—ëŸ¬ë¥¼ ë³´ë‚¼ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë¡œê·¸ë§Œ ë‚¨ê¹€
        finally:
            logger.info(f"SSE connection closed: {task_id}")
            # ì´ ì—°ê²°ì´ ì¢…ë£Œë˜ì—ˆë‹¤ê³  í•´ì„œ ë‹¤ë¥¸ í´ë¼ì´ì–¸íŠ¸ë¥¼ ìœ„í•œ íë¥¼ ì‚­ì œí•˜ì§€ ì•ŠìŒ
            # í ì‚­ì œëŠ” íƒœìŠ¤í¬ê°€ ì™„ë£Œëœ í›„ _process_daily_taskì˜ ë§ˆì§€ë§‰ì´ë‚˜ ë³„ë„ ê´€ë¦¬ ë£¨í‹´ì—ì„œ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì•ˆì „í•¨
            
            task = self.tasks.get(task_id)
            if task and task.status in {TaskStatus.COMPLETED, TaskStatus.FAILED}:
                # íƒœìŠ¤í¬ê°€ ì´ë¯¸ ì¢…ë£Œëœ ìƒíƒœì—ì„œ ì—°ê²°ì´ ëŠê¸´ ê²½ìš°ì—ë§Œ ì •ë¦¬ ê³ ë ¤
                # ë‹¨, ì—¬ëŸ¬ í´ë¼ì´ì–¸íŠ¸ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‹ ì¤‘í•´ì•¼ í•¨
                pass
    
    # ----------------------------------------------------------------------------------
    # Core Workflow
    # ----------------------------------------------------------------------------------
    async def _process_daily_task(
        self, 
        task_id: str, 
        force_recrawl: bool,
        limit: Optional[int],
        url_ids: Optional[List[int]] = None,
        mode: str = "sequential",
        concurrency: int = 5,
        update_menu_links: bool = True
    ) -> None:
        """Daily Crawling íƒœìŠ¤í¬ ì²˜ë¦¬"""
        # íƒ€ì„ì•„ì›ƒ ì‹œ TargetClosedError ë“± ë¬´ì‹œí•˜ë„ë¡ ì„¤ì •
        _setup_asyncio_exception_handler()
        
        try:
            self.tasks[task_id].status = TaskStatus.RUNNING
            mode_text = "ë³‘ë ¬" if mode == "parallel" else "ìˆœì°¨"
            await self._send_update(task_id, "status", {
                "message": f"Daily Crawling ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤... ({mode_text} ëª¨ë“œ)",
                "status": "active",
                "mode": mode
            })
            
            # 1. input_urlsì—ì„œ URL ì¡°íšŒ
            if url_ids:
                # íŠ¹ì • ID ëª©ë¡ìœ¼ë¡œ ì¡°íšŒ (í…ŒìŠ¤íŠ¸ìš©)
                urls = await input_url_repository.get_by_ids(url_ids)
                logger.info(f"ğŸ” Test mode: {len(urls)} URLs (IDs: {url_ids})")
            else:
                # ê¸°ì¡´ ë°©ì‹: í™œì„± URL ì¡°íšŒ
                urls = await input_url_repository.get_active_urls(
                    force_recrawl=force_recrawl,
                    limit=limit
                )
            
            if not urls:
                await self._send_update(task_id, "status", {
                    "message": "í¬ë¡¤ë§í•  URLì´ ì—†ìŠµë‹ˆë‹¤.",
                    "status": "completed"
                })
                self.tasks[task_id].status = TaskStatus.COMPLETED
                self.tasks[task_id].completedAt = datetime.now().isoformat()
                await self._send_update(task_id, "complete", {"message": "ì‘ì—… ì™„ë£Œ (í¬ë¡¤ë§ ëŒ€ìƒ ì—†ìŒ)"})
                return
            
            test_mode_text = " [í…ŒìŠ¤íŠ¸]" if url_ids else ""
            await self._send_update(task_id, "status", {
                "message": f"{len(urls)}ê°œ URL í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...{test_mode_text} ({mode_text} ëª¨ë“œ, ë™ì‹œì„±: {concurrency})",
                "status": "active",
                "total_urls": len(urls),
                "mode": mode,
                "concurrency": concurrency
            })
            
            # 2. ëª¨ë“œì— ë”°ë¼ í¬ë¡¤ë§ ì‹¤í–‰ (DB ì—…ë°ì´íŠ¸ ì—†ì´ ê²°ê³¼ë§Œ ìˆ˜ì§‘)
            if mode == "parallel":
                crawl_results = await self._process_parallel(
                    task_id, urls, concurrency
                )
            else:
                crawl_results = await self._process_sequential(
                    task_id, urls
                )
            
            # 3. ì¼ê´„ DB ì—…ë°ì´íŠ¸
            db_update_msg = "DB ì—…ë°ì´íŠ¸ ì¤‘..." if update_menu_links else "ê²°ê³¼ ì²˜ë¦¬ ì¤‘... (menu_links ì—…ë°ì´íŠ¸ ìŠ¤í‚µ)"
            await self._send_update(task_id, "status", {
                "message": db_update_msg,
                "status": "active"
            })
            success_count, failed_count = await self._batch_update_db(task_id, crawl_results, update_menu_links)
            
            # 4. JSON íŒŒì¼ ì €ì¥
            json_file_path = await self._save_json_output(task_id)
            
            # 4. ì™„ë£Œ ì²˜ë¦¬
            self.tasks[task_id].status = TaskStatus.COMPLETED
            self.tasks[task_id].completedAt = datetime.now().isoformat()
            
            # ê²°ê³¼ ì €ì¥ (API ì¡°íšŒìš©)
            self.tasks[task_id].result = CrawlingResult(
                json_file=str(json_file_path) if json_file_path else None,
                success=success_count,
                failed=failed_count,
                total=len(urls),
                failed_items=self._failed_items.get(task_id, [])
            )
            
            summary = {
                "total": len(urls),
                "success": success_count,
                "failed": failed_count,
                "json_file": str(json_file_path) if json_file_path else None,
                "message": f"Daily Crawling ì™„ë£Œ: {success_count}/{len(crawl_results)} ì„±ê³µ",
                "failed_items": [item.model_dump() for item in self._failed_items.get(task_id, [])]
            }
            
            await self._send_update(task_id, "final", summary)
            await self._send_update(task_id, "complete", summary)
            
            # í´ë¼ì´ì–¸íŠ¸ê°€ ì™„ë£Œ ë©”ì‹œì§€ë¥¼ ë°›ì„ ìˆ˜ ìˆë„ë¡ ì ì‹œ ëŒ€ê¸°
            await asyncio.sleep(1.0)
            
            logger.info(f"âœ… Crawling done: {success_count}/{len(urls)} success, {failed_count} failed")
            
            # ì •ë¦¬ (ì¶©ë¶„í•œ ëŒ€ê¸° í›„ ìŠ¤íŠ¸ë¦¼ í ì‚­ì œ)
            self._collected_results.pop(task_id, None)
            self._failed_items.pop(task_id, None)
            asyncio.create_task(self._delayed_cleanup(task_id))
            
        except Exception as exc:
            logger.error(f"âŒ Task {task_id} failed: {exc}")
            self.tasks[task_id].status = TaskStatus.FAILED
            self.tasks[task_id].error = str(exc)
            self.tasks[task_id].completedAt = datetime.now().isoformat()
            await self._send_update(task_id, "error", {"message": str(exc)})
            # í´ë¼ì´ì–¸íŠ¸ê°€ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°›ì„ ìˆ˜ ìˆë„ë¡ ì ì‹œ ëŒ€ê¸°
            await asyncio.sleep(1.0)
            self._collected_results.pop(task_id, None)
            self._failed_items.pop(task_id, None)
            asyncio.create_task(self._delayed_cleanup(task_id))

    async def _delayed_cleanup(self, task_id: str, delay: float = 300.0) -> None:
        """íƒœìŠ¤í¬ ì™„ë£Œ í›„ ì§€ì—°ëœ ì •ë¦¬ (ìŠ¤íŠ¸ë¦¼ í ì‚­ì œ ë“±)"""
        await asyncio.sleep(delay)
        logger.info(f"ğŸ§¹ Delayed cleanup for task: {task_id}")
        self.task_streams.pop(task_id, None)
    
    async def _process_sequential(
        self,
        task_id: str,
        urls: List[InputUrl]
    ) -> List[Dict[str, Any]]:
        """ìˆœì°¨ í¬ë¡¤ë§ ì²˜ë¦¬ (ê²°ê³¼ë§Œ ìˆ˜ì§‘, DB ì—…ë°ì´íŠ¸ ì—†ìŒ)"""
        results = []
        success_count = 0
        failed_count = 0
        
        for idx, input_url in enumerate(urls, start=1):
            try:
                await self._send_update(task_id, "progress", {
                    "current": idx,
                    "total": len(urls),
                    "success": success_count,
                    "failed": failed_count,
                    "url": input_url.pc_url,
                    "message": f"í¬ë¡¤ë§ ì¤‘: {idx}/{len(urls)}"
                })
                
                # í¬ë¡¤ë§ ì‹¤í–‰
                crawl_result = await self._crawl_single_url(input_url)
                
                if crawl_result.get("success"):
                    success_count += 1
                    # ì „ì²˜ë¦¬ ì‹¤í–‰
                    processed_result = self._preprocess_result(crawl_result, input_url)
                    results.append({
                        "success": True,
                        "input_url": input_url,
                        "processed_result": processed_result,
                    })
                    logger.info(f"âœ… [{idx}/{len(urls)}] Success: {input_url.pc_url}")
                else:
                    failed_count += 1
                    results.append({
                        "success": False,
                        "input_url": input_url,
                        "error": crawl_result.get("error"),
                    })
                    logger.warning(f"âŒ [{idx}/{len(urls)}] Failed: {input_url.pc_url}")
                
                # ê°œë³„ ì‘ì—… í›„ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ (count ë°˜ì˜)
                await self._send_update(task_id, "progress", {
                    "current": idx,
                    "total": len(urls),
                    "success": success_count,
                    "failed": failed_count,
                    "url": input_url.pc_url,
                    "message": f"í¬ë¡¤ë§ ì™„ë£Œ: {idx}/{len(urls)}"
                })
                    
            except Exception as exc:
                failed_count += 1
                logger.error(f"âŒ [{idx}/{len(urls)}] Error: {input_url.pc_url} - {exc}")
                results.append({
                    "success": False,
                    "input_url": input_url,
                    "error": str(exc),
                })
        
        return results
    
    async def _process_parallel(
        self,
        task_id: str,
        urls: List[InputUrl],
        concurrency: int
    ) -> List[Dict[str, Any]]:
        """ë³‘ë ¬ í¬ë¡¤ë§ ì²˜ë¦¬ (ê²°ê³¼ë§Œ ìˆ˜ì§‘, DB ì—…ë°ì´íŠ¸ ì—†ìŒ)"""
        semaphore = asyncio.Semaphore(concurrency)
        results: List[Dict[str, Any]] = []
        processed_count = 0
        success_count = 0
        failed_count = 0
        total = len(urls)
        lock = asyncio.Lock()
        
        async def crawl_with_semaphore(idx: int, input_url: InputUrl) -> Dict[str, Any]:
            nonlocal processed_count, success_count, failed_count
            
            async with semaphore:
                try:
                    # í¬ë¡¤ë§ ì‹¤í–‰
                    crawl_result = await self._crawl_single_url(input_url)
                    
                    async with lock:
                        processed_count += 1
                        current = processed_count
                        
                        if crawl_result.get("success"):
                            success_count += 1
                            is_success = True
                        else:
                            failed_count += 1
                            is_success = False
                        
                        curr_success = success_count
                        curr_failed = failed_count
                    
                    if is_success:
                        # ì „ì²˜ë¦¬ ì‹¤í–‰
                        processed_result = self._preprocess_result(crawl_result, input_url)
                        result = {
                            "success": True,
                            "input_url": input_url,
                            "processed_result": processed_result,
                        }
                        logger.info(f"âœ… [{current}/{total}] Success: {input_url.pc_url}")
                    else:
                        result = {
                            "success": False,
                            "input_url": input_url,
                            "error": crawl_result.get("error"),
                        }
                        logger.warning(f"âŒ [{current}/{total}] Failed: {input_url.pc_url}")
                    
                    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                    await self._send_update(task_id, "progress", {
                        "current": current,
                        "total": total,
                        "success": curr_success,
                        "failed": curr_failed,
                        "url": input_url.pc_url,
                        "message": f"í¬ë¡¤ë§ ì™„ë£Œ: {current}/{total} (ë³‘ë ¬ ì²˜ë¦¬ ì¤‘)"
                    })
                    
                    return result
                    
                except Exception as exc:
                    async with lock:
                        processed_count += 1
                        current = processed_count
                        failed_count += 1
                        curr_success = success_count
                        curr_failed = failed_count
                    
                    logger.error(f"âŒ [{current}/{total}] Error: {input_url.pc_url} - {exc}")
                    
                    await self._send_update(task_id, "progress", {
                        "current": current,
                        "total": total,
                        "success": curr_success,
                        "failed": curr_failed,
                        "url": input_url.pc_url,
                        "message": f"í¬ë¡¤ë§ ì™„ë£Œ: {current}/{total} (ë³‘ë ¬ ì²˜ë¦¬ ì¤‘)"
                    })
                    
                    return {
                        "success": False,
                        "input_url": input_url,
                        "error": str(exc),
                    }
        
        # ëª¨ë“  URLì— ëŒ€í•´ ë³‘ë ¬ ì‹¤í–‰
        tasks = [crawl_with_semaphore(idx, url) for idx, url in enumerate(urls, start=1)]
        task_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬ ë° ê²°ê³¼ ìˆ˜ì§‘
        for i, result in enumerate(task_results):
            if isinstance(result, Exception):
                results.append({
                    "success": False,
                    "input_url": urls[i],
                    "error": str(result),
                })
            else:
                results.append(result)
        
        return results
    
    async def _crawl_single_url(self, input_url: InputUrl, timeout: int = 300) -> Dict[str, Any]:
        """
        ë‹¨ì¼ URL í¬ë¡¤ë§
        
        Args:
            input_url: InputUrl ì—”í‹°í‹°
            timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ, ê¸°ë³¸ê°’ 5ë¶„)
        """
        url = input_url.pc_url
        menu = input_url.menu_path
        
        # í•¸ë“¤ëŸ¬ íƒ€ì…ì— ë”°ë¼ íƒ€ì„ì•„ì›ƒ ì¡°ì •
        handler_info = get_handler_for_url(url)
        skip_timeout = False
        if handler_info:
            _, handler_func = handler_info
            handler_name = handler_func.__name__
            
            # ë‹¤ì¤‘ í˜ì´ì§€ ìˆœíšŒ í•¸ë“¤ëŸ¬ íŒ¨í„´ë“¤
            multi_page_patterns = [
                "_main",  # ê¸°ì¡´: ë‹¤ì¤‘ ê²°ê³¼ ë©”ì¸ í•¸ë“¤ëŸ¬
                "_list",  # ëª©ë¡ í•¸ë“¤ëŸ¬ (ë‚´ë¶€ì—ì„œ ìƒì„¸ í˜ì´ì§€ ìˆœíšŒ)
                "gigagenie_faq",  # FAQ ì „ì²´ í˜ì´ì§€ ìˆœíšŒ
                "gigagenie_news",  # ë‰´ìŠ¤ ì „ì²´ í˜ì´ì§€ ìˆœíšŒ
                "winner_announcements",  # ë‹¹ì²¨ìë°œí‘œ í˜ì´ì§€ë„¤ì´ì…˜ + ìƒì„¸ ìˆœíšŒ
            ]
            
            # íŒ¨í„´ ë§¤ì¹­ í™•ì¸
            is_multi_page = any(pattern in handler_name for pattern in multi_page_patterns)
            
            if is_multi_page:
                # ë‹¤ì¤‘ í˜ì´ì§€ ìˆœíšŒ í•¸ë“¤ëŸ¬: ì „ì²´ íƒ€ì„ì•„ì›ƒ ë¯¸ì ìš© (ê°œë³„ í˜ì´ì§€ì— ìì²´ íƒ€ì„ì•„ì›ƒ)
                skip_timeout = True
                logger.info(f"ğŸ”— Multi-page handler detected, skipping global timeout: {handler_name}")
            else:
                # ì¼ë°˜ í•¸ë“¤ëŸ¬: 3ë¶„(180ì´ˆ) íƒ€ì„ì•„ì›ƒ
                timeout = 180
        
        try:
            if skip_timeout:
                # ë‹¤ì¤‘ ê²°ê³¼ í•¸ë“¤ëŸ¬ëŠ” íƒ€ì„ì•„ì›ƒ ì—†ì´ ì‹¤í–‰ (ê°œë³„ í˜ì´ì§€ì— ìì²´ íƒ€ì„ì•„ì›ƒ ìˆìŒ)
                return await self._do_crawl_single_url(input_url)
            else:
                # ì¼ë°˜ URL/í•¸ë“¤ëŸ¬ëŠ” íƒ€ì„ì•„ì›ƒ ì ìš©
                return await asyncio.wait_for(
                    self._do_crawl_single_url(input_url),
                    timeout=timeout
                )
        except asyncio.TimeoutError:
            logger.error(f"âŒ Timeout ({timeout}s): {url}")
            # íƒ€ì„ì•„ì›ƒ í›„ ì ì‹œ ëŒ€ê¸°í•˜ì—¬ ë¹„ë™ê¸° ì‘ì—… ì •ë¦¬ ì‹œê°„ í™•ë³´
            await asyncio.sleep(0.5)
            return {
                "success": False,
                "url": url,
                "error": f"í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ)"
            }
        except asyncio.CancelledError:
            logger.warning(f"âš ï¸ Cancelled: {url}")
            return {
                "success": False,
                "url": url,
                "error": "í¬ë¡¤ë§ ì·¨ì†Œë¨"
            }
        except Exception as exc:
            logger.error(f"âŒ Crawl failed {url}: {exc}")
            return {
                "success": False,
                "url": url,
                "error": str(exc)
            }
    
    async def _do_crawl_single_url(self, input_url: InputUrl) -> Dict[str, Any]:
        """ì‹¤ì œ í¬ë¡¤ë§ ë¡œì§ (íƒ€ì„ì•„ì›ƒ ë˜í¼ì—ì„œ í˜¸ì¶œ)"""
        url = input_url.pc_url
        menu = input_url.menu_path
        
        try:
            # 1. ì „ìš© í•¸ë“¤ëŸ¬ í™•ì¸
            handler_info = get_handler_for_url(url)
            
            if handler_info:
                pattern, handler_func = handler_info
                logger.info(f"ğŸ”— Handler matched: {url} -> {handler_func.__name__}")
                
                handler_result = await route_url(url, page_handler_client, menu)
                
                if handler_result:
                    # datas ë°°ì—´ì´ ìˆëŠ” ê²½ìš° ëª¨ë“  í•­ëª©ì„ ì²˜ë¦¬
                    if "datas" in handler_result and handler_result.get("datas"):
                        datas = handler_result["datas"]
                        menus = handler_result.get("menus", [])  # menus ë°°ì—´ë„ ê°€ì ¸ì˜¤ê¸°
                        logger.info(f"âœ… Handler result: {len(datas)} items, {len(menus)} menus ({url})")
                        
                        # ì—¬ëŸ¬ ë°ì´í„°ë¥¼ í¬í•¨í•œ ê²°ê³¼ ë°˜í™˜
                        return {
                            "success": True,
                            "url": url,
                            "mobile_url": input_url.mobile_url,
                            "title": handler_result.get("title"),
                            "markdown": handler_result.get("markdown", ""),
                            "html_content": handler_result.get("html", ""),
                            "hierarchy": input_url.get_hierarchy_list(),
                            "handler_name": handler_func.__name__,
                            "datas": datas,  # ëª¨ë“  datas í¬í•¨
                            "menus": menus,  # menus ë°°ì—´ í¬í•¨
                            "is_multi_result": True,
                        }
                    else:
                        return {
                            "success": True,
                            "url": url,
                            "mobile_url": input_url.mobile_url,
                            "title": handler_result.get("title"),
                            "markdown": handler_result.get("markdown", ""),
                            "html_content": handler_result.get("html", ""),
                            "hierarchy": input_url.get_hierarchy_list(),
                            "handler_name": handler_func.__name__,
                        }
            
            # 2. ê¸°ë³¸ MCP ìŠ¤í¬ë˜í•‘
            logger.info(f"ğŸ” Default scraping: {url}")
            tool_result = await crawler_tools.scrape(url)
            
            if tool_result.get("success"):
                return {
                    "success": True,
                    "url": url,
                    "mobile_url": input_url.mobile_url,
                    "title": tool_result.get("title"),
                    "markdown": tool_result.get("markdown", ""),
                    "html_content": tool_result.get("html_content", ""),
                    "hierarchy": input_url.get_hierarchy_list(),
                }
            else:
                return {
                    "success": False,
                    "url": url,
                    "error": tool_result.get("error", "ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨")
                }
                
        except Exception as exc:
            logger.error(f"âŒ Crawl failed {url}: {exc}")
            return {
                "success": False,
                "url": url,
                "error": str(exc)
            }
    
    # ----------------------------------------------------------------------------------
    # ì „ì²˜ë¦¬ ë° JSON ë³€í™˜
    # ----------------------------------------------------------------------------------
    def _preprocess_result(
        self, 
        crawl_result: Dict[str, Any], 
        input_url: InputUrl
    ) -> Dict[str, Any]:
        """
        í¬ë¡¤ë§ ê²°ê³¼ ì „ì²˜ë¦¬
        
        Args:
            crawl_result: í¬ë¡¤ë§ ê²°ê³¼
            input_url: InputUrl ì—”í‹°í‹°
            
        Returns:
            ì „ì²˜ë¦¬ëœ ê²°ê³¼
        """
        menu_path = input_url.menu_path or ""
        
        # is_multi_resultì¸ ê²½ìš° datas ë°°ì—´ì˜ ê° í•­ëª©ì„ ì „ì²˜ë¦¬
        if crawl_result.get("is_multi_result") and crawl_result.get("datas"):
            processed_datas = []
            for data in crawl_result["datas"]:
                markdown = data.get("markdown", "")
                html_content = data.get("html", "")
                
                processed_text, process_type = preprocess_content(
                    markdown_text=markdown,
                    menu_path=menu_path,
                    html_content=html_content
                )
                
                processed_datas.append({
                    **data,
                    "processed_text": processed_text,
                    "process_type": process_type,
                })
            
            return {
                **crawl_result,
                "processed_datas": processed_datas,
            }
        
        # ë‹¨ì¼ ê²°ê³¼ì¸ ê²½ìš°
        markdown = crawl_result.get("markdown", "")
        html_content = crawl_result.get("html_content", "")
        
        # ì „ì²˜ë¦¬ ì‹¤í–‰
        processed_text, process_type = preprocess_content(
            markdown_text=markdown,
            menu_path=menu_path,
            html_content=html_content
        )
        
        return {
            **crawl_result,
            "processed_text": processed_text,
            "process_type": process_type,
        }
    
    def _convert_to_json_format(
        self, 
        processed_result: Dict[str, Any], 
        input_url: InputUrl,
        document_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ì „ì²˜ë¦¬ëœ ê²°ê³¼ë¥¼ ìµœì¢… JSON í¬ë§·ìœ¼ë¡œ ë³€í™˜
        data_*.json í˜•ì‹ì— ë§ì¶¤
        
        Args:
            processed_result: ì „ì²˜ë¦¬ëœ ê²°ê³¼
            input_url: InputUrl ì—”í‹°í‹°
            document_id: menu_linksì—ì„œ íšë“í•œ document_id
            
        Returns:
            JSON í¬ë§· ë”•ì…”ë„ˆë¦¬
        """
        url = processed_result.get("url", "")
        mobile_url = processed_result.get("mobile_url") or input_url.mobile_url or ""
        processed_text = processed_result.get("processed_text", "")
        html_content = processed_result.get("html_content", "")
        hierarchy = processed_result.get("hierarchy", []) or input_url.get_hierarchy_list()
        
        # title ê²°ì •
        # 1. í•¸ë“¤ëŸ¬ ë°ì´í„°ì¸ ê²½ìš°: í•¸ë“¤ëŸ¬ì—ì„œ ì¶”ì¶œí•œ title ìš°ì„  ì‚¬ìš©
        # 2. ì¼ë°˜ ë°ì´í„°: menu_pathì˜ ^ ê¸°ì¤€ ë§ˆì§€ë§‰ ê°’ ì‚¬ìš©
        title = ""
        
        if processed_result.get("is_handler_data"):
            # í•¸ë“¤ëŸ¬ì—ì„œ ì¶”ì¶œí•œ ê°œë³„ title ì‚¬ìš©
            title = processed_result.get("title") or ""
        
        if not title and input_url.menu_path:
            # menu_pathì˜ ë§ˆì§€ë§‰ ê°’ ì‚¬ìš©
            menu_parts = input_url.menu_path.split("^")
            title = menu_parts[-1].strip() if menu_parts else ""
        
        # ê·¸ë˜ë„ ì—†ìœ¼ë©´ fallback
        if not title:
            title = processed_result.get("title") or "ì œëª© ì—†ìŒ"
        
        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        title = unicodedata.normalize('NFC', title)
        url = unicodedata.normalize('NFC', url)
        processed_text = unicodedata.normalize('NFC', processed_text)
        
        # ê°œí–‰ë¬¸ìë¥¼ \\nìœ¼ë¡œ ë³€í™˜
        final_text = processed_text.replace("\n", "\\n")
        
        # hierarchy ì •ê·œí™”
        normalized_hierarchy = None
        if hierarchy:
            normalized_hierarchy = [
                unicodedata.normalize('NFC', item) 
                for item in hierarchy 
                if item
            ]
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = self._extract_metadata(html_content, url)
        
        # recommendations í•„ë“œ ì¶”ê°€ (ìƒí’ˆ í˜ì´ì§€ì—ì„œ ì‚¬ìš©)
        if "recommendations" in processed_result:
            recommendations = processed_result.get("recommendations")
            if recommendations:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¶”ê°€
                metadata["recommendations"] = recommendations
        
        # ìµœì¢… JSON êµ¬ì¡°
        json_data = {
            "docId": document_id or "",
            "url": url,
            "murl": mobile_url,
            "hierarchy": normalized_hierarchy or [],
            "title": title,
            "text": final_text,
            "startdate": JSON_START_DATE,
            "enddate": JSON_END_DATE,
            "metadata": metadata,
            "status": "new",
        }
        
        return json_data
    
    def _extract_metadata(
        self, 
        html_content: str, 
        base_url: str
    ) -> Dict[str, Any]:
        """HTMLì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì´ë¯¸ì§€, ë§í¬ ë“±)"""
        metadata: Dict[str, Any] = {}
        
        if not html_content:
            return metadata
        
        try:
            from bs4 import BeautifulSoup
            from urllib.parse import urljoin
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ
            images = []
            for img in soup.find_all('img'):
                if img.find_parent(id=['cfmClHeader', 'cfmClFooter']):
                    continue
                alt_text = (img.get('alt') or '').strip()
                if len(alt_text) > 2:
                    src = img.get('src', '')
                    if src and not src.startswith('http'):
                        src = urljoin(base_url, src)
                    images.append({'alt': alt_text, 'src': src})
            if images:
                metadata['images'] = images
            
            # ë§í¬ ì¶”ì¶œ
            urls_data = []
            for link in soup.find_all('a', href=True):
                if link.find_parent(id=['cfmClHeader', 'cfmClFooter']):
                    continue
                link_text = link.get_text().strip()
                if len(link_text) < 2:
                    continue
                href = link.get('href')
                if href.startswith('http') or href.startswith('/'):
                    if href.startswith('/'):
                        href = urljoin(base_url, href)
                    urls_data.append({'desc': link_text, 'url': href})
            
            if urls_data:
                # ì¤‘ë³µ ì œê±°
                seen = set()
                unique_urls = []
                for item in urls_data:
                    if item['url'] not in seen:
                        seen.add(item['url'])
                        unique_urls.append(item)
                metadata['urls'] = unique_urls
                
        except Exception as e:
            logger.warning(f"âš ï¸ Metadata extraction failed: {e}")
        
        return metadata
    
    def _pc_to_mobile_url(self, pc_url: str) -> str:
        """PC URLì„ ëª¨ë°”ì¼ URLë¡œ ë³€í™˜"""
        if not pc_url:
            return ""
        
        # KT ì´ë²¤íŠ¸ URL ë³€í™˜
        if "event.kt.com" in pc_url:
            return pc_url.replace("https://event.kt.com", "https://m.kt.com")
        
        # KT Shop URL ë³€í™˜
        if "shop.kt.com" in pc_url:
            return pc_url.replace("https://shop.kt.com", "https://m.shop.kt.com")
        
        # product.kt.com ë³€í™˜
        if "product.kt.com" in pc_url:
            return pc_url.replace("https://product.kt.com", "https://m.product.kt.com")
        
        # ê¸°íƒ€ kt.com ë„ë©”ì¸
        if "kt.com" in pc_url and "://m." not in pc_url:
            # https://xxx.kt.com -> https://m.xxx.kt.com í˜•íƒœë¡œ ë³€í™˜ ì‹œë„
            import re
            match = re.match(r'https://([^.]+)\.kt\.com(.*)', pc_url)
            if match:
                subdomain = match.group(1)
                path = match.group(2)
                return f"https://m.{subdomain}.kt.com{path}"
        
        return pc_url
    
    # ----------------------------------------------------------------------------------
    # ì¼ê´„ DB ì—…ë°ì´íŠ¸
    # ----------------------------------------------------------------------------------
    async def _batch_update_db(
        self,
        task_id: str,
        crawl_results: List[Dict[str, Any]],
        update_menu_links: bool = True
    ) -> tuple[int, int]:
        """
        í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì¼ê´„ë¡œ DBì— ì—…ë°ì´íŠ¸
        
        Args:
            task_id: íƒœìŠ¤í¬ ID
            crawl_results: í¬ë¡¤ë§ ê²°ê³¼ ëª©ë¡
            update_menu_links: menu_links DB ì—…ë°ì´íŠ¸ ì—¬ë¶€
            
        Returns:
            (success_count, failed_count)
        """
        success_count = 0
        failed_count = 0
        total = len(crawl_results)
        
        logger.info(f"ğŸ” DB batch update start: {total} items")
        
        for idx, result in enumerate(crawl_results, start=1):
            input_url: InputUrl = result.get("input_url")
            
            try:
                if result.get("success"):
                    processed_result = result.get("processed_result", {})
                    
                    # is_multi_resultì¸ ê²½ìš° ê° data í•­ëª©ì„ ê°œë³„ ì²˜ë¦¬
                    if processed_result.get("is_multi_result") and processed_result.get("processed_datas"):
                        processed_datas = processed_result["processed_datas"]
                        menus = processed_result.get("menus", [])  # menus ë°°ì—´ ê°€ì ¸ì˜¤ê¸°
                        logger.info(f"âœ… Handler result: {len(processed_datas)} items, {len(menus)} menus ({input_url.pc_url})")
                        
                        for data_idx, data in enumerate(processed_datas):
                            # menus ë°°ì—´ì—ì„œ í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ë©”ë‰´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                            menu_info = menus[data_idx] if data_idx < len(menus) else {}
                            
                            # menu ë¬¸ìì—´ì„ ^ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ hierarchyì™€ title ì¶”ì¶œ
                            menu_str = menu_info.get("menu", "")
                            if menu_str:
                                menu_parts = [p.strip() for p in menu_str.split("^") if p.strip()]
                                data_hierarchy = menu_parts  # ì „ì²´ë¥¼ hierarchyë¡œ
                                data_title = menu_parts[-1] if menu_parts else ""  # ë§ˆì§€ë§‰ì„ titleë¡œ
                            else:
                                data_hierarchy = processed_result.get("hierarchy", []) or input_url.get_hierarchy_list()
                                data_title = data.get("title") or ""
                            
                            # URL ì •ë³´: menusì—ì„œ ìš°ì„ , ì—†ìœ¼ë©´ dataì—ì„œ
                            data_url = menu_info.get("url") or data.get("url") or input_url.pc_url
                            data_murl = menu_info.get("mobile_url") or self._pc_to_mobile_url(data_url)
                            
                            single_result = {
                                "url": data_url,
                                "mobile_url": data_murl,
                                "title": data_title,
                                "processed_text": data.get("processed_text", ""),
                                "html_content": data.get("html", ""),
                                "hierarchy": data_hierarchy,
                                "is_handler_data": True,  # í•¸ë“¤ëŸ¬ ë°ì´í„° í‘œì‹œ
                            }
                            
                            # recommendations í•„ë“œ í¬í•¨ (ìˆëŠ” ê²½ìš°)
                            if "recommendations" in data:
                                single_result["recommendations"] = data["recommendations"]
                            
                            # menu_links ì—…ë°ì´íŠ¸ (docId íšë“)
                            document_id = None
                            if update_menu_links:
                                document_id = await self._update_menu_links(single_result, input_url)
                            
                            # JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (docId í¬í•¨)
                            json_data = self._convert_to_json_format(single_result, input_url, document_id)
                            
                            # ê²°ê³¼ ìˆ˜ì§‘
                            self._collected_results[task_id].append(json_data)
                        
                        # input_urls ìƒíƒœ ì—…ë°ì´íŠ¸ (í•œ ë²ˆë§Œ)
                        handler_name = processed_result.get("handler_name")
                        await input_url_repository.update_crawl_status(
                            input_url.id, "success", handler_name=handler_name
                        )
                        success_count += 1
                    else:
                        # ë‹¨ì¼ ê²°ê³¼ ì²˜ë¦¬
                        # menu_links ì—…ë°ì´íŠ¸ (docId íšë“)
                        document_id = None
                        if update_menu_links:
                            document_id = await self._update_menu_links(processed_result, input_url)
                        
                        # JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (docId í¬í•¨)
                        json_data = self._convert_to_json_format(processed_result, input_url, document_id)
                        
                        # ê²°ê³¼ ìˆ˜ì§‘
                        self._collected_results[task_id].append(json_data)
                        
                        # input_urls ìƒíƒœ ì—…ë°ì´íŠ¸
                        handler_name = processed_result.get("handler_name")
                        await input_url_repository.update_crawl_status(
                            input_url.id, "success", handler_name=handler_name
                        )
                        success_count += 1
                else:
                    # ì‹¤íŒ¨í•œ ê²½ìš°
                    error_msg = result.get("error") or "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
                    await input_url_repository.update_crawl_status(
                        input_url.id, "failed", error_msg
                    )
                    failed_count += 1
                    
                    # ì‹¤íŒ¨ ë‚´ì—­ ì €ì¥
                    self._failed_items[task_id].append(FailedItem(
                        id=input_url.id,
                        url=input_url.pc_url,
                        error=error_msg
                    ))
                    
            except Exception as exc:
                error_msg = str(exc)
                logger.error(f"âŒ DB update error [{idx}/{total}]: {input_url.pc_url} - {error_msg}")
                await input_url_repository.update_crawl_status(
                    input_url.id, "failed", error_msg
                )
                failed_count += 1
                
                # ì‹¤íŒ¨ ë‚´ì—­ ì €ì¥
                self._failed_items[task_id].append(FailedItem(
                    id=input_url.id,
                    url=input_url.pc_url,
                    error=error_msg
                ))
            
            # ì§„í–‰ ìƒí™© (10ê°œë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰)
            if idx % 10 == 0 or idx == total:
                await self._send_update(task_id, "progress", {
                    "current": idx,
                    "total": total,
                    "success": success_count,
                    "failed": failed_count,
                    "message": f"DB ì—…ë°ì´íŠ¸ ì¤‘: {idx}/{total}"
                })
        
        logger.info(f"âœ… DB batch update done: {success_count} success, {failed_count} failed")
        return success_count, failed_count
    
    # ----------------------------------------------------------------------------------
    # menu_links ì—…ë°ì´íŠ¸ (menu_path ìš°ì„  ì¡°íšŒ)
    # ----------------------------------------------------------------------------------
    async def _update_menu_links(
        self, 
        processed_result: Dict[str, Any], 
        input_url: InputUrl
    ) -> Optional[str]:
        """
        í¬ë¡¤ë§ ê²°ê³¼ë¥¼ menu_links í…Œì´ë¸”ì— ë°˜ì˜
        
        ì¡°íšŒ ìˆœì„œ: menu_path â†’ pc_url â†’ mobile_url
        
        Returns:
            document_id
        """
        pc_url = processed_result.get("url")
        mobile_url = processed_result.get("mobile_url") or input_url.mobile_url
        hierarchy = processed_result.get("hierarchy", [])
        
        # hierarchy â†’ menu_path ë³€í™˜
        if hierarchy:
            menu_path = "^".join([seg.strip() for seg in hierarchy if seg and seg.strip()])
        else:
            menu_path = input_url.menu_path or ""
        
        document_id = None
        
        async for session in get_database_session():
            try:
                existing = None
                
                # menu_path + pc_url ì¡°í•©ìœ¼ë¡œ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
                if menu_path and pc_url:
                    stmt = select(MenuLink).where(
                        MenuLink.menu_path == menu_path,
                        MenuLink.pc_url == pc_url
                    )
                    result = await session.execute(stmt)
                    existing = result.scalar_one_or_none()
                
                if existing:
                    # ì—…ë°ì´íŠ¸
                    existing.menu_path = menu_path
                    existing.pc_url = pc_url
                    if mobile_url:
                        existing.mobile_url = mobile_url
                    existing.updated_by = "daily_crawling"
                    existing.updated_at = datetime.now()
                    
                    await session.commit()
                    document_id = existing.document_id
                    logger.debug(f"âœ… menu_links updated: {document_id}")
                else:
                    # ìƒˆ ë ˆì½”ë“œ ìƒì„±
                    max_num = await self._get_max_document_num(session)
                    document_id = f"ktcom_{max_num + 1}"
                    
                    new_record = MenuLink(
                        document_id=document_id,
                        menu_path=menu_path,
                        pc_url=pc_url,
                        mobile_url=mobile_url,
                        created_by="daily_crawling",
                    )
                    session.add(new_record)
                    await session.commit()
                    logger.debug(f"âœ… menu_links created: {document_id}")
                    
            except Exception as exc:
                logger.error(f"âŒ menu_links update failed {pc_url}: {exc}")
                await session.rollback()
                raise
            
            break
        
        return document_id
    
    async def _get_max_document_num(self, session) -> int:
        """í˜„ì¬ ìµœëŒ€ document_id ë²ˆí˜¸ ì¡°íšŒ"""
        stmt = select(MenuLink.document_id).where(
            MenuLink.document_id.like("ktcom_%")
        )
        result = await session.execute(stmt)
        doc_ids = result.scalars().all()
        
        max_num = 0
        for doc_id in doc_ids:
            match = re.match(r"^ktcom_(\d+)$", doc_id)
            if match:
                try:
                    num = int(match.group(1))
                    max_num = max(max_num, num)
                except ValueError:
                    pass
        
        return max_num
    
    # ----------------------------------------------------------------------------------
    # JSON íŒŒì¼ ì¶œë ¥
    # ----------------------------------------------------------------------------------
    async def _save_json_output(self, task_id: str) -> Optional[Path]:
        """
        ìˆ˜ì§‘ëœ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        í˜•ì‹: data_YYYY-MM-DD_HHMMSS.json
        """
        results = self._collected_results.get(task_id, [])
        
        if not results:
            logger.warning(f"âš ï¸ No results to save: {task_id}")
            return None
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        RESULT_DIR.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime("%Y-%m-%d_%H%M%S")
        file_path = RESULT_DIR / f"data_{timestamp}.json"
        
        try:
            # JSON ì €ì¥ (í•œê¸€ ìœ ì§€)
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… JSON saved: {file_path} ({len(results)} items)")
            return file_path
            
        except Exception as e:
            logger.error(f"âŒ JSON save failed: {e}")
            return None
    
    async def _send_update(self, task_id: str, update_type: str, data: Dict[str, Any]) -> None:
        """SSE ì—…ë°ì´íŠ¸ ì „ì†¡"""
        if task_id in self.task_streams:
            message = json.dumps({"type": update_type, "data": data})
            await self.task_streams[task_id].put(message)


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
daily_crawling_service = DailyCrawlingService()
