"""RAG Crawling Service - Based on rag-scraping app.py workflow"""
import asyncio
import json
import logging
import re
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any, AsyncGenerator, Dict, List, Optional, Tuple
from urllib.parse import urlparse

from sqlalchemy import select

from app.application.crawler.tools_client import crawler_tools
from app.application.crawler.page_handlers import (
    route_url,
    get_handler_for_url,
    page_handler_client,
)
from app.domains.menu.entities.menu_link import MenuLink
from app.models import CrawlingResult, TaskResult, TaskStatus
from app.shared.database.base import get_database_session

logger = logging.getLogger(__name__)

# ì •ê·œì‹ ìƒìˆ˜
URL_PATTERN = re.compile(r"https?://[^\s,;]+")
# ë©”ë‰´ ê²½ë¡œ êµ¬ë¶„ì (í”„ë¡ íŠ¸/ë°ì´í„°ì™€ ë™ì¼í•˜ê²Œ '^' ì‚¬ìš©)
MENU_PATH_DELIMITER = "^"
# convert_to_json_format ê¸°ë³¸ ë‚ ì§œ ê°’
JSON_START_DATE = "1900-01-01"
JSON_END_DATE = "2999-12-31"
# ë§ˆí¬ë‹¤ìš´ ì €ì¥ ê²½ë¡œ
MARKDOWN_RESULT_DIR = Path(__file__).parent / "result"


class RAGCrawlingService:
    """RAG ìŠ¤í¬ë˜í•‘ ì„œë¹„ìŠ¤ - rag-scrapingì˜ app.py ì›Œí¬í”Œë¡œìš° ê¸°ë°˜"""
    
    def __init__(self) -> None:
        self.tasks: Dict[str, TaskResult] = {}
        self.task_streams: Dict[str, asyncio.Queue] = {}
        
    # ----------------------------------------------------------------------------------
    # Public APIs
    # ----------------------------------------------------------------------------------
    def create_task(self, urls_input: str) -> str:
        task_id = str(uuid.uuid4())
        task_result = TaskResult(
            taskId=task_id,
            status=TaskStatus.PENDING,
            createdAt=datetime.now().isoformat(),
        )
        self.tasks[task_id] = task_result
        self.task_streams[task_id] = asyncio.Queue()
        logger.info("âœ… RAG Task created: %s", task_id)
        asyncio.create_task(self._process_rag_task(task_id, urls_input))
        return task_id
    
    def get_task(self, task_id: str) -> Optional[TaskResult]:
        return self.tasks.get(task_id)
    
    async def get_task_stream(self, task_id: str) -> AsyncGenerator[str, None]:
        logger.info("ğŸ” RAG SSE stream requested for task: %s", task_id)
        if task_id not in self.task_streams:
            logger.error("âŒ RAG Task stream not found: %s", task_id)
            yield f"data: {json.dumps({'type': 'error', 'data': {'message': 'Task not found'}})}\n\n"
            return
            
        # ì´ˆê¸° ì—°ê²° ì•Œë¦¼
        yield f"data: {json.dumps({'type': 'connected', 'data': {'message': 'RAG Stream connected'}})}\n\n"
        queue = self.task_streams[task_id]
        try:
            while True:
                try:
                    message = await asyncio.wait_for(queue.get(), timeout=30.0)
                    yield f"data: {message}\n\n"
                    try:
                        payload = json.loads(message)
                        if payload.get("type") in {"final", "complete", "error"}:
                            await asyncio.sleep(0.5)
                            break
                    except json.JSONDecodeError:
                        logger.debug("SSE message is not JSON: %s", message)
                    
                    task = self.tasks.get(task_id)
                    if task and task.status in {TaskStatus.COMPLETED, TaskStatus.FAILED}:
                        break
                except asyncio.TimeoutError:
                    await self._send_update(task_id, "heartbeat", {})
                    task = self.tasks.get(task_id)
                    if task and task.status in {TaskStatus.COMPLETED, TaskStatus.FAILED}:
                        await self._send_update(task_id, "complete", {"message": "RAG ì‘ì—… ì™„ë£Œ"})
                        break
        except Exception as exc:  # pragma: no cover - SSE ì—ëŸ¬ ì²˜ë¦¬
            logger.error("RAG Task stream error %s: %s", task_id, exc)
            await self._send_update(task_id, "error", {"message": str(exc)})
        finally:
            logger.info("Cleaning up RAG task stream: %s", task_id)
            self.task_streams.pop(task_id, None)

    # ----------------------------------------------------------------------------------
    # Core workflow
    # ----------------------------------------------------------------------------------
    async def _process_rag_task(self, task_id: str, urls_input: str) -> None:
        try:
            self.tasks[task_id].status = TaskStatus.RUNNING
            await self._send_update(task_id, "status", {"message": "RAG í¬ë¡¤ë§ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...", "status": "active"})

            urls = await self._extract_urls_from_input(urls_input)
            if not urls:
                raise ValueError("ìœ íš¨í•œ URLì´ ì—†ìŠµë‹ˆë‹¤")
            await self._send_update(task_id, "status", {"message": f"{len(urls)}ê°œ URL í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...", "status": "active"})

            # ë©”ë‰´ ë§¤í•‘ (PC URL ê¸°ì¤€)
            url_menu_map = await self._build_url_menu_map(urls)

            scraped_results = await self._scrape_data(task_id, urls)
            if not scraped_results:
                raise ValueError("í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

            processed_results = await self._preprocess_data(task_id, scraped_results)
            
            # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì€ ì´ë¯¸ ê°œë³„ì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ê±´ë„ˆëœ€
            await self._send_update(task_id, "status", {"message": "ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤", "status": "active"})
            
            json_results = await self._convert_to_json(task_id, processed_results, url_menu_map)

            result = CrawlingResult(json_data=json_results)
            self.tasks[task_id].result = result
            self.tasks[task_id].status = TaskStatus.COMPLETED
            self.tasks[task_id].completedAt = datetime.now().isoformat()
            await self._send_update(task_id, "final", result.model_dump())
            await self._send_update(task_id, "complete", {"message": f"RAG í¬ë¡¤ë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê° URLë§ˆë‹¤ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ê°œë³„ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤"})
        except Exception as exc:  # pragma: no cover
            logger.error("RAG Task %s failed: %s", task_id, exc)
            self.tasks[task_id].status = TaskStatus.FAILED
            self.tasks[task_id].error = str(exc)
            self.tasks[task_id].completedAt = datetime.now().isoformat()
            await self._send_update(task_id, "error", {"message": str(exc)})
            
    # ----------------------------------------------------------------------------------
    # URL ì²˜ë¦¬
    # ----------------------------------------------------------------------------------
    async def _extract_urls_from_input(self, raw_input: str) -> List[str]:
        regex_urls = URL_PATTERN.findall(raw_input)
        cleaned_regex = [url.rstrip(".,;)]}") for url in regex_urls]
        ordered_unique: List[str] = []
        seen: set[str] = set()

        for url in cleaned_regex:
            if url and url not in seen and url.startswith(("http://", "https://")):
                ordered_unique.append(url)
                seen.add(url)

        return ordered_unique
            
    async def _build_url_menu_map(self, urls: List[str]) -> Dict[str, MenuLink]:
        if not urls:
            return {}
        unique_urls = list(dict.fromkeys(urls))
        url_map: Dict[str, MenuLink] = {}
        async for session in get_database_session():
            stmt = select(MenuLink).where(MenuLink.pc_url.in_(unique_urls))
            result = await session.execute(stmt)
            for row in result.scalars().all():
                if row.pc_url:
                    url_map[row.pc_url] = row
            break
        logger.info("ë©”ë‰´ ë§¤í•‘ ì™„ë£Œ: %s/%s", len(url_map), len(unique_urls))
        return url_map

    # ----------------------------------------------------------------------------------
    # Scraping + preprocessing
    # ----------------------------------------------------------------------------------
    async def _scrape_data(self, task_id: str, urls: List[str]) -> List[Dict[str, Any]]:
        results: List[Dict[str, Any]] = []
        logger.info(f"ğŸš€ ìŠ¤í¬ë˜í•‘ ì‹œì‘: ì´ {len(urls)}ê°œ URL ì²˜ë¦¬ ì˜ˆì •")
        
        for idx, url in enumerate(urls, start=1):
            logger.info(f"ğŸ“„ URL {idx}/{len(urls)} ì²˜ë¦¬ ì‹œì‘: {url}")
            await self._send_update(task_id, "status", {"message": f"í¬ë¡¤ë§ ì§„í–‰: {idx}/{len(urls)} - {url}", "status": "active"})
            try:
                # 1. ë¨¼ì € page_handlersì—ì„œ ë§¤ì¹­ë˜ëŠ” í•¸ë“¤ëŸ¬ í™•ì¸
                handler_info = get_handler_for_url(url)
                
                if handler_info:
                    # ì „ìš© í•¸ë“¤ëŸ¬ê°€ ìˆëŠ” ê²½ìš° route_url ì‚¬ìš©
                    pattern, handler_func = handler_info
                    logger.info(f"ğŸ¯ ì „ìš© í•¸ë“¤ëŸ¬ ë°œê²¬: {handler_func.__name__} for {url}")
                    await self._send_update(
                        task_id, 
                        "status", 
                        {"message": f"ì „ìš© í•¸ë“¤ëŸ¬ ì‹¤í–‰: {handler_func.__name__}", "status": "active"}
                    )
                    
                    handler_result = await route_url(url, page_handler_client)
                    
                    if handler_result:
                        # í•¸ë“¤ëŸ¬ ê²°ê³¼ ì²˜ë¦¬ - menus/datas êµ¬ì¡°ì¸ ê²½ìš°
                        if "datas" in handler_result and handler_result.get("datas"):
                            # ëª©ë¡ í•¸ë“¤ëŸ¬ ê²°ê³¼ (ì—¬ëŸ¬ í•­ëª© ë°˜í™˜)
                            for data_item in handler_result["datas"]:
                                result_data = {
                                    "url": data_item.get("url", url),
                                    "title": data_item.get("title"),
                                    "html_content": data_item.get("html", ""),
                                    "markdown": data_item.get("markdown", ""),
                                    "special_processed": True,
                                    "handler_name": handler_func.__name__,
                                }
                                results.append(result_data)
                                await self._save_single_markdown_file(task_id, result_data, idx, len(urls))
                            logger.info(f"âœ… í•¸ë“¤ëŸ¬ ì²˜ë¦¬ ì™„ë£Œ: {len(handler_result['datas'])}ê°œ í•­ëª©")
                        else:
                            # ë‹¨ì¼ ê²°ê³¼ í•¸ë“¤ëŸ¬
                            result_data = {
                                "url": url,
                                "title": handler_result.get("title"),
                                "html_content": handler_result.get("html", ""),
                                "markdown": handler_result.get("markdown", ""),
                                "special_processed": True,
                                "handler_name": handler_func.__name__,
                            }
                            results.append(result_data)
                            logger.info(f"âœ… URL {idx}/{len(urls)} í•¸ë“¤ëŸ¬ ì²˜ë¦¬ ì„±ê³µ: {url}")
                            await self._save_single_markdown_file(task_id, result_data, idx, len(urls))
                    else:
                        # í•¸ë“¤ëŸ¬ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ í´ë°±
                        logger.warning(f"âš ï¸ í•¸ë“¤ëŸ¬ ì‹¤íŒ¨, ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ í´ë°±: {url}")
                        await self._scrape_with_default_tool(task_id, url, idx, len(urls), results)
                else:
                    # 2. ì „ìš© í•¸ë“¤ëŸ¬ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ MCP ìŠ¤í¬ë˜í•‘
                    await self._scrape_with_default_tool(task_id, url, idx, len(urls), results)
                    
            except Exception as exc:  # pragma: no cover
                logger.error(f"âŒ URL {idx}/{len(urls)} ì²˜ë¦¬ ì‹¤íŒ¨: {url} - {exc}")
                results.append({"url": url, "error": str(exc), "success": False})
        
        logger.info(f"âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: ì´ {len(results)}ê°œ ê²°ê³¼ (ì„±ê³µ: {len([r for r in results if not r.get('error')])}ê°œ)")
        return results

    async def _scrape_with_default_tool(
        self, 
        task_id: str, 
        url: str, 
        idx: int, 
        total: int, 
        results: List[Dict[str, Any]]
    ) -> None:
        """ê¸°ë³¸ MCP ìŠ¤í¬ë˜í•‘ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ URL ì²˜ë¦¬"""
        try:
            tool_result = await crawler_tools.scrape(url)
            if tool_result.get("success"):
                result_data = {
                    "url": url,
                    "title": tool_result.get("title"),
                    "html_content": tool_result.get("html_content", ""),
                    "markdown": tool_result.get("markdown", ""),
                }
                results.append(result_data)
                logger.info(f"âœ… URL {idx}/{total} í¬ë¡¤ë§ ì„±ê³µ: {url} - ì œëª©: {result_data.get('title')}")
                
                # ì¦‰ì‹œ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥
                await self._save_single_markdown_file(task_id, result_data, idx, total)
            else:
                error_result = {"url": url, "error": tool_result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"), "success": False}
                results.append(error_result)
                logger.warning(f"âš ï¸ URL {idx}/{total} í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {error_result.get('error')}")
        except Exception as exc:
            logger.error(f"âŒ URL {idx}/{total} MCP scrape failed: {url} - {exc}")
            results.append({"url": url, "error": str(exc), "success": False})

    async def _preprocess_data(self, task_id: str, scraped_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        processed: List[Dict[str, Any]] = []
        for idx, result in enumerate(scraped_results, start=1):
            await self._send_update(task_id, "status", {"message": f"ì „ì²˜ë¦¬ ì§„í–‰: {idx}/{len(scraped_results)} - {result['url']}", "status": "active"})
            if result.get("error"):
                processed.append(result)
                continue
            markdown = (result.get("markdown") or "").strip()
            processed.append({
                **result,
                "processed_markdown": markdown,
                "processed_at": datetime.now().isoformat(),
                "text_length": len(markdown),
            })
        return processed

    # ----------------------------------------------------------------------------------
    # JSON conversion
    # ----------------------------------------------------------------------------------
    async def _convert_to_json(
        self,
        task_id: str,
        processed_results: List[Dict[str, Any]],
        url_menu_map: Dict[str, MenuLink],
    ) -> List[Dict[str, Any]]:
        json_results: List[Dict[str, Any]] = []

        for idx, result in enumerate(processed_results, start=1):
            await self._send_update(
                task_id,
                "status",
                {
                    "message": f"JSON ë³€í™˜ ì§„í–‰: {idx}/{len(processed_results)} - {result['url']}",
                    "status": "active",
                },
            )

            if result.get("error"):
                json_results.append(
                    {
                        "url": result["url"],
                        "error": result["error"],
                        "status": "failed",
                    }
                )
                continue
                
            menu = url_menu_map.get(result["url"])
            hierarchy, title = await self._resolve_hierarchy_and_title(result, menu)
            markdown_content = result.get("processed_markdown", "")
            html_content = result.get("html_content", "")
            mobile_url = menu.mobile_url if menu and menu.mobile_url else None

            try:
                json_payload = await crawler_tools.convert_to_json(
                    url=result["url"],
                    title=title,
                    markdown_content=markdown_content,
                    html_content=html_content,
                    hierarchy=hierarchy,
                    mobile_url=mobile_url,
                    startdate=JSON_START_DATE,
                    enddate=JSON_END_DATE,
                )
                if json_payload.get("success"):
                    json_data = json_payload.get("json_data", {})
                else:
                    json_data = self._build_fallback_json(
                        result["url"],
                        mobile_url,
                        title,
                        hierarchy,
                        markdown_content,
                    )
            except Exception as exc:  # pragma: no cover
                logger.error("JSON ë³€í™˜ ì‹¤íŒ¨ %s: %s", result["url"], exc)
                json_data = self._build_fallback_json(
                    result["url"],
                    mobile_url,
                    title,
                    hierarchy,
                    markdown_content,
                    error=str(exc),
                )

            metadata = json_data.setdefault("metadata", {})
            metadata.update(await self._build_media_metadata(html_content, result["url"]))
            # source í•„ë“œëŠ” ì›ë³¸ HTML/ë©”ë‰´ ì •ë³´ ì¶”ì ìš© ë©”íƒ€ë°ì´í„° (ì „í™˜ í›„ ê²€í†  ê°€ëŠ¥)
            json_data["source"] = {
                "title": result.get("title"),
                "menu_path": menu.menu_path if menu else None,
            }
            json_results.append(json_data)
            
        return json_results
            
    async def _resolve_hierarchy_and_title(self, result: Dict[str, Any], menu: Optional[MenuLink]) -> Tuple[List[str], str]:
        if menu:
            hierarchy = [segment.strip() for segment in menu.menu_path.split(MENU_PATH_DELIMITER) if segment.strip()]
            title = hierarchy[-1] if hierarchy else (result.get("title") or "ì œëª© ì—†ìŒ")
            return hierarchy, title

        meta_title = await self._extract_meta_title(result.get("html_content", ""))
        if meta_title:
            return [], meta_title

        fallback_title = result.get("title") or "ì œëª© ì—†ìŒ"
        return [], fallback_title

    async def _extract_meta_title(self, html_content: str) -> Optional[str]:
        if not html_content:
            return None
        try:
            meta_result = await crawler_tools.extract_meta_title(html_content)
            if meta_result.get("success") and meta_result.get("title"):
                return meta_result.get("title")
        except Exception as exc:  # pragma: no cover
            logger.warning("Meta title ì¶”ì¶œ ì‹¤íŒ¨: %s", exc)
        return None

    async def _build_media_metadata(self, html_content: str, base_url: str) -> Dict[str, Any]:
        if not html_content:
            return {}
        metadata: Dict[str, Any] = {}
        try:
            images = await crawler_tools.extract_images(html_content, base_url)
            if images.get("success") and images.get("images"):
                metadata["images"] = images.get("images", [])
        except Exception as exc:  # pragma: no cover
            logger.warning("ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: %s", exc)
        try:
            links = await crawler_tools.extract_links(html_content, base_url)
            if links.get("success") and links.get("links"):
                metadata["links"] = [link for link in links.get("links", []) if link.get("url")]
        except Exception as exc:  # pragma: no cover
            logger.warning("ë§í¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: %s", exc)
        return metadata

    def _build_fallback_json(
        self,
        url: str,
        mobile_url: Optional[str],
        title: str,
        hierarchy: List[str],
        markdown_content: str,
        *,
        error: Optional[str] = None,
    ) -> Dict[str, Any]:
        payload = {
            "url": url,
            "murl": mobile_url or "",
            "hierarchy": hierarchy or [],
            "title": title,
            "text": markdown_content.replace("\n", "\\n"),
            "startdate": JSON_START_DATE,
            "enddate": JSON_END_DATE,
            "metadata": {},
        }
        if error:
            payload["error"] = error
        return payload

    # ----------------------------------------------------------------------------------
    # SSE helper
    # ----------------------------------------------------------------------------------
    async def _send_update(self, task_id: str, event_type: str, data: Dict[str, Any]) -> None:
        queue = self.task_streams.get(task_id)
        if not queue:
            return
        message = json.dumps({"type": event_type, "data": data, "timestamp": datetime.now().isoformat()}, ensure_ascii=False)
        try:
            await queue.put(message)
        except Exception as exc:  # pragma: no cover
            logger.error("[RAG SSE] Failed to send update for task %s: %s", task_id, exc)

    # ----------------------------------------------------------------------------------
    # Markdown saving helpers
    # ----------------------------------------------------------------------------------
    def _sanitize_filename(self, url: str, title: Optional[str] = None) -> str:
        """ì œëª©ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ URL ê¸°ë°˜ íŒŒì¼ëª… ìƒì„±"""
        base = title.strip() if title and title.strip() and title != "None" else ""
        if base:
            name = re.sub(r"[^0-9A-Za-zê°€-í£._-]", "_", base)
        else:
            parsed = urlparse(url)
            domain = (parsed.netloc or "unknown").replace("www.", "")
            stem = parsed.path.strip("/").split("/")[-1] or "index"
            stem = stem.split(".")[0]
            name = f"{domain}_{stem}" if stem and stem != "index" else domain
            name = re.sub(r"[^0-9A-Za-z._-]", "_", name)
        return name[:80] or "untitled"

    def _save_markdown_file(self, url: str, title: Optional[str], markdown_content: str) -> Optional[str]:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥ í›„ ê²½ë¡œ ë°˜í™˜"""
        if not markdown_content.strip():
            logger.warning("âš ï¸ ë§ˆí¬ë‹¤ìš´ ë‚´ìš©ì´ ë¹„ì–´ìˆì–´ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤: %s", url)
            return None

        try:
            MARKDOWN_RESULT_DIR.mkdir(parents=True, exist_ok=True)
            filename = f"{self._sanitize_filename(url, title)}_{datetime.now():%Y%m%d_%H%M%S}.md"
            file_path = MARKDOWN_RESULT_DIR / filename
            heading = title if title and title != "None" else url
            payload = (
                f"# {heading}\n\n"
                f"**URL:** {url}\n\n"
                f"**ì¶”ì¶œ ì‹œê°„:** {datetime.now():%Y-%m-%d %H:%M:%S}\n\n"
                "---\n\n"
                f"{markdown_content}\n"
            )
            file_path.write_text(payload, encoding="utf-8")
            logger.info("âœ… ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥ ì™„ë£Œ: %s", file_path)
            return str(file_path)
        except Exception as exc:  # pragma: no cover
            logger.error("âŒ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ (%s): %s", url, exc)
            return None

    async def _save_single_markdown_file(self, task_id: str, result_data: Dict[str, Any], idx: int, total: int) -> None:
        """ë‹¨ì¼ ê²°ê³¼ë¥¼ ì¦‰ì‹œ ì €ì¥"""
        url = result_data.get("url", "")
        markdown_content = result_data.get("markdown", "")
        title = result_data.get("title")

        status_prefix = f"{idx}/{total}"
        await self._send_update(task_id, "status", {"message": f"ë§ˆí¬ë‹¤ìš´ ì €ì¥ ì¤‘: {status_prefix} - {title or url}", "status": "active"})

        file_path = self._save_markdown_file(url, title, markdown_content)
        if file_path:
            logger.info("âœ… %s ë§ˆí¬ë‹¤ìš´ ì €ì¥ ì™„ë£Œ: %s", status_prefix, file_path)
            await self._send_update(task_id, "status", {"message": f"âœ… ì €ì¥ ì™„ë£Œ: {status_prefix}", "status": "active"})
        else:
            logger.warning("âš ï¸ %s ë§ˆí¬ë‹¤ìš´ ì €ì¥ ì‹¤íŒ¨", status_prefix)
            await self._send_update(task_id, "status", {"message": f"âŒ ì €ì¥ ì‹¤íŒ¨: {status_prefix}", "status": "active"})


crawling_service = RAGCrawlingService()