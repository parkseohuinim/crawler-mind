"""RAG Crawling Service - Based on rag-scraping app.py workflow"""
import asyncio
import re
import uuid
from datetime import datetime
from typing import Dict, Optional, AsyncGenerator, Any, List
import logging
import json

from app.models import TaskStatus, CrawlingResult, TaskResult, ProcessingMode
from app.infrastructure.mcp.mcp_service import mcp_service
from app.infrastructure.llm.llm_service import llm_service

logger = logging.getLogger(__name__)

class RAGCrawlingService:
    """RAG ìŠ¤í¬ë˜í•‘ ì„œë¹„ìŠ¤ - rag-scrapingì˜ app.py ì›Œí¬í”Œë¡œìš° ê¸°ë°˜"""
    
    def __init__(self):
        self.tasks: Dict[str, TaskResult] = {}
        self.task_streams: Dict[str, asyncio.Queue] = {}
        
    def create_task(self, urls_input: str) -> str:
        """ìƒˆ í¬ë¡¤ë§ ì‘ì—… ìƒì„±"""
        task_id = str(uuid.uuid4())
        
        task_result = TaskResult(
            taskId=task_id,
            status=TaskStatus.PENDING,
            createdAt=datetime.now().isoformat()
        )
        
        self.tasks[task_id] = task_result
        self.task_streams[task_id] = asyncio.Queue()
        
        logger.info(f"âœ… RAG Task created: {task_id} for URLs: {urls_input[:100]}...")
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‘ì—… ì‹œì‘
        asyncio.create_task(self._process_rag_task(task_id, urls_input))
        
        return task_id
    
    def get_task(self, task_id: str) -> Optional[TaskResult]:
        """ì‘ì—… ì¡°íšŒ"""
        return self.tasks.get(task_id)
    
    async def get_task_stream(self, task_id: str) -> AsyncGenerator[str, None]:
        """SSE ìŠ¤íŠ¸ë¦¼ ì œê³µ"""
        logger.info(f"ğŸ” RAG SSE stream requested for task: {task_id}")
        
        if task_id not in self.task_streams:
            logger.error(f"âŒ RAG Task stream not found: {task_id}")
            yield f"data: {json.dumps({'type': 'error', 'data': {'message': 'Task not found'}})}\n\n"
            return
            
        # ì´ˆê¸° ì—°ê²° í™•ì¸
        yield f"data: {json.dumps({'type': 'connected', 'data': {'message': 'RAG Stream connected'}})}\n\n"
            
        queue = self.task_streams[task_id]
        
        try:
            while True:
                try:
                    message = await asyncio.wait_for(queue.get(), timeout=30.0)
                    yield f"data: {message}\n\n"
                    
                    # ì™„ë£Œ ì‹ í˜¸ í™•ì¸
                    try:
                        msg_data = json.loads(message)
                        if msg_data.get('type') in ['final', 'complete', 'error']:
                            logger.warning(f"ğŸ”š [RAG SSE] Sending final event, waiting before close...")
                            await asyncio.sleep(0.5)
                            break
                    except:
                        pass
                    
                    # ì‘ì—… ì™„ë£Œ í™•ì¸
                    task = self.tasks.get(task_id)
                    if task and task.status in [TaskStatus.COMPLETED, TaskStatus.FAILED]:
                        break
                        
                except asyncio.TimeoutError:
                    # í•˜íŠ¸ë¹„íŠ¸
                    yield f"data: {json.dumps({'type': 'heartbeat', 'data': {}})}\n\n"
                    
                    task = self.tasks.get(task_id)
                    if task and task.status in [TaskStatus.COMPLETED, TaskStatus.FAILED]:
                        yield f"data: {json.dumps({'type': 'complete', 'data': {'message': 'RAG ì‘ì—… ì™„ë£Œ'}})}\n\n"
                        break
                    
        except Exception as e:
            logger.error(f"RAG Task stream error {task_id}: {e}")
            try:
                yield f"data: {json.dumps({'type': 'error', 'data': {'message': str(e)}})}\n\n"
            except:
                pass
        finally:
            logger.info(f"Cleaning up RAG task stream: {task_id}")
            if task_id in self.task_streams:
                del self.task_streams[task_id]
    
    async def _process_rag_task(self, task_id: str, urls_input: str):
        """RAG ì‘ì—… ì²˜ë¦¬ - rag-scraping app.pyì˜ ì „ì²´ ì›Œí¬í”Œë¡œìš°"""
        try:
            # ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
            self.tasks[task_id].status = TaskStatus.RUNNING
            await self._send_update(task_id, 'status', {'message': 'RAG í¬ë¡¤ë§ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...', 'status': 'active'})

            # 1ë‹¨ê³„: URL ì¶”ì¶œ ë° ê²€ì¦
            await self._send_update(task_id, 'status', {'message': 'URL ëª©ë¡ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
            urls_list = await self._extract_urls_from_input(urls_input)
            
            if not urls_list:
                raise Exception("ìœ íš¨í•œ URLì´ ì—†ìŠµë‹ˆë‹¤")
            
            logger.info(f"ğŸ“‹ RAG Task {task_id}: {len(urls_list)} URLs ì¶”ì¶œë¨")

            # 2ë‹¨ê³„: ë°ì´í„° ìŠ¤í¬ë˜í•‘ (rag-scrapingì˜ scrape_data ë‹¨ê³„)
            await self._send_update(task_id, 'status', {'message': f'{len(urls_list)}ê°œ URL í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...', 'status': 'active'})
            scraped_results = await self._scrape_data(task_id, urls_list)
            
            if not scraped_results:
                raise Exception("í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

            # 3ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ (rag-scrapingì˜ preprocess_data ë‹¨ê³„)
            await self._send_update(task_id, 'status', {'message': 'ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤...', 'status': 'active'})
            processed_results = await self._preprocess_data(task_id, scraped_results)

            # 4ë‹¨ê³„: JSON ë³€í™˜ (rag-scrapingì˜ convert_to_json ë‹¨ê³„)
            await self._send_update(task_id, 'status', {'message': 'JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤...', 'status': 'active'})
            json_results = await self._convert_to_json(task_id, processed_results)

            # 5ë‹¨ê³„: AI ë¶„ì„ (ì„ íƒì , ë§í¬ ì¶”ì¶œ ì œì™¸)
            await self._send_update(task_id, 'status', {'message': 'AI ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...', 'status': 'active'})
            final_results = await self._analyze_with_ai(task_id, json_results)

            # ìµœì¢… ê²°ê³¼ êµ¬ì„± (ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±°)
            result = CrawlingResult(
                json_data=final_results  # í•µì‹¬ JSON ê²°ê³¼ë§Œ í¬í•¨
                # title, textLength, linkCount, links, summary, screenshot, error ì œê±°
            )

            # ì‘ì—… ì™„ë£Œ
            self.tasks[task_id].result = result
            self.tasks[task_id].status = TaskStatus.COMPLETED
            self.tasks[task_id].completedAt = datetime.now().isoformat()
            
            await self._send_update(task_id, 'final', result.model_dump())
            await self._send_update(task_id, 'complete', {'message': 'RAG í¬ë¡¤ë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤'})
            
        except Exception as e:
            logger.error(f"RAG Task {task_id} failed: {e}")
            
            # ì—ëŸ¬ ì²˜ë¦¬
            self.tasks[task_id].status = TaskStatus.FAILED
            self.tasks[task_id].error = str(e)
            self.tasks[task_id].completedAt = datetime.now().isoformat()
            
            await self._send_update(task_id, 'error', {'message': str(e)})

    async def _extract_urls_from_input(self, raw_input: str) -> List[str]:
        """ì…ë ¥ì—ì„œ URL ì¶”ì¶œ - LLM ë¶„ì„ + ì •ê·œì‹ ë°±ì—…"""
        try:
            # 1ë‹¨ê³„: ì •ê·œì‹ìœ¼ë¡œ ê¸°ë³¸ URL ì¶”ì¶œ
            regex_urls = re.findall(r"https?://[^\s,;]+", raw_input)
            
            # 2ë‹¨ê³„: LLMì„ ì‚¬ìš©í•œ ê³ ê¸‰ ë¶„ì„ (ìì—°ì–´ ì²˜ë¦¬)
            llm_urls = await self._extract_urls_with_llm(raw_input)
            
            # 3ë‹¨ê³„: ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
            all_urls = regex_urls + llm_urls
            ordered_unique: List[str] = []
            seen = set()
            
            for url in all_urls:
                # URL ì •ë¦¬ (ëì˜ íŠ¹ìˆ˜ë¬¸ì ì œê±°)
                clean_url = url.rstrip('.,;)]}')
                if clean_url and clean_url not in seen and clean_url.startswith(('http://', 'https://')):
                    ordered_unique.append(clean_url)
                    seen.add(clean_url)
            
            logger.info(f"ğŸ” URL ì¶”ì¶œ ì™„ë£Œ: {len(ordered_unique)}ê°œ (ì •ê·œì‹: {len(regex_urls)}, LLM: {len(llm_urls)})")
            return ordered_unique
            
        except Exception as e:
            logger.error(f"URL ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    async def _extract_urls_with_llm(self, raw_input: str) -> List[str]:
        """LLMì„ ì‚¬ìš©í•œ URL ì¶”ì¶œ ë° ìì—°ì–´ ë¶„ì„"""
        try:
            # LLM ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            from app.infrastructure.llm.llm_service import llm_service
            
            # ìì—°ì–´ ì…ë ¥ì—ì„œ URL ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
            extraction_prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì›¹í˜ì´ì§€ URLì„ ëª¨ë‘ ì¶”ì¶œí•´ì£¼ì„¸ìš”. 
ì‚¬ìš©ìê°€ í¬ë¡¤ë§í•˜ê³  ì‹¶ì–´í•˜ëŠ” ì›¹í˜ì´ì§€ë“¤ì„ ì°¾ì•„ì„œ ì™„ì „í•œ URL í˜•íƒœë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.

ì…ë ¥ í…ìŠ¤íŠ¸:
{raw_input}

ê·œì¹™:
1. http:// ë˜ëŠ” https://ë¡œ ì‹œì‘í•˜ëŠ” ì™„ì „í•œ URLë§Œ ì¶”ì¶œ
2. ë¶ˆì™„ì „í•œ URLì´ë‚˜ ë„ë©”ì¸ë§Œ ìˆëŠ” ê²½ìš°ëŠ” ì œì™¸
3. í•œ ì¤„ì— í•˜ë‚˜ì”©, URLë§Œ ì¶œë ¥
4. ì¤‘ë³µ ì œê±°
5. URLì´ ì—†ìœ¼ë©´ "NO_URLS" ì¶œë ¥

ì˜ˆì‹œ ì¶œë ¥:
https://example1.com/page1
https://example2.com/page2
"""

            # LLM í˜¸ì¶œ
            response = await llm_service.query(extraction_prompt, [])
            
            if not response or "NO_URLS" in response:
                logger.info("ğŸ¤– LLM: URLì„ ì°¾ì§€ ëª»í•¨")
                return []
            
            # ì‘ë‹µì—ì„œ URL ì¶”ì¶œ
            extracted_urls = []
            for line in response.strip().split('\n'):
                line = line.strip()
                if line and (line.startswith('http://') or line.startswith('https://')):
                    extracted_urls.append(line)
            
            logger.info(f"ğŸ¤– LLM URL ì¶”ì¶œ ì„±ê³µ: {len(extracted_urls)}ê°œ")
            return extracted_urls
            
        except Exception as e:
            logger.warning(f"âš ï¸ LLM URL ì¶”ì¶œ ì‹¤íŒ¨, ì •ê·œì‹ìœ¼ë¡œ ëŒ€ì²´: {e}")
            return []

    async def _scrape_data(self, task_id: str, urls_list: List[str]) -> List[Dict[str, Any]]:
        """ë°ì´í„° ìŠ¤í¬ë˜í•‘ - rag-scrapingì˜ scrape_data ë©”ì„œë“œ ê¸°ë°˜"""
        try:
            # MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ë§
            await self._send_update(task_id, 'status', {'message': 'ìˆœì°¨ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...', 'status': 'active'})
            
            scraped_results = []
            for i, url in enumerate(urls_list, 1):
                await self._send_update(task_id, 'status', {
                    'message': f'í¬ë¡¤ë§ ì§„í–‰: {i}/{len(urls_list)} - {url}',
                    'status': 'active'
                })
                
                # crawl4ai_scrape ë„êµ¬ ì‚¬ìš©
                try:
                    result = await mcp_service.call_tool("crawl4ai_scrape", {"url": url})
                    
                    # ê²°ê³¼ ì²˜ë¦¬
                    if hasattr(result, 'structured_content'):
                        data = result.structured_content
                    elif hasattr(result, 'data'):
                        data = result.data
                    else:
                        data = result
                    
                    if isinstance(data, dict) and data.get('success'):
                        scraped_results.append({
                            'url': url,
                            'title': data.get('title', 'ì œëª© ì—†ìŒ'),
                            'html_content': data.get('html_content', ''),
                            'markdown': data.get('markdown', ''),
                            'status_code': data.get('status_code'),
                            'success': True
                        })
                        logger.info(f"âœ… RAG í¬ë¡¤ë§ ì„±ê³µ: {url}")
                    else:
                        error_msg = data.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜') if isinstance(data, dict) else 'ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜'
                        scraped_results.append({
                            'url': url,
                            'title': 'í¬ë¡¤ë§ ì‹¤íŒ¨',
                            'html_content': '',
                            'markdown': '',
                            'error': error_msg,
                            'success': False
                        })
                        logger.warning(f"âš ï¸ RAG í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {error_msg}")
                        
                except Exception as e:
                    logger.error(f"âŒ RAG í¬ë¡¤ë§ ë„êµ¬ í˜¸ì¶œ ì‹¤íŒ¨: {url} - {e}")
                    scraped_results.append({
                        'url': url,
                        'title': 'í¬ë¡¤ë§ ì‹¤íŒ¨',
                        'html_content': '',
                        'markdown': '',
                        'error': str(e),
                        'success': False
                    })

            successful_count = len([r for r in scraped_results if r.get('success', False)])
            logger.info(f"ğŸ“Š RAG í¬ë¡¤ë§ ì™„ë£Œ - ì„±ê³µ: {successful_count}/{len(urls_list)}")
            
            return scraped_results
            
        except Exception as e:
            logger.error(f"RAG ìŠ¤í¬ë˜í•‘ ë‹¨ê³„ ì‹¤íŒ¨: {e}")
            raise

    async def _preprocess_data(self, task_id: str, scraped_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë°ì´í„° ì „ì²˜ë¦¬ - í…ìŠ¤íŠ¸ ì •ë¦¬ ë° êµ¬ì¡°í™”"""
        try:
            processed_results = []
            
            for i, result in enumerate(scraped_results, 1):
                await self._send_update(task_id, 'status', {
                    'message': f'ì „ì²˜ë¦¬ ì§„í–‰: {i}/{len(scraped_results)} - {result["url"]}',
                    'status': 'active'
                })
                
                if not result.get('success', False):
                    # ì‹¤íŒ¨í•œ í•­ëª©ë„ í¬í•¨ (ì—ëŸ¬ ì •ë³´ ìœ ì§€)
                    processed_results.append(result)
                    continue
                
                # ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ ì •ë¦¬
                markdown_content = result.get('markdown', '')
                
                # ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì •ë¦¬
                if markdown_content:
                    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
                    markdown_content = re.sub(r'\n\s*\n', '\n\n', markdown_content)
                    markdown_content = re.sub(r' +', ' ', markdown_content)
                    markdown_content = markdown_content.strip()
                
                # ì „ì²˜ë¦¬ëœ ê²°ê³¼ êµ¬ì„±
                processed_result = {
                    **result,
                    'processed_markdown': markdown_content,
                    'processed_at': datetime.now().isoformat(),
                    'text_length': len(markdown_content),
                    'word_count': len(markdown_content.split()) if markdown_content else 0
                }
                
                processed_results.append(processed_result)
            
            logger.info(f"ğŸ“ RAG ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_results)}ê°œ í•­ëª©")
            return processed_results
            
        except Exception as e:
            logger.error(f"RAG ì „ì²˜ë¦¬ ë‹¨ê³„ ì‹¤íŒ¨: {e}")
            raise

    async def _convert_to_json(self, task_id: str, processed_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜ - rag-scrapingì˜ to_json.py ê¸°ë°˜"""
        try:
            json_results = []
            
            for i, result in enumerate(processed_results, 1):
                await self._send_update(task_id, 'status', {
                    'message': f'JSON ë³€í™˜ ì§„í–‰: {i}/{len(processed_results)} - {result["url"]}',
                    'status': 'active'
                })
                
                if not result.get('success', False):
                    # ì‹¤íŒ¨í•œ í•­ëª©ì€ ì—ëŸ¬ ì •ë³´ë§Œ í¬í•¨
                    json_results.append({
                        'url': result['url'],
                        'title': result.get('title', 'í¬ë¡¤ë§ ì‹¤íŒ¨'),
                        'text': '',
                        'error': result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'),
                        'status': 'failed'
                    })
                    continue
                
                # MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                try:
                    url = result['url']
                    title = result.get('title') or 'ì œëª© ì—†ìŒ'  # None ì²˜ë¦¬
                    markdown_content = result.get('processed_markdown', result.get('markdown', ''))
                    html_content = result.get('html_content', '')
                    
                    # hierarchyëŠ” í˜„ì¬ ë©”ë‰´ ì •ë³´ê°€ ì—†ì–´ ì£¼ì„ ì²˜ë¦¬
                    # from urllib.parse import urlparse
                    # parsed_url = urlparse(url)
                    # hierarchy = [parsed_url.netloc, title]
                    
                    # convert_to_json_format ë„êµ¬ í˜¸ì¶œ (hierarchy ì œê±°)
                    json_result = await mcp_service.call_tool("convert_to_json_format", {
                        "url": url,
                        "title": title,  # ì´ì œ í•­ìƒ ë¬¸ìì—´
                        "markdown_content": markdown_content,
                        "html_content": html_content
                        # hierarchy ë§¤ê°œë³€ìˆ˜ ì œê±°
                    })
                    
                    # ê²°ê³¼ ì²˜ë¦¬
                    if hasattr(json_result, 'structured_content'):
                        data = json_result.structured_content
                    elif hasattr(json_result, 'data'):
                        data = json_result.data
                    else:
                        data = json_result
                    
                    if isinstance(data, dict) and data.get('success'):
                        json_data = data.get('json_data', {})
                        json_results.append(json_data)
                        logger.info(f"âœ… RAG JSON ë³€í™˜ ì„±ê³µ: {url}")
                    else:
                        # ë³€í™˜ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í˜•ì‹ìœ¼ë¡œ ìƒì„± (ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±°)
                        json_results.append({
                            'url': url,
                            'title': title,  # ì´ë¯¸ ìœ„ì—ì„œ None ì²˜ë¦¬ë¨
                            'text': markdown_content.replace('\n', '\\n'),
                            # 'hierarchy': hierarchy,  # ì£¼ì„ ì²˜ë¦¬
                            # 'status': 'new',  # ì£¼ì„ ì²˜ë¦¬
                            'startdate': '0000-00-00',
                            'enddate': '9999-99-99',
                            'metadata': {}
                        })
                        logger.warning(f"âš ï¸ RAG JSON ë³€í™˜ ì‹¤íŒ¨, ê¸°ë³¸ í˜•ì‹ ì‚¬ìš©: {url}")
                        
                except Exception as e:
                    logger.error(f"âŒ RAG JSON ë³€í™˜ ë„êµ¬ í˜¸ì¶œ ì‹¤íŒ¨: {result['url']} - {e}")
                    # ê¸°ë³¸ í˜•ì‹ìœ¼ë¡œ ìƒì„± (ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±°)
                    safe_title = result.get('title') or 'ì œëª© ì—†ìŒ'  # None ì²˜ë¦¬
                    json_results.append({
                        'url': result['url'],
                        'title': safe_title,
                        'text': result.get('processed_markdown', '').replace('\n', '\\n'),
                        # 'hierarchy': [result['url']],  # ì£¼ì„ ì²˜ë¦¬
                        # 'status': 'new',  # ì£¼ì„ ì²˜ë¦¬
                        'error': str(e)
                    })

            successful_count = len([r for r in json_results if not r.get('error')])
            logger.info(f"ğŸ“„ RAG JSON ë³€í™˜ ì™„ë£Œ - ì„±ê³µ: {successful_count}/{len(processed_results)}")
            
            return json_results
            
        except Exception as e:
            logger.error(f"RAG JSON ë³€í™˜ ë‹¨ê³„ ì‹¤íŒ¨: {e}")
            raise

    async def _analyze_with_ai(self, task_id: str, json_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """AI ë¶„ì„ - ì œì•½ì‚¬í•­: ë§í¬ ì¶”ì¶œ ê³¼ì • ì œì™¸"""
        try:
            analyzed_results = []
            
            for i, json_item in enumerate(json_results, 1):
                await self._send_update(task_id, 'status', {
                    'message': f'AI ë¶„ì„ ì§„í–‰: {i}/{len(json_results)} - {json_item.get("url", "Unknown")}',
                    'status': 'active'
                })
                
                if json_item.get('error'):
                    # ì—ëŸ¬ê°€ ìˆëŠ” í•­ëª©ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
                    analyzed_results.append(json_item)
                    continue
                
                # AI ë¶„ì„ ìˆ˜í–‰ (ë§í¬ ì¶”ì¶œ ì œì™¸)
                try:
                    text_content = json_item.get('text', '').replace('\\n', '\n')
                    
                    if text_content and len(text_content.strip()) > 50:
                        # AIì—ê²Œ ì»¨í…ì¸  ë¶„ì„ ìš”ì²­ (ë§í¬ ì •ë³´ ì œì™¸)
                        analysis_prompt = f"""ë‹¤ìŒ ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

URL: {json_item.get('url', '')}
ì œëª©: {json_item.get('title', '')}

ë‚´ìš©:
{text_content[:2000]}{'...' if len(text_content) > 2000 else ''}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. [ìš”ì•½] 
- í˜ì´ì§€ì˜ ì£¼ìš” ëª©ì ê³¼ ë‚´ìš© (100ì ì´ë‚´)ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
2. [í‚¤ì›Œë“œ] 
- í•µì‹¬ í‚¤ì›Œë“œ 3-5ê°œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
3. [íŠ¹ì§•] 
- ì‚¬ìš©ìì—ê²Œ ìœ ìš©í•œ ì •ë³´ë‚˜ íŠ¹ì§•ì„ ì‘ì„±í•©ë‹ˆë‹¤.

ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."""
                        
                        try:
                            ai_response = await llm_service.query(analysis_prompt, [])
                            
                            if ai_response and len(ai_response.strip()) > 10:
                                # AI ë¶„ì„ ê²°ê³¼ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
                                if 'metadata' not in json_item:
                                    json_item['metadata'] = {}
                                json_item['metadata']['ai_analysis'] = ai_response.strip()
                                # ai_summary í•„ë“œëŠ” ì¤‘ë³µì´ë¯€ë¡œ ì œê±°
                                logger.info(f"âœ… RAG AI ë¶„ì„ ì„±ê³µ: {json_item.get('url', '')}")
                            else:
                                logger.warning(f"âš ï¸ RAG AI ë¶„ì„ ì‘ë‹µ ë¶€ì¡±: {json_item.get('url', '')}")
                                
                        except Exception as ai_e:
                            logger.warning(f"âš ï¸ RAG AI ë¶„ì„ ì‹¤íŒ¨: {json_item.get('url', '')} - {ai_e}")
                    
                    analyzed_results.append(json_item)
                    
                except Exception as e:
                    logger.error(f"âŒ RAG AI ë¶„ì„ ì²˜ë¦¬ ì‹¤íŒ¨: {json_item.get('url', '')} - {e}")
                    analyzed_results.append(json_item)

            logger.info(f"ğŸ¤– RAG AI ë¶„ì„ ì™„ë£Œ: {len(analyzed_results)}ê°œ í•­ëª©")
            return analyzed_results
            
        except Exception as e:
            logger.error(f"RAG AI ë¶„ì„ ë‹¨ê³„ ì‹¤íŒ¨: {e}")
            # AI ë¶„ì„ ì‹¤íŒ¨ì‹œì—ë„ ê¸°ì¡´ ê²°ê³¼ ë°˜í™˜
            return json_results

    async def _send_update(self, task_id: str, event_type: str, data: dict):
        """ì‘ì—… ìŠ¤íŠ¸ë¦¼ì— ì—…ë°ì´íŠ¸ ì „ì†¡"""
        if task_id in self.task_streams:
            message = json.dumps({
                'type': event_type,
                'data': data,
                'timestamp': datetime.now().isoformat()
            }, ensure_ascii=False)  # í•œê¸€ ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„ ë°©ì§€
            
            logger.warning(f"ğŸ“¤ [RAG SSE] Sending {event_type} event to task {task_id}: {message[:100]}...")
            
            try:
                await self.task_streams[task_id].put(message)
                logger.warning(f"âœ… [RAG SSE] Successfully queued {event_type} event")
            except Exception as e:
                logger.error(f"âŒ [RAG SSE] Failed to send update for task {task_id}: {e}")

# Global service instance
crawling_service = RAGCrawlingService()