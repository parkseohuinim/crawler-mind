"""
Í∏ÄÎ°úÎ≤åÎ°úÎ∞ç Í¥ÄÎ†® Ìï∏Îì§Îü¨

Î°úÎ∞ç Í≥µÏßÄÏÇ¨Ìï≠ Î™©Î°ù Î∞è ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄ Ï≤òÎ¶¨
"""

import asyncio
import logging
import re
from datetime import datetime, timedelta
from typing import Any, Dict, Optional

from playwright.async_api import async_playwright
from markdownify import markdownify as md

from ..handler_registry import register_page_handler
from ..utils import (
    sanitize_filename, 
    format_content, 
    create_markdown, 
    to_mglobalroaming_url,
    smart_goto
)

logger = logging.getLogger(__name__)


async def handle_roaming_notice(
    url: str, 
    fclient: Any, 
    list_date: Optional[str] = None, 
    list_title: Optional[str] = None
) -> Dict[str, Any]:
    """
    Î°úÎ∞ç Í≥µÏßÄÏÇ¨Ìï≠ ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄ Ï≤òÎ¶¨
    """
    logger.info(f"üîó Roaming notice detail: {url}")
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        
        try:
            response = await smart_goto(page, url, wait_for_selector='div.txt', timeout=30000)
            
            status_code = response.status if response else None
            if status_code:
                if status_code >= 400:
                    logger.error(f"‚ùå HTTP {status_code}: {url}")
                elif status_code >= 300:
                    logger.warning(f"‚ö†Ô∏è HTTP {status_code} redirect: {url}")
                else:
                    logger.info(f"‚úÖ HTTP {status_code}: {url}")
            
            metadata = await page.evaluate("""() => {
                const getText = (sel) => {
                    const el = document.querySelector(sel);
                    return el ? (el.textContent || '').trim() : '';
                };

                let title = getText('.contents-title') ||
                            getText('.board-view .subject') ||
                            getText('h2.title') ||
                            getText('.title') ||
                            getText('h1');
                if (!title) {
                    const og = document.querySelector('meta[property="og:title"]')?.getAttribute('content')?.trim();
                    if (og) title = og;
                }

                const rawDate = getText('.reg_date') ||
                                getText('.date') ||
                                getText('.info .date') ||
                                getText('.board-info .date') ||
                                'ÎÇ†Ïßú Ï†ïÎ≥¥ ÏóÜÏùå';

                const contentElement = document.querySelector('div.txt') ||
                                       document.querySelector('.board-content') ||
                                       document.querySelector('#cfmClContents') ||
                                       document.querySelector('.content') ||
                                       document.querySelector('.board-body');
                const contentHtml = contentElement ? contentElement.innerHTML : '';

                let nextLink = '';
                let nextText = '';
                const anchors = Array.from(document.querySelectorAll('a[href]'));
                for (const a of anchors) {
                    const t = (a.textContent || '').trim();
                    if (/Îã§ÏùåÍ∏Ä|Îã§Ïùå|Next/i.test(t)) {
                        nextLink = a.getAttribute('href') || '';
                        nextText = t;
                        break;
                    }
                }

                return {
                    title: title || 'Ï†úÎ™© ÏóÜÏùå',
                    rawDate,
                    contentHtml,
                    nextLink,
                    nextText
                };
            }""")
            
            await browser.close()
        except Exception as e:
            await browser.close()
            logger.error(f"‚ùå Playwright error: {str(e)}")
            return {"error": f"Playwright Ï≤òÎ¶¨ Ïã§Ìå®: {str(e)}"}
    
    # Ïª®ÌÖêÏ∏† HTMLÏùÑ ÎßàÌÅ¨Îã§Ïö¥ÏúºÎ°ú Î≥ÄÌôò
    if metadata['contentHtml']:
        content = md(metadata['contentHtml'])
        logger.info(f"‚úÖ Markdown converted: {len(content)} chars")
    else:
        logger.info("‚ö†Ô∏è No HTML, trying fallback")
        try:
            result = await fclient.scrape_single_url(url)
            if result.get("markdown"):
                content = result["markdown"]
                logger.info(f"‚úÖ Fallback success: {len(content)} chars")
            else:
                content = "Ïª®ÌÖêÏ∏† Ïä§ÌÅ¨ÎûòÌïë Ïã§Ìå®"
                logger.error("‚ùå Fallback failed")
        except Exception as e:
            content = "Ïª®ÌÖêÏ∏† Ïä§ÌÅ¨ÎûòÌïë Ïã§Ìå®"
            logger.error(f"‚ùå Fallback failed: {str(e)}")
    
    # Ïπ¥ÌÖåÍ≥†Î¶¨ÏôÄ ÎÇ†Ïßú Î∂ÑÎ¶¨ Ï≤òÎ¶¨
    category = ''
    category_date_match = re.search(r'^(.+?)\s*(\d{4}[\.\-]\d{2}[\.\-]\d{2})', metadata['rawDate'])
    if category_date_match:
        category = category_date_match.group(1).strip()
        actual_date = category_date_match.group(2).replace('-', '.')
    else:
        date_only_match = re.search(r'(\d{4}[\.\-]\d{2}[\.\-]\d{2})', metadata['rawDate'])
        if date_only_match:
            actual_date = date_only_match.group(1).replace('-', '.')
        else:
            if list_date:
                list_date_match = re.search(r'(\d{4}[\.\-]\d{2}[\.\-]\d{2})', list_date)
                if list_date_match:
                    actual_date = list_date_match.group(1).replace('-', '.')
                else:
                    actual_date = ''
            else:
                actual_date = ''
    
    # ÎßàÌÅ¨Îã§Ïö¥ ÏΩòÌÖêÏ∏† Ìè¨Îß∑ÌåÖ
    formatted_content = format_content(content)
    date_display = f"{actual_date}" + (f" (Ïπ¥ÌÖåÍ≥†Î¶¨: {category})" if category else "")
    
    final_title = metadata['title'] if metadata['title'] else (list_title or 'Ï†úÎ™© ÏóÜÏùå')
    markdown_content = create_markdown(final_title, date_display, formatted_content)
    
    # Îã§ÏùåÍ∏Ä URL Ï≤òÎ¶¨
    next_url = None
    if metadata['nextLink']:
        base_url = url.split('/news/')[0]
        next_url = f"{base_url}/news/{metadata['nextLink']}"
    
    # startdate/enddate Í≥ÑÏÇ∞
    def _normalize_hyphen_date(s: str) -> str:
        s = s.strip()
        m = re.search(r"(\d{4})[\.\-ÎÖÑ]\s*(\d{1,2})[\.\-Ïõî]\s*(\d{1,2})", s)
        if not m:
            m = re.search(r"(\d{4})[\.\-](\d{1,2})[\.\-](\d{1,2})", s)
        if m:
            y, mo, d = m.group(1), m.group(2), m.group(3)
            return f"{y}-{mo.zfill(2)}-{d.zfill(2)}"
        return ""

    startdate_hyphen = "0000-00-00"
    enddate_hyphen = "9999-99-99"

    text_for_range = (metadata.get('contentHtml') or '') + ' ' + (content or '')
    inherit_left_year = False
    m_range = re.search(
        r"(\d{4}[ÎÖÑ\.\-]\s*\d{1,2}[Ïõî\.\-]\s*\d{1,2}Ïùº?)\s*[~\-‚Äì]\s*(\d{4}[ÎÖÑ\.\-]?\s*\d{1,2}[Ïõî\.\-]\s*\d{1,2}Ïùº?)", 
        text_for_range
    )
    if not m_range:
        m_range = re.search(
            r"(\d{4}[ÎÖÑ\.\-]\s*\d{1,2}[Ïõî\.\-]\s*\d{1,2}Ïùº?)\s*[~\-‚Äì]\s*(\d{1,2}[Ïõî\.\-]\s*\d{1,2}Ïùº?)", 
            text_for_range
        )
        if m_range:
            inherit_left_year = True

    if m_range:
        left = m_range.group(1)
        right = m_range.group(2)
        left_h = _normalize_hyphen_date(left)
        if inherit_left_year and left_h:
            ly = left_h.split('-')[0]
            m_right = re.search(r"(\d{1,2})[Ïõî\.\-]\s*(\d{1,2})", right)
            if m_right:
                right_h = f"{ly}-{m_right.group(1).zfill(2)}-{m_right.group(2).zfill(2)}"
            else:
                right_h = _normalize_hyphen_date(right)
        else:
            right_h = _normalize_hyphen_date(right)
        if left_h:
            startdate_hyphen = left_h
        if right_h:
            enddate_hyphen = right_h
    else:
        if actual_date:
            startdate_hyphen = actual_date.replace('.', '-')

    logger.info(f"‚úÖ Roaming notice done: '{final_title}'")

    return {
        "url": url,
        "murl": to_mglobalroaming_url(url),
        "title": final_title,
        "date": actual_date,
        "markdown": markdown_content,
        "html": metadata['contentHtml'] or content,
        "startdate": startdate_hyphen,
        "enddate": enddate_hyphen,
        "next_url": next_url,
        "special_processed": True,
        "playwright_processed": True
    }


async def handle_globalroaming_notice_main(
    url: str, 
    fclient: Any, 
    menu: Optional[str] = None
) -> Dict[str, Any]:
    """
    Í∏ÄÎ°úÎ≤åÎ°úÎ∞ç Í≥µÏßÄÏÇ¨Ìï≠ Î©îÏù∏ Î™©Î°ù ÌéòÏù¥ÏßÄ Ï≤òÎ¶¨
    - Î™©Î°ùÏóêÏÑú Í≥µÏßÄÏÇ¨Ìï≠ Ï†ïÎ≥¥ Ï∂îÏ∂ú
    - Í∞Å Í≥µÏßÄÏÇ¨Ìï≠ ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄ Ï≤òÎ¶¨
    - 1ÎÖÑ Ïù¥ÎÇ¥ Í≤åÏãúÎ¨ºÎßå Ï≤òÎ¶¨
    """
    logger.info(f"üîó Global roaming notice main: {url}")
    cutoff_date = datetime.now() - timedelta(days=365)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        response = await page.goto(url, wait_until='domcontentloaded', timeout=60000)
        
        # Í≥µÏßÄÏÇ¨Ìï≠ Î™©Î°ù ÎåÄÍ∏∞
        try:
            await page.wait_for_selector('.board-content, table.board', timeout=15000)
            logger.info("‚úÖ Roaming notice list loaded")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Notice list not loaded: {e}")
        await page.wait_for_timeout(2000)
        
        status_code = response.status if response else None
        if status_code:
            if status_code >= 400:
                logger.error(f"‚ùå HTTP {status_code}: {url}")
            elif status_code >= 300:
                logger.warning(f"‚ö†Ô∏è HTTP {status_code} redirect: {url}")
            else:
                logger.info(f"‚úÖ HTTP {status_code}: {url}")
        
        notice_data = await page.evaluate(r"""() => {
            const table = document.querySelector('table.board.dir-vertical');
            if (!table) return { error: 'table not found' };
            
            const notices = [];
            const links = table.querySelectorAll('a[href*="view.asp"]');
            
            links.forEach(link => {
                const row = link.closest('tr');
                if (row) {
                    const cells = row.querySelectorAll('td');
                    if (cells.length >= 4) {
                        notices.push({
                            title: link.textContent.trim(),
                            href: link.href,
                            number: cells[0] ? cells[0].textContent.trim() : '',
                            date: cells[2] ? cells[2].textContent.trim() : '',
                            views: cells[3] ? cells[3].textContent.trim() : ''
                        });
                    }
                }
            });
            
            return { notices: notices };
        }""")
        await browser.close()
    
    notices = notice_data.get('notices', [])
    if not notices:
        return {"message": "No roaming notices found", "total_processed": 0}
    
    logger.info(f"üîç Found {len(notices)} roaming notices")
    
    menus, datas = [], []
    total_processed = 0
    
    consecutive_errors = 0
    max_consecutive_errors = 3
    
    for notice in notices:
        try:
            date_str = notice['date']
            date_match = re.search(r'(\d{4})[\.\-](\d{1,2})[\.\-](\d{1,2})', date_str)
            if date_match:
                year, month, day = map(int, date_match.groups())
                post_date = datetime(year, month, day)
                if post_date < cutoff_date:
                    break
            
            # Í∞úÎ≥Ñ ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄÏóê 120Ï¥à(2Î∂Ñ) ÌÉÄÏûÑÏïÑÏõÉ Ï†ÅÏö©
            try:
                result = await asyncio.wait_for(
                    handle_roaming_notice(notice['href'], fclient, notice.get('date')),
                    timeout=120
                )
                consecutive_errors = 0
            except asyncio.TimeoutError:
                logger.warning(f"‚ö†Ô∏è Timeout (120s): {notice['href']}")
                consecutive_errors += 1
                if consecutive_errors >= max_consecutive_errors:
                    logger.error(f"‚ùå Stopped: {max_consecutive_errors} consecutive failures")
                    break
                continue
            
            if "error" in result:
                logger.warning(f"‚ùå Failed: {result['error']}")
                consecutive_errors += 1
                if consecutive_errors >= max_consecutive_errors:
                    break
                continue
            
            formatted_date = ''
            if result.get('date'):
                date_match = re.search(r'(\d{4})[.\-](\d{1,2})[.\-](\d{1,2})', result['date'])
                if date_match:
                    formatted_date = f"{date_match.group(1)[2:]}-{date_match.group(2).zfill(2)}-{date_match.group(3).zfill(2)}"
            
            title_clean = sanitize_filename(result.get('title', 'unknown'))
            
            menus.append({
                'menu': f"{menu}^{title_clean}" if menu else title_clean,
                'url': notice['href'],
                'murl': to_mglobalroaming_url(notice['href'])
            })

            if not result.get('murl'):
                result['murl'] = to_mglobalroaming_url(result.get('url', notice['href']))
            
            datas.append(result)
            total_processed += 1
            logger.info(f"‚úÖ Done: {total_processed} items")
            
        except Exception as e:
            logger.error(f"‚ùå Error: {str(e)}")
            consecutive_errors += 1
            if consecutive_errors >= max_consecutive_errors:
                break
            continue
    
    return {
        "menus": menus,
        "datas": datas,
        "total_processed": total_processed,
        "status": "completed",
        "message": f"Ï¥ù {total_processed}Í∞ú Î°úÎ∞ç Í≥µÏßÄÏÇ¨Ìï≠ Ï≤òÎ¶¨ ÏôÑÎ£å"
    }


# Ìï∏Îì§Îü¨ Îì±Î°ù
register_page_handler(
    r'https?://globalroaming\.kt\.com/news/list\.asp(?:\?.*)?$',
    handle_globalroaming_notice_main
)




