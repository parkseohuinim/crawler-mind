"""Crawler Application Service - orchestrates crawling operations"""
import asyncio
import uuid
from datetime import datetime
from typing import Dict, Optional, AsyncGenerator, Any, List
import logging
import json

from app.models import TaskStatus, CrawlingResult, TaskResult, ProcessingMode
from app.domains.crawler.entities.task import Task
from app.domains.crawler.entities.crawling_result import CrawlingResult as DomainCrawlingResult
from app.infrastructure.mcp.mcp_service import mcp_service
from app.infrastructure.llm.llm_service import llm_service

logger = logging.getLogger(__name__)

class CrawlerApplicationService:
    """Application service for managing crawling operations"""
    
    def __init__(self):
        self.tasks: Dict[str, TaskResult] = {}
        self.task_streams: Dict[str, asyncio.Queue] = {}
        
    def create_task(self, url: str, mode: ProcessingMode) -> str:
        """Create a new crawling task"""
        task_id = str(uuid.uuid4())
        
        task_result = TaskResult(
            taskId=task_id,
            status=TaskStatus.PENDING,
            createdAt=datetime.now().isoformat()
        )
        
        self.tasks[task_id] = task_result
        self.task_streams[task_id] = asyncio.Queue()
        
        logger.info(f"âœ… Task created: {task_id} for URL: {url} with mode: {mode}")
        logger.info(f"ğŸ“Š Total tasks in memory: {len(self.tasks)}")
        logger.info(f"ğŸ“Š Task streams in memory: {len(self.task_streams)}")
        
        # Start task processing in background
        asyncio.create_task(self._process_task(task_id, url, mode))
        
        return task_id
    
    def get_task(self, task_id: str) -> Optional[TaskResult]:
        """Get task by ID"""
        return self.tasks.get(task_id)
    
    async def get_task_stream(self, task_id: str) -> AsyncGenerator[str, None]:
        """Get SSE stream for task updates"""
        logger.info(f"ğŸ” SSE stream requested for task: {task_id}")
        logger.info(f"ğŸ“Š Available tasks: {list(self.tasks.keys())}")
        logger.info(f"ğŸ“Š Available streams: {list(self.task_streams.keys())}")
        
        if task_id not in self.task_streams:
            logger.error(f"âŒ Task stream not found: {task_id}")
            yield f"data: {json.dumps({'type': 'error', 'data': {'message': 'Task not found'}})}\n\n"
            return
            
        # ì´ˆê¸° ì—°ê²° í™•ì¸ ë©”ì‹œì§€ ì „ì†¡
        yield f"data: {json.dumps({'type': 'connected', 'data': {'message': 'Stream connected'}})}\n\n"
            
        queue = self.task_streams[task_id]
        
        try:
            while True:
                try:
                    # Wait for next update with timeout
                    message = await asyncio.wait_for(queue.get(), timeout=30.0)
                    yield f"data: {message}\n\n"
                    
                    # Parse message to check if it's a completion signal
                    try:
                        msg_data = json.loads(message)
                        if msg_data.get('type') in ['final', 'complete', 'error']:
                            # Send final message and break
                            break
                    except:
                        pass
                    
                    # Check if task is completed
                    task = self.tasks.get(task_id)
                    if task and task.status in [TaskStatus.COMPLETED, TaskStatus.FAILED]:
                        break
                        
                except asyncio.TimeoutError:
                    # Send heartbeat
                    yield f"data: {json.dumps({'type': 'heartbeat', 'data': {}})}\n\n"
                    
                    # Check if task is completed during timeout
                    task = self.tasks.get(task_id)
                    if task and task.status in [TaskStatus.COMPLETED, TaskStatus.FAILED]:
                        yield f"data: {json.dumps({'type': 'complete', 'data': {'message': 'ì‘ì—… ì™„ë£Œ'}})}\n\n"
                        break
                    
        except Exception as e:
            logger.error(f"Error in task stream {task_id}: {e}")
            try:
                yield f"data: {json.dumps({'type': 'error', 'data': {'message': str(e)}})}\n\n"
            except:
                pass
        finally:
            # Clean up stream
            logger.info(f"Cleaning up task stream: {task_id}")
            if task_id in self.task_streams:
                del self.task_streams[task_id]
    
    async def _process_task(self, task_id: str, url: str, mode: ProcessingMode):
        """Process crawling task"""
        try:
            # Update task status
            self.tasks[task_id].status = TaskStatus.RUNNING
            await self._send_update(task_id, 'status', {'message': 'ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...', 'status': 'active'})
            
            if mode == ProcessingMode.AUTO:
                # AI ìë™ ë¶„ì„ ëª¨ë“œ
                result = await self._process_auto_mode(task_id, url)
            else:
                # ê¸°ë³¸ í¬ë¡¤ë§ ëª¨ë“œ
                result = await self._process_basic_mode(task_id, url)
            
            # Update task with result
            self.tasks[task_id].result = result
            self.tasks[task_id].status = TaskStatus.COMPLETED
            self.tasks[task_id].completedAt = datetime.now().isoformat()
            
            await self._send_update(task_id, 'final', result.model_dump() if result else {})
            await self._send_update(task_id, 'complete', {'message': 'ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤'})
            
        except Exception as e:
            logger.error(f"Task {task_id} failed: {e}")
            
            # Update task with error
            self.tasks[task_id].status = TaskStatus.FAILED
            self.tasks[task_id].error = str(e)
            self.tasks[task_id].completedAt = datetime.now().isoformat()
            
            await self._send_update(task_id, 'error', {'message': str(e)})
    
    async def _process_auto_mode(self, task_id: str, url: str) -> CrawlingResult:
        """Process task using AI automatic mode - programmatic tools with AI analysis"""
        try:
            await self._send_update(task_id, 'status', {'message': 'AIê°€ ì›¹ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...', 'status': 'active'})
            
            # Step 1: í”„ë¡œê·¸ë˜ë°ì ìœ¼ë¡œ ëª¨ë“  ë„êµ¬ ì‹¤í–‰ (í™•ì‹¤í•œ ë°ì´í„° ìˆ˜ì§‘)
            tool_results = {}
            
            # 1. Crawl webpage
            await self._send_update(task_id, 'status', {'message': 'ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
            logger.warning(f"[AUTO MODE] Starting crawl_webpage for {url}")
            try:
                crawl_result = await mcp_service.call_tool("crawl_webpage", {"url": url})
                tool_results['crawl_webpage'] = crawl_result
                logger.warning(f"[AUTO MODE] crawl_webpage completed - result type: {type(crawl_result)}")
                await self._send_update(task_id, 'status', {'message': 'crawl_webpage ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ', 'status': 'active'})
            except Exception as e:
                logger.error(f"[AUTO MODE] Crawl webpage failed: {e}")
                return await self._process_basic_mode(task_id, url)
            
            # Get HTML content for subsequent tools
            html_content = None
            if hasattr(crawl_result, 'structured_content'):
                html_content = crawl_result.structured_content.get('html_content')
            elif hasattr(crawl_result, 'data'):
                html_content = crawl_result.data.get('html_content')
            else:
                html_content = crawl_result.get('html_content') if isinstance(crawl_result, dict) else None
            
            if not html_content:
                logger.warning("[AUTO MODE] No HTML content obtained, falling back to basic mode")
                return await self._process_basic_mode(task_id, url)
            
            logger.warning(f"[AUTO MODE] HTML content obtained: {len(html_content)} characters")
            
            # 2. Extract text content
            await self._send_update(task_id, 'status', {'message': 'í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
            logger.warning(f"[AUTO MODE] Starting extract_text_content")
            try:
                text_result = await mcp_service.call_tool("extract_text_content", {"html_content": html_content})
                tool_results['extract_text_content'] = text_result
                logger.warning(f"[AUTO MODE] extract_text_content completed")
                await self._send_update(task_id, 'status', {'message': 'extract_text_content ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ', 'status': 'active'})
            except Exception as e:
                logger.warning(f"[AUTO MODE] Extract text failed: {e}")
            
            # 3. Extract links
            await self._send_update(task_id, 'status', {'message': 'ë§í¬ë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
            logger.warning(f"[AUTO MODE] Starting extract_links")
            try:
                links_result = await mcp_service.call_tool("extract_links", {"html_content": html_content, "base_url": url})
                tool_results['extract_links'] = links_result
                logger.warning(f"[AUTO MODE] extract_links completed")
                await self._send_update(task_id, 'status', {'message': 'extract_links ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ', 'status': 'active'})
            except Exception as e:
                logger.warning(f"[AUTO MODE] Extract links failed: {e}")
            
            # 4. Take screenshot first (for AI analysis)
            await self._send_update(task_id, 'status', {'message': 'ìŠ¤í¬ë¦°ìƒ·ì„ ì´¬ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
            logger.warning(f"[AUTO MODE] Starting take_screenshot for {url}")
            try:
                screenshot_result = await mcp_service.call_tool("take_screenshot", {"url": url})
                tool_results['take_screenshot'] = screenshot_result
                logger.warning(f"[AUTO MODE] take_screenshot completed")
                await self._send_update(task_id, 'status', {'message': 'take_screenshot ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ', 'status': 'active'})
            except Exception as e:
                logger.warning(f"[AUTO MODE] Take screenshot failed: {e}")
            
            # Step 2: AI ë¶„ì„ ë° ìš”ì•½ ìƒì„±
            text_content = None
            if 'extract_text_content' in tool_results:
                text_result = tool_results['extract_text_content']
                if hasattr(text_result, 'structured_content'):
                    text_content = text_result.structured_content.get('text_content')
                elif hasattr(text_result, 'data'):
                    text_content = text_result.data.get('text_content')
                else:
                    text_content = text_result.get('text_content') if isinstance(text_result, dict) else None
            
            if text_content:
                await self._send_update(task_id, 'status', {'message': 'AIê°€ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
                logger.warning(f"[AUTO MODE] Starting AI analysis with {len(text_content)} characters")
                
                # AIì—ê²Œ ì›¹í˜ì´ì§€ ë¶„ì„ ë° ìš”ì•½ ìš”ì²­
                analysis_result = await self._analyze_with_ai(url, text_content, tool_results)
                if analysis_result:
                    tool_results['ai_analysis'] = analysis_result
                    logger.warning(f"[AUTO MODE] AI analysis completed")
                    await self._send_update(task_id, 'status', {'message': 'AI ë¶„ì„ ì™„ë£Œ', 'status': 'active'})
                else:
                    # AI ë¶„ì„ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ìš”ì•½ ì‚¬ìš©
                    logger.warning(f"[AUTO MODE] AI analysis failed, using basic summarization")
                    try:
                        summary_result = await mcp_service.call_tool("summarize_content", {"text_content": text_content})
                        tool_results['summarize_content'] = summary_result
                    except Exception as e:
                        logger.warning(f"[AUTO MODE] Basic summarization failed: {e}")
            else:
                logger.warning(f"[AUTO MODE] No text content available for AI analysis")
            
            # Step 3: ê²°ê³¼ êµ¬ì„±
            logger.warning(f"[AUTO MODE] Building result from {len(tool_results)} tools: {list(tool_results.keys())}")
            result = self._build_result_from_tools(tool_results, url)
            logger.warning(f"[AUTO MODE] Final result - textLength: {result.textLength}, linkCount: {result.linkCount}")
            await self._send_update(task_id, 'status', {'message': 'AI ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!', 'status': 'completed'})
            return result
            
        except Exception as e:
            logger.error(f"Auto mode processing failed: {e}")
            # Fallback to basic mode
            return await self._process_basic_mode(task_id, url)
    
    async def _process_basic_mode(self, task_id: str, url: str) -> CrawlingResult:
        """Process task using basic crawling mode"""
        try:
            result_data = {}
            
            # 1. ì›¹í˜ì´ì§€ í¬ë¡¤ë§
            await self._send_update(task_id, 'status', {'message': 'ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
            
            if not mcp_service.is_connected:
                raise Exception("MCP ì„œë²„ì— ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            crawl_result = await mcp_service.call_tool("crawl_webpage", {"url": url})
            crawl_data = crawl_result.structured_content if hasattr(crawl_result, 'structured_content') else crawl_result.data
            
            if not crawl_data.get("success"):
                raise Exception(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {crawl_data.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            
            result_data["title"] = crawl_data.get("title")
            html_content = crawl_data.get("html_content", "")
            
            # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            await self._send_update(task_id, 'status', {'message': 'í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
            
            text_result = await mcp_service.call_tool("extract_text_content", {"html_content": html_content})
            text_data = text_result.structured_content if hasattr(text_result, 'structured_content') else text_result.data
            
            if text_data.get("success"):
                result_data["textLength"] = text_data.get("text_length", 0)
                text_content = text_data.get("text_content", "")
            else:
                text_content = ""
            
            # 3. ë§í¬ ì¶”ì¶œ
            await self._send_update(task_id, 'status', {'message': 'ë§í¬ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
            
            links_result = await mcp_service.call_tool("extract_links", {"html_content": html_content, "base_url": url})
            links_data_result = links_result.structured_content if hasattr(links_result, 'structured_content') else links_result.data
            
            if links_data_result.get("success"):
                links_data = links_data_result.get("links", [])
                result_data["linkCount"] = len(links_data)
                result_data["links"] = [link["url"] for link in links_data[:20]]
            
            # 4. ìš”ì•½ ìƒì„±
            if text_content:
                await self._send_update(task_id, 'status', {'message': 'ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
                
                summary_result = await mcp_service.call_tool("summarize_content", {"text_content": text_content})
                summary_data = summary_result.structured_content if hasattr(summary_result, 'structured_content') else summary_result.data
                
                if summary_data.get("success"):
                    result_data["summary"] = summary_data.get("summary")
            
            # 5. ìŠ¤í¬ë¦°ìƒ·
            try:
                await self._send_update(task_id, 'status', {'message': 'ìŠ¤í¬ë¦°ìƒ·ì„ ì´¬ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
                
                screenshot_result = await mcp_service.call_tool("take_screenshot", {"url": url})
                screenshot_data = screenshot_result.structured_content if hasattr(screenshot_result, 'structured_content') else screenshot_result.data
                
                if screenshot_data.get("success"):
                    result_data["screenshot"] = screenshot_data.get("screenshot")
                    
            except Exception as e:
                logger.warning(f"Screenshot failed: {e}")
            
            await self._send_update(task_id, 'status', {'message': 'ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!', 'status': 'completed'})
            
            return CrawlingResult(**result_data)
            
        except Exception as e:
            logger.error(f"Basic mode processing failed: {e}")
            return CrawlingResult(error=str(e))
    
    def _build_result_from_tools(self, tool_results: Dict[str, Any], url: str) -> CrawlingResult:
        """Build CrawlingResult from tool execution results"""
        result_data = {}
        
        logger.warning(f"[BUILD] Building result from {len(tool_results)} tool results: {list(tool_results.keys())}")
        
        # Process each tool result
        for tool_name, result in tool_results.items():
            logger.warning(f"[BUILD] Processing tool: {tool_name}")
            
            if hasattr(result, 'structured_content'):
                data = result.structured_content
                logger.warning(f"[BUILD] Using structured_content for {tool_name}")
            elif hasattr(result, 'data'):
                data = result.data
                logger.warning(f"[BUILD] Using data for {tool_name}")
            else:
                data = result
                logger.warning(f"[BUILD] Using direct result for {tool_name}")
            
            logger.warning(f"[BUILD] Data type for {tool_name}: {type(data)}, success: {data.get('success') if isinstance(data, dict) else 'not dict'}")
            
            if not isinstance(data, dict) or not data.get('success'):
                logger.warning(f"[BUILD] Skipping {tool_name} - not dict or not successful")
                continue
            
            if tool_name == 'crawl_webpage':
                title = data.get('title')
                result_data['title'] = title
                logger.warning(f"[BUILD] crawl_webpage - title: {title}")
            elif tool_name == 'extract_text_content':
                text_length = data.get('text_length', 0)
                result_data['textLength'] = text_length
                logger.warning(f"[BUILD] extract_text_content - textLength: {text_length}")
            elif tool_name == 'extract_links':
                links = data.get('links', [])
                result_data['linkCount'] = len(links)
                result_data['links'] = [link['url'] for link in links[:20] if isinstance(link, dict) and 'url' in link]
                logger.warning(f"[BUILD] extract_links - linkCount: {len(links)}")
            elif tool_name == 'summarize_content':
                summary = data.get('summary')
                result_data['summary'] = summary
                logger.warning(f"[BUILD] summarize_content - summary length: {len(summary) if summary else 0}")
            elif tool_name == 'ai_analysis':
                # AI ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬
                analysis = data.get('analysis', '')
                summary = data.get('summary', '')
                result_data['summary'] = summary or analysis
                result_data['ai_analysis'] = analysis
                logger.warning(f"[BUILD] ai_analysis - analysis length: {len(analysis) if analysis else 0}")
            elif tool_name == 'take_screenshot':
                screenshot = data.get('screenshot')
                result_data['screenshot'] = screenshot
                logger.warning(f"[BUILD] take_screenshot - screenshot size: {len(screenshot) if screenshot else 0} chars")
        
        # Set defaults for missing fields
        result_data.setdefault('textLength', 0)
        result_data.setdefault('linkCount', 0)
        result_data.setdefault('links', [])
        
        logger.warning(f"[BUILD] Final result_data: textLength={result_data.get('textLength')}, linkCount={result_data.get('linkCount')}, title='{result_data.get('title', 'N/A')}'")
        
        return CrawlingResult(**result_data)
    
    async def _analyze_with_ai(self, url: str, text_content: str, tool_results: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Use AI to analyze the webpage content and generate insights"""
        try:
            # ë§í¬ ì •ë³´ ì¤€ë¹„
            links_info = ""
            if 'extract_links' in tool_results:
                links_result = tool_results['extract_links']
                if hasattr(links_result, 'structured_content'):
                    links_data = links_result.structured_content.get('links', [])
                elif hasattr(links_result, 'data'):
                    links_data = links_result.data.get('links', [])
                else:
                    links_data = links_result.get('links', []) if isinstance(links_result, dict) else []
                
                if links_data:
                    links_info = f"ì£¼ìš” ë§í¬ë“¤: {', '.join([link.get('text', link.get('url', ''))[:50] for link in links_data[:10]])}"
            
            # í˜ì´ì§€ ì •ë³´ ì¤€ë¹„
            page_info = ""
            if 'crawl_webpage' in tool_results:
                crawl_result = tool_results['crawl_webpage']
                if hasattr(crawl_result, 'structured_content'):
                    title = crawl_result.structured_content.get('title', '')
                elif hasattr(crawl_result, 'data'):
                    title = crawl_result.data.get('title', '')
                else:
                    title = crawl_result.get('title', '') if isinstance(crawl_result, dict) else ''
                
                if title:
                    page_info = f"í˜ì´ì§€ ì œëª©: {title}"
            
            # AI ë¶„ì„ ìš”ì²­
            analysis_prompt = f"""ë‹¤ìŒ ì›¹í˜ì´ì§€ë¥¼ ë¶„ì„í•˜ê³  í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

URL: {url}
{page_info}
{links_info}

í˜ì´ì§€ ë‚´ìš©:
{text_content[:3000]}{'...' if len(text_content) > 3000 else ''}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
1. í˜ì´ì§€ì˜ ì£¼ìš” ëª©ì ê³¼ ë‚´ìš© (100ì ì´ë‚´)
2. í•µì‹¬ í‚¤ì›Œë“œ 3-5ê°œ
3. ì‚¬ìš©ìì—ê²Œ ìœ ìš©í•œ ì •ë³´ë‚˜ íŠ¹ì§•

ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."""
            
            # LLM í˜¸ì¶œ (ë„êµ¬ ì—†ì´ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ë¶„ì„)
            response = await llm_service.query(analysis_prompt, [])
            
            if response and len(response.strip()) > 10:
                return {
                    'success': True,
                    'analysis': response.strip(),
                    'analysis_type': 'ai_generated',
                    'summary': response.strip()[:500]  # ì²« 500ìë¥¼ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
                }
            else:
                return None
                
        except Exception as e:
            logger.warning(f"AI analysis failed: {e}")
            return None
    
    def _create_llm_question(self, url: str, missing_tools: List[str], attempt: int) -> str:
        """Create LLM question with strong emphasis on required tools"""
        if attempt == 0:
            # ì²« ë²ˆì§¸ ì‹œë„: ì •ì¤‘í•œ ìš”ì²­
            return f"""Please analyze the website at {url}. You MUST use ALL of these tools in order:
{', '.join(missing_tools)}

IMPORTANT: Do not skip any tools. Each tool provides essential data for complete analysis."""
        
        elif attempt == 1:
            # ë‘ ë²ˆì§¸ ì‹œë„: ë” ê°•í•œ ìš”êµ¬
            return f"""CRITICAL: You failed to use all required tools. You MUST use these remaining tools:
{', '.join(missing_tools)}

URL: {url}

This is mandatory. Do not provide text responses - only use the specified tools."""
        
        else:
            # ì„¸ ë²ˆì§¸ ì‹œë„: ìµœí›„ ê²½ê³ 
            return f"""FINAL ATTEMPT: Use these tools immediately or the task will fail:
{', '.join(missing_tools)}

URL: {url}

NO TEXT RESPONSES. TOOLS ONLY."""
    
    async def _fill_missing_tools(self, task_id: str, url: str, missing_tools: List[str], tool_results: Dict[str, Any]):
        """Fill missing tools programmatically"""
        # HTML content ë¨¼ì € í™•ë³´
        html_content = None
        if 'crawl_webpage' in tool_results:
            crawl_result = tool_results['crawl_webpage']
            if hasattr(crawl_result, 'structured_content'):
                html_content = crawl_result.structured_content.get('html_content')
            elif hasattr(crawl_result, 'data'):
                html_content = crawl_result.data.get('html_content')
            else:
                html_content = crawl_result.get('html_content') if isinstance(crawl_result, dict) else None
        
        # ëˆ„ë½ëœ ë„êµ¬ë“¤ ì‹¤í–‰
        for tool_name in missing_tools:
            try:
                await self._send_update(task_id, 'status', {
                    'message': f'{tool_name} ë„êµ¬ ì‹¤í–‰ ì¤‘ (ë³´ì™„)',
                    'status': 'active'
                })
                
                if tool_name == 'crawl_webpage':
                    result = await mcp_service.call_tool("crawl_webpage", {"url": url})
                elif tool_name == 'extract_text_content' and html_content:
                    result = await mcp_service.call_tool("extract_text_content", {"html_content": html_content})
                elif tool_name == 'extract_links' and html_content:
                    result = await mcp_service.call_tool("extract_links", {"html_content": html_content, "base_url": url})
                elif tool_name == 'summarize_content':
                    # í…ìŠ¤íŠ¸ ì½˜í…ì¸  ë¨¼ì € í™•ë³´
                    text_content = None
                    if 'extract_text_content' in tool_results:
                        text_result = tool_results['extract_text_content']
                        if hasattr(text_result, 'structured_content'):
                            text_content = text_result.structured_content.get('text_content')
                        elif hasattr(text_result, 'data'):
                            text_content = text_result.data.get('text_content')
                    if text_content:
                        result = await mcp_service.call_tool("summarize_content", {"text_content": text_content})
                    else:
                        continue
                elif tool_name == 'take_screenshot':
                    result = await mcp_service.call_tool("take_screenshot", {"url": url})
                else:
                    continue
                
                tool_results[tool_name] = result
                logger.warning(f"ğŸ”§ [AUTO MODE] Filled missing tool: {tool_name}")
                
            except Exception as e:
                logger.warning(f"âŒ [AUTO MODE] Failed to fill {tool_name}: {e}")
    
    async def _send_update(self, task_id: str, event_type: str, data: dict):
        """Send update to task stream"""
        if task_id in self.task_streams:
            message = json.dumps({
                'type': event_type,
                'data': data,
                'timestamp': datetime.now().isoformat()
            })
            
            try:
                await self.task_streams[task_id].put(message)
            except Exception as e:
                logger.error(f"Failed to send update for task {task_id}: {e}")

# Global service instance
crawler_service = CrawlerApplicationService()
