"""Task management service for handling async crawling operations"""
import asyncio
import uuid
from datetime import datetime
from typing import Dict, Optional, AsyncGenerator, Any, List
import logging
import json

from app.models import TaskStatus, CrawlingResult, TaskResult, ProcessingMode
from app.services.mcp_service import mcp_service
from app.services.llm_service import llm_service

logger = logging.getLogger(__name__)

class TaskService:
    """Service for managing crawling tasks"""
    
    def __init__(self):
        self.tasks: Dict[str, TaskResult] = {}
        self.task_streams: Dict[str, asyncio.Queue] = {}
        
    def create_task(self, url: str, mode: ProcessingMode) -> str:
        """Create a new crawling task"""
        task_id = str(uuid.uuid4())
        
        task_result = TaskResult(
            taskId=task_id,
            status=TaskStatus.PENDING,
            createdAt=datetime.now().isoformat()
        )
        
        self.tasks[task_id] = task_result
        self.task_streams[task_id] = asyncio.Queue()
        
        logger.info(f"âœ… Task created: {task_id} for URL: {url} with mode: {mode}")
        logger.info(f"ğŸ“Š Total tasks in memory: {len(self.tasks)}")
        logger.info(f"ğŸ“Š Task streams in memory: {len(self.task_streams)}")
        
        # Start task processing in background
        asyncio.create_task(self._process_task(task_id, url, mode))
        
        return task_id
    
    def get_task(self, task_id: str) -> Optional[TaskResult]:
        """Get task by ID"""
        return self.tasks.get(task_id)
    
    async def get_task_stream(self, task_id: str) -> AsyncGenerator[str, None]:
        """Get SSE stream for task updates"""
        logger.info(f"ğŸ” SSE stream requested for task: {task_id}")
        logger.info(f"ğŸ“Š Available tasks: {list(self.tasks.keys())}")
        logger.info(f"ğŸ“Š Available streams: {list(self.task_streams.keys())}")
        
        if task_id not in self.task_streams:
            logger.error(f"âŒ Task stream not found: {task_id}")
            yield f"data: {json.dumps({'type': 'error', 'data': {'message': 'Task not found'}})}\n\n"
            return
            
        # ì´ˆê¸° ì—°ê²° í™•ì¸ ë©”ì‹œì§€ ì „ì†¡
        yield f"data: {json.dumps({'type': 'connected', 'data': {'message': 'Stream connected'}})}\n\n"
            
        queue = self.task_streams[task_id]
        
        try:
            while True:
                try:
                    # Wait for next update with timeout
                    message = await asyncio.wait_for(queue.get(), timeout=30.0)
                    yield f"data: {message}\n\n"
                    
                    # Parse message to check if it's a completion signal
                    try:
                        msg_data = json.loads(message)
                        if msg_data.get('type') in ['final', 'complete', 'error']:
                            # Send final message and break
                            break
                    except:
                        pass
                    
                    # Check if task is completed
                    task = self.tasks.get(task_id)
                    if task and task.status in [TaskStatus.COMPLETED, TaskStatus.FAILED]:
                        break
                        
                except asyncio.TimeoutError:
                    # Send heartbeat
                    yield f"data: {json.dumps({'type': 'heartbeat', 'data': {}})}\n\n"
                    
                    # Check if task is completed during timeout
                    task = self.tasks.get(task_id)
                    if task and task.status in [TaskStatus.COMPLETED, TaskStatus.FAILED]:
                        yield f"data: {json.dumps({'type': 'complete', 'data': {'message': 'ì‘ì—… ì™„ë£Œ'}})}\n\n"
                        break
                    
        except Exception as e:
            logger.error(f"Error in task stream {task_id}: {e}")
            try:
                yield f"data: {json.dumps({'type': 'error', 'data': {'message': str(e)}})}\n\n"
            except:
                pass
        finally:
            # Clean up stream
            logger.info(f"Cleaning up task stream: {task_id}")
            if task_id in self.task_streams:
                del self.task_streams[task_id]
    
    async def _process_task(self, task_id: str, url: str, mode: ProcessingMode):
        """Process crawling task"""
        try:
            # Update task status
            self.tasks[task_id].status = TaskStatus.RUNNING
            await self._send_update(task_id, 'status', {'message': 'ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...', 'status': 'active'})
            
            if mode == ProcessingMode.AUTO:
                # AI ìë™ ë¶„ì„ ëª¨ë“œ (ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í”Œë¡œìš° í´ë°±)
                result = await self._process_auto_mode(task_id, url)
            else:
                # ê¸°ë³¸ í¬ë¡¤ë§ ëª¨ë“œ
                result = await self._process_basic_mode(task_id, url)
            
            # Update task with result
            self.tasks[task_id].result = result
            self.tasks[task_id].status = TaskStatus.COMPLETED
            self.tasks[task_id].completedAt = datetime.now().isoformat()
            
            await self._send_update(task_id, 'final', result.model_dump() if result else {})
            
            # ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ ì‹ í˜¸ ì „ì†¡
            await self._send_update(task_id, 'complete', {'message': 'ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤'})
            
        except Exception as e:
            logger.error(f"Task {task_id} failed: {e}")
            
            # Update task with error
            self.tasks[task_id].status = TaskStatus.FAILED
            self.tasks[task_id].error = str(e)
            self.tasks[task_id].completedAt = datetime.now().isoformat()
            
            await self._send_update(task_id, 'error', {'message': str(e)})
    
    async def _process_auto_mode(self, task_id: str, url: str) -> CrawlingResult:
        """Process task using AI automatic mode"""
        try:
            # LLMì—ê²Œ URL ë¶„ì„ ìš”ì²­ (ì—„ê²© ëª¨ë“œ: ê° ë„êµ¬ë¥¼ LLMì´ ì§ì ‘ í˜¸ì¶œí•˜ë„ë¡ ìˆœì°¨ ê°•ì œ)
            await self._send_update(task_id, 'status', {'message': 'AIê°€ ë„êµ¬ í˜¸ì¶œì„ ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤...', 'status': 'active'})

            all_tools: List[Dict[str, Any]] = mcp_service.available_tools

            def filter_tools(name: str) -> List[Dict[str, Any]]:
                # available_tools í•­ëª©ì€ {'type':'function', 'name':..., 'parameters':...}
                return [t for t in all_tools if t.get('name') == name]

            tool_results: Dict[str, Any] = {}
            html_content: Optional[str] = None
            text_content: Optional[str] = None

            # 1) crawl_webpage (URLë§Œ í•„ìš”)
            await self._send_update(task_id, 'status', {'message': 'ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤ (LLM í˜¸ì¶œ)...', 'status': 'active'})
            cw_prompt = f"Call crawl_webpage for URL: {url}. Return a function call only."
            planned_calls: List[Dict[str, Any]] = []
            async for ev in llm_service.query_stream(cw_prompt, filter_tools('crawl_webpage')):
                if ev.get('type') == 'tool_calls_ready':
                    planned_calls = ev['data'].get('tool_calls', [])
                    break
            # ì‹¤í–‰ (í•„ìš” ì¸ì ë³´ê°•)
            crawl_args = {'url': url}
            crawl_res = await mcp_service.call_tool('crawl_webpage', crawl_args)
            tool_results['crawl_webpage'] = crawl_res
            # HTML ë³´ê´€
            crawl_data = getattr(crawl_res, 'structured_content', None) or getattr(crawl_res, 'data', None) or crawl_res
            if isinstance(crawl_data, dict) and crawl_data.get('success'):
                html_content = crawl_data.get('html_content')

            # 2) extract_text_content (HTMLì€ ë‚´ë¶€ì—ì„œ ë³´ê°•)
            await self._send_update(task_id, 'status', {'message': 'í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (LLM í˜¸ì¶œ)...', 'status': 'active'})
            etc_prompt = "Call extract_text_content. Do not include the full HTML in arguments; use an empty placeholder."
            planned_calls = []
            async for ev in llm_service.query_stream(etc_prompt, filter_tools('extract_text_content')):
                if ev.get('type') == 'tool_calls_ready':
                    planned_calls = ev['data'].get('tool_calls', [])
                    break
            text_args = {'html_content': html_content or ''}
            text_res = await mcp_service.call_tool('extract_text_content', text_args)
            tool_results['extract_text_content'] = text_res
            text_data = getattr(text_res, 'structured_content', None) or getattr(text_res, 'data', None) or text_res
            if isinstance(text_data, dict) and text_data.get('success'):
                text_content = text_data.get('text_content')

            # 3) extract_links (HTMLì€ ë‚´ë¶€ì—ì„œ ë³´ê°•)
            await self._send_update(task_id, 'status', {'message': 'ë§í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤ (LLM í˜¸ì¶œ)...', 'status': 'active'})
            el_prompt = "Call extract_links. Do not include the full HTML in arguments; use an empty placeholder."
            planned_calls = []
            async for ev in llm_service.query_stream(el_prompt, filter_tools('extract_links')):
                if ev.get('type') == 'tool_calls_ready':
                    planned_calls = ev['data'].get('tool_calls', [])
                    break
            links_args = {'html_content': html_content or '', 'base_url': url}
            links_res = await mcp_service.call_tool('extract_links', links_args)
            tool_results['extract_links'] = links_res

            # 4) take_screenshot (URLë§Œ í•„ìš”)
            await self._send_update(task_id, 'status', {'message': 'ìŠ¤í¬ë¦°ìƒ·ì„ ì´¬ì˜í•©ë‹ˆë‹¤ (LLM í˜¸ì¶œ)...', 'status': 'active'})
            ss_prompt = f"Call take_screenshot for URL: {url}. Return a function call only."
            planned_calls = []
            async for ev in llm_service.query_stream(ss_prompt, filter_tools('take_screenshot')):
                if ev.get('type') == 'tool_calls_ready':
                    planned_calls = ev['data'].get('tool_calls', [])
                    break
            ss_args = {'url': url}
            ss_res = await mcp_service.call_tool('take_screenshot', ss_args)
            tool_results['take_screenshot'] = ss_res

            # 5) summarize_content (í…ìŠ¤íŠ¸ëŠ” ë‚´ë¶€ì—ì„œ ë³´ê°•)
            await self._send_update(task_id, 'status', {'message': 'ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤ (LLM í˜¸ì¶œ)...', 'status': 'active'})
            sum_prompt = "Call summarize_content. Do not include the full text in arguments; use an empty placeholder."
            planned_calls = []
            async for ev in llm_service.query_stream(sum_prompt, filter_tools('summarize_content')):
                if ev.get('type') == 'tool_calls_ready':
                    planned_calls = ev['data'].get('tool_calls', [])
                    break
            sum_args = {'text_content': text_content or ''}
            sum_res = await mcp_service.call_tool('summarize_content', sum_args)
            tool_results['summarize_content'] = sum_res

            await self._send_update(task_id, 'status', {'message': 'AI ë„êµ¬ í˜¸ì¶œ ê²°ê³¼ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤...', 'status': 'active'})
            return self._build_result_from_llm_tools(tool_results, url)
            
        except Exception as e:
            logger.error(f"Auto mode processing failed: {e}")
            return CrawlingResult(error=str(e))
    
    async def _enhance_llm_results(self, task_id: str, url: str, tool_results: Dict[str, Any]) -> Dict[str, Any]:
        """LLMì´ ì„ íƒí•œ ë„êµ¬ ê²°ê³¼ì— ëˆ„ë½ëœ í•„ìˆ˜ ë„êµ¬ë“¤ì„ ë³´ì™„"""
        enhanced_results = tool_results.copy()
        
        # crawl_webpageê°€ ìˆì–´ì•¼ ë‹¤ë¥¸ ë„êµ¬ë“¤ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŒ
        if 'crawl_webpage' not in enhanced_results:
            logger.warning(f"ğŸ”§ Missing crawl_webpage, executing...")
            await self._send_update(task_id, 'status', {'message': 'ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
            crawl_result = await mcp_service.call_tool('crawl_webpage', {'url': url})
            enhanced_results['crawl_webpage'] = crawl_result
        
        # HTML ë°ì´í„° ì¶”ì¶œ
        html_content = None
        crawl_result = enhanced_results.get('crawl_webpage')
        if crawl_result:
            if hasattr(crawl_result, 'structured_content'):
                crawl_data = crawl_result.structured_content
            elif hasattr(crawl_result, 'data'):
                crawl_data = crawl_result.data
            else:
                crawl_data = crawl_result
            
            if isinstance(crawl_data, dict) and crawl_data.get('success'):
                html_content = crawl_data.get('html')
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ëˆ„ë½ë˜ì—ˆë‹¤ë©´ ì‹¤í–‰
        if 'extract_text_content' not in enhanced_results and html_content:
            logger.warning(f"ğŸ”§ Missing extract_text_content, executing...")
            await self._send_update(task_id, 'status', {'message': 'í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
            text_result = await mcp_service.call_tool('extract_text_content', {'html': html_content})
            enhanced_results['extract_text_content'] = text_result
        
        # ë§í¬ ì¶”ì¶œì´ ëˆ„ë½ë˜ì—ˆë‹¤ë©´ ì‹¤í–‰
        if 'extract_links' not in enhanced_results and html_content:
            logger.warning(f"ğŸ”§ Missing extract_links, executing...")
            await self._send_update(task_id, 'status', {'message': 'ë§í¬ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
            links_result = await mcp_service.call_tool('extract_links', {'html': html_content})
            enhanced_results['extract_links'] = links_result
        
        # ìš”ì•½ì´ ëˆ„ë½ë˜ì—ˆë‹¤ë©´ ì‹¤í–‰
        text_content = None
        if 'extract_text_content' in enhanced_results:
            text_result = enhanced_results['extract_text_content']
            if hasattr(text_result, 'structured_content'):
                text_data = text_result.structured_content
            elif hasattr(text_result, 'data'):
                text_data = text_result.data
            else:
                text_data = text_result
            
            if isinstance(text_data, dict) and text_data.get('success'):
                text_content = text_data.get('text')
        
        if 'summarize_content' not in enhanced_results and text_content:
            logger.warning(f"ğŸ”§ Missing summarize_content, executing...")
            await self._send_update(task_id, 'status', {'message': 'ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
            summary_result = await mcp_service.call_tool('summarize_content', {'text': text_content})
            enhanced_results['summarize_content'] = summary_result
        
        # ìŠ¤í¬ë¦°ìƒ·ì´ ëˆ„ë½ë˜ì—ˆë‹¤ë©´ ì‹¤í–‰
        if 'take_screenshot' not in enhanced_results:
            logger.warning(f"ğŸ”§ Missing take_screenshot, executing...")
            await self._send_update(task_id, 'status', {'message': 'ìŠ¤í¬ë¦°ìƒ·ì„ ì´¬ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
            screenshot_result = await mcp_service.call_tool('take_screenshot', {'url': url})
            enhanced_results['take_screenshot'] = screenshot_result
        
        logger.warning(f"ğŸ”§ Enhanced results: {list(enhanced_results.keys())}")
        return enhanced_results
    
    def _build_result_from_llm_tools(self, tool_results: Dict[str, Any], url: str) -> CrawlingResult:
        """LLMì´ ì‹¤í–‰í•œ ë„êµ¬ ê²°ê³¼ë“¤ì„ ì¡°í•©í•˜ì—¬ CrawlingResult ìƒì„±"""
        result_data = {}
        
        # crawl_webpage ê²°ê³¼ ì²˜ë¦¬
        if 'crawl_webpage' in tool_results:
            crawl_result = tool_results['crawl_webpage']
            if hasattr(crawl_result, 'structured_content'):
                crawl_data = crawl_result.structured_content
            elif hasattr(crawl_result, 'data'):
                crawl_data = crawl_result.data
            else:
                crawl_data = crawl_result
            
            if isinstance(crawl_data, dict) and crawl_data.get('success'):
                result_data['title'] = crawl_data.get('title')
        
        # extract_text_content ê²°ê³¼ ì²˜ë¦¬
        if 'extract_text_content' in tool_results:
            text_result = tool_results['extract_text_content']
            if hasattr(text_result, 'structured_content'):
                text_data = text_result.structured_content
            elif hasattr(text_result, 'data'):
                text_data = text_result.data
            else:
                text_data = text_result
                
            if isinstance(text_data, dict) and text_data.get('success'):
                result_data['textLength'] = text_data.get('text_length', 0)
        
        # extract_links ê²°ê³¼ ì²˜ë¦¬
        if 'extract_links' in tool_results:
            links_result = tool_results['extract_links']
            if hasattr(links_result, 'structured_content'):
                links_data = links_result.structured_content
            elif hasattr(links_result, 'data'):
                links_data = links_result.data
            else:
                links_data = links_result
                
            if isinstance(links_data, dict) and links_data.get('success'):
                links_list = links_data.get('links', [])
                result_data['linkCount'] = len(links_list)
                result_data['links'] = [link['url'] for link in links_list[:20] if isinstance(link, dict) and 'url' in link]
        
        # summarize_content ê²°ê³¼ ì²˜ë¦¬
        if 'summarize_content' in tool_results:
            summary_result = tool_results['summarize_content']
            if hasattr(summary_result, 'structured_content'):
                summary_data = summary_result.structured_content
            elif hasattr(summary_result, 'data'):
                summary_data = summary_result.data
            else:
                summary_data = summary_result
                
            if isinstance(summary_data, dict) and summary_data.get('success'):
                result_data['summary'] = summary_data.get('summary')
        
        # take_screenshot ê²°ê³¼ ì²˜ë¦¬
        if 'take_screenshot' in tool_results:
            screenshot_result = tool_results['take_screenshot']
            if hasattr(screenshot_result, 'structured_content'):
                screenshot_data = screenshot_result.structured_content
            elif hasattr(screenshot_result, 'data'):
                screenshot_data = screenshot_result.data
            else:
                screenshot_data = screenshot_result
                
            if isinstance(screenshot_data, dict) and screenshot_data.get('success'):
                result_data['screenshot'] = screenshot_data.get('screenshot')
        
        # ëˆ„ë½ëœ í•„ë“œì— ê¸°ë³¸ê°’ ì„¤ì • (LLMì´ ì¼ë¶€ ë„êµ¬ë§Œ ì„ íƒí•œ ê²½ìš°)
        if 'textLength' not in result_data:
            result_data['textLength'] = 0
        if 'linkCount' not in result_data:
            result_data['linkCount'] = 0
        if 'links' not in result_data:
            result_data['links'] = []
        
        return CrawlingResult(**result_data)
    
    async def _process_basic_mode(self, task_id: str, url: str) -> CrawlingResult:
        """Process task using basic crawling mode"""
        try:
            result_data = {}
            
            # 1. ì›¹í˜ì´ì§€ í¬ë¡¤ë§
            await self._send_update(task_id, 'status', {'message': 'ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
            
            if not mcp_service.is_connected:
                raise Exception("MCP ì„œë²„ì— ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # crawl_webpage ë„êµ¬ í˜¸ì¶œ
            crawl_result = await mcp_service.call_tool("crawl_webpage", {"url": url})
            
            # MCP ë„êµ¬ ê²°ê³¼ëŠ” CallToolResult ê°ì²´ì´ë¯€ë¡œ .data ë˜ëŠ” .structured_content ì‚¬ìš©
            crawl_data = crawl_result.structured_content if hasattr(crawl_result, 'structured_content') else crawl_result.data
            
            if not crawl_data.get("success"):
                raise Exception(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {crawl_data.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            
            result_data["title"] = crawl_data.get("title")
            html_content = crawl_data.get("html_content", "")
            
            # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            await self._send_update(task_id, 'status', {'message': 'í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
            
            text_result = await mcp_service.call_tool("extract_text_content", {"html_content": html_content})
            text_data = text_result.structured_content if hasattr(text_result, 'structured_content') else text_result.data
            
            if text_data.get("success"):
                result_data["textLength"] = text_data.get("text_length", 0)
                text_content = text_data.get("text_content", "")
            else:
                text_content = ""
            
            # 3. ë§í¬ ì¶”ì¶œ
            await self._send_update(task_id, 'status', {'message': 'ë§í¬ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
            
            links_result = await mcp_service.call_tool("extract_links", {"html_content": html_content, "base_url": url})
            links_data_result = links_result.structured_content if hasattr(links_result, 'structured_content') else links_result.data
            
            if links_data_result.get("success"):
                links_data = links_data_result.get("links", [])
                result_data["linkCount"] = len(links_data)
                result_data["links"] = [link["url"] for link in links_data[:20]]  # ìµœëŒ€ 20ê°œë§Œ
            
            # 4. ìš”ì•½ ìƒì„±
            if text_content:
                await self._send_update(task_id, 'status', {'message': 'ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
                
                summary_result = await mcp_service.call_tool("summarize_content", {"text_content": text_content})
                summary_data = summary_result.structured_content if hasattr(summary_result, 'structured_content') else summary_result.data
                
                if summary_data.get("success"):
                    result_data["summary"] = summary_data.get("summary")
            
            # 5. ìŠ¤í¬ë¦°ìƒ· (ì„ íƒì )
            try:
                await self._send_update(task_id, 'status', {'message': 'ìŠ¤í¬ë¦°ìƒ·ì„ ì´¬ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤...', 'status': 'active'})
                
                screenshot_result = await mcp_service.call_tool("take_screenshot", {"url": url})
                screenshot_data = screenshot_result.structured_content if hasattr(screenshot_result, 'structured_content') else screenshot_result.data
                
                if screenshot_data.get("success"):
                    result_data["screenshot"] = screenshot_data.get("screenshot")
                    
            except Exception as e:
                logger.warning(f"Screenshot failed: {e}")
                # ìŠ¤í¬ë¦°ìƒ· ì‹¤íŒ¨ëŠ” ì „ì²´ ì‘ì—… ì‹¤íŒ¨ë¡œ ê°„ì£¼í•˜ì§€ ì•ŠìŒ
            
            await self._send_update(task_id, 'status', {'message': 'ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!', 'status': 'completed'})
            
            return CrawlingResult(**result_data)
            
        except Exception as e:
            logger.error(f"Basic mode processing failed: {e}")
            return CrawlingResult(error=str(e))
    
    async def _send_update(self, task_id: str, event_type: str, data: dict):
        """Send update to task stream"""
        if task_id in self.task_streams:
            message = json.dumps({
                'type': event_type,
                'data': data,
                'timestamp': datetime.now().isoformat()
            })
            
            try:
                await self.task_streams[task_id].put(message)
            except Exception as e:
                logger.error(f"Failed to send update for task {task_id}: {e}")

# Global task service instance
task_service = TaskService()
