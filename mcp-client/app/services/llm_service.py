"""LLM Service - Handles OpenAI API interactions"""
from typing import List, Dict, Any, AsyncGenerator
import json
import logging
from openai import AsyncOpenAI
from app.config import settings
from app.services.mcp_service import mcp_service
from app.utils.exceptions import LLMQueryError

logger = logging.getLogger(__name__)

class LLMService:
    """Service class for managing LLM interactions"""
    
    def __init__(self):
        self._client = AsyncOpenAI(api_key=settings.openai_api_key)
    
    async def query(self, question: str, available_tools: List[Dict[str, Any]]) -> str:
        """
        Execute a query against the LLM with available tools
        
        Args:
            question: User's question
            available_tools: List of available MCP tools in OpenAI format
            
        Returns:
            LLM response as string
        """
        try:
            # 1ì°¨ ìš”ì²­
            resp = await self._client.responses.create(
                model=settings.openai_model,
                input=[{"role": "user", "content": question}],
                tools=available_tools,
            )

            # Tool í˜¸ì¶œì´ ì—†ì„ ë•Œ
            tool_calls = [o for o in resp.output if getattr(o, "type", "") == "function_call"]
            if not tool_calls:
                return resp.output_text

            # Tool í˜¸ì¶œ ì²˜ë¦¬
            next_input = await self._process_tool_calls(question, tool_calls)

            # 2ì°¨ í˜¸ì¶œ -> ìµœì¢… ë‹µë³€
            final = await self._client.responses.create(
                model=settings.openai_model,
                input=next_input,
            )
            return final.output_text
        
        except Exception as e:
            logger.error(f"LLM query failed: {e}")
            raise LLMQueryError(f"LLM ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    async def query_stream(self, question: str, available_tools: List[Dict[str, Any]]) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Execute a streaming query against the LLM with available tools
        
        Args:
            question: User's question
            available_tools: List of available MCP tools in OpenAI format
            
        Yields:
            Dict with event type and data (not SSE formatted strings)
        """
        try:
            # 1ì°¨ ìš”ì²­ - ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ (ê°„ë‹¨ ì¬ì‹œë„)
            logger.warning(f"ğŸ”§ LLM tools available: {len(available_tools)} tools")
            logger.warning(f"ğŸ”§ LLM question: {question[:100]}...")
            
            attempt = 0
            stream = None
            last_error: Exception | None = None
            while attempt < 2 and stream is None:
                try:
                    logger.warning(f"ğŸ”§ Creating OpenAI stream with {len(available_tools)} tools")
                    stream = await self._client.responses.create(
                        model=settings.openai_model,
                        input=[
                            {"role": "system", "content": "You are a web analysis assistant. When given a task, you MUST use ALL available tools as requested. Do not skip any tools."},
                            {"role": "user", "content": question}
                        ],
                        tools=available_tools,
                        stream=True
                    )
                    logger.warning(f"ğŸ”§ OpenAI stream created successfully")
                except Exception as e:
                    last_error = e
                    attempt += 1
                    logger.warning(f"Streaming create failed (attempt {attempt}): {e}")
            if stream is None and last_error is not None:
                raise last_error

            tool_calls = []
            current_call = None
            
            async for chunk in stream:
                # OpenAI Responses API ì´ë²¤íŠ¸ ì²˜ë¦¬
                event_type = type(chunk).__name__
                logger.warning(f"ğŸ”§ Event: {event_type}")
                
                # Function call ì‹œì‘
                if event_type == 'ResponseOutputItemAddedEvent':
                    if hasattr(chunk, 'item') and hasattr(chunk.item, 'type') and chunk.item.type == 'function_call':
                        # chunk.itemì´ ì§ì ‘ ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆìŒ
                        tool_call = {
                            'id': chunk.item.id,
                            'name': chunk.item.name,
                            'arguments': chunk.item.arguments or ''
                        }
                        tool_calls.append(tool_call)
                        current_call = tool_call
                        logger.warning(f"ğŸ”§ Function call started: {chunk.item.name}")
                
                # Function call arguments ìˆ˜ì§‘
                elif event_type == 'ResponseFunctionCallArgumentsDeltaEvent':
                    if current_call and hasattr(chunk, 'delta'):
                        current_call['arguments'] += chunk.delta
                        logger.warning(f"ğŸ”§ Arguments delta: {chunk.delta[:20]}...")
                
                # Function call ì™„ë£Œ
                elif event_type == 'ResponseFunctionCallArgumentsDoneEvent':
                    if current_call:
                        logger.warning(f"ğŸ”§ Function call completed: {current_call['name']}")
                        yield {'type': 'tool_call_delta', 'data': {'tool_calls': tool_calls}}
                
                # ì „ì²´ ì‘ë‹µ ì™„ë£Œ
                elif event_type == 'ResponseCompletedEvent':
                    logger.warning(f"ğŸ”§ Response completed with {len(tool_calls)} tool calls")
                    break

            # Tool í˜¸ì¶œì´ ìˆëŠ” ê²½ìš° ì™„ì„±ëœ tool_calls ë°˜í™˜
            if tool_calls:
                yield {'type': 'tool_calls_ready', 'data': {'tool_calls': tool_calls}}
                async for event in self._process_tool_calls_stream(question, tool_calls):
                    yield event
            
            yield {'type': 'done', 'data': {}}
            
        except Exception as e:
            # ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ: ìƒíƒœë¡œ ì•Œë¦¬ê³  ìƒìœ„ì—ì„œ í´ë°± ì§„í–‰
            logger.warning(f"Streaming LLM query failed, will fallback: {e}")
            yield {'type': 'error', 'data': {'message': f'LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}'}}
            yield {'type': 'done', 'data': {}}
    
    async def _process_tool_calls(self, question: str, tool_calls: List[Any]) -> List[Any]:
        """Process tool calls and return next input for LLM"""
        next_input: List[Any] = [{"role": "user", "content": question}]
        
        for call in tool_calls:
            args = call.arguments
            if isinstance(args, str):
                args = json.loads(args)
            
            result = await mcp_service.call_tool(call.name, args)

            # í˜¸ì¶œ ìì²´ë¥¼ ë©”ì‹œì§€ ë°°ì—´ì— ì¶”ê°€
            next_input.append(call)
            # ì‹¤í–‰ ê²°ê³¼ë¥¼ function_call_output í˜•ì‹ìœ¼ë¡œ ì¶”ê°€
            next_input.append({
                "type": "function_call_output",
                "call_id": call.call_id,
                "output": str(result),
            })
        
        return next_input
    
    async def _handle_streaming_tool_calls(self, delta_tool_calls, tool_calls, current_call):
        """Handle streaming tool calls from delta"""
        for tool_call in delta_tool_calls:
            if tool_call.index == len(tool_calls):
                tool_calls.append({
                    'id': tool_call.id,
                    'name': tool_call.function.name if tool_call.function else '',
                    'arguments': tool_call.function.arguments if tool_call.function else ''
                })
                current_call = tool_calls[-1]
            elif current_call:
                if tool_call.function and tool_call.function.arguments:
                    current_call['arguments'] += tool_call.function.arguments
        return current_call
    
    async def _process_tool_calls_stream(self, question: str, tool_calls: List[Dict]) -> AsyncGenerator[Dict[str, Any], None]:
        """Process tool calls in streaming mode"""
        yield {'type': 'tool_start', 'data': {'message': 'Tool ì‹¤í–‰ ì¤‘...'}}
        
        next_input: List[Any] = [{"role": "user", "content": question}]
        
        # ê° íˆ´ í˜¸ì¶œ ì²˜ë¦¬
        for call in tool_calls:
            try:
                args = json.loads(call['arguments']) if call['arguments'] else {}
                result = await mcp_service.call_tool(call['name'], args)
                
                # Tool ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì „ì†¡
                tool_message = f"Tool '{call['name']}' ì‹¤í–‰ ì™„ë£Œ"
                yield {'type': 'tool_result', 'data': {'tool_name': call['name'], 'message': tool_message, 'result': result}}
                
                # ë‹¤ìŒ ìš”ì²­ì„ ìœ„í•œ ë©”ì‹œì§€ êµ¬ì„±
                next_input.append({
                    "type": "function_call",
                    "call_id": call['id'],
                    "name": call['name'],
                    "arguments": call['arguments']
                })
                next_input.append({
                    "type": "function_call_output", 
                    "call_id": call['id'],
                    "output": str(result)
                })
                
            except Exception as tool_error:
                error_message = f'Tool ì‹¤í–‰ ì˜¤ë¥˜: {str(tool_error)}'
                yield {'type': 'tool_error', 'data': {'tool_name': call['name'], 'error': error_message}}

        # 2ì°¨ í˜¸ì¶œ - ìµœì¢… ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë°
        final_stream = await self._client.responses.create(
            model=settings.openai_model,
            input=next_input,
            stream=True
        )
        
        async for chunk in final_stream:
            if hasattr(chunk, 'choices') and chunk.choices:
                choice = chunk.choices[0]
                if hasattr(choice, 'delta') and choice.delta and hasattr(choice.delta, 'content') and choice.delta.content:
                    yield {'type': 'final_content', 'data': {'content': choice.delta.content}}

# Global service instance
llm_service = LLMService()