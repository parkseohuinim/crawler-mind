"""RAG API router"""
import json
import logging
from typing import List
from fastapi import APIRouter, UploadFile, File, HTTPException, Depends
from fastapi.responses import JSONResponse

from app.domains.rag.schemas.rag_schemas import (
    RagUploadResponse, RagQueryRequest, RagQueryResponse
)
from app.application.rag.rag_service import rag_service

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/rag", tags=["RAG"])


@router.post("/upload", response_model=RagUploadResponse)
async def upload_rag_data(file: UploadFile = File(...)):
    """
    Upload JSON file containing documents for RAG system
    
    Expected JSON format:
    [
        {
            "docId": "unique_id",
            "title": "Document Title",
            "text": "Document content...",
            "url": "https://example.com",
            "murl": "https://m.example.com",
            "hierarchy": ["Category", "Subcategory"],
            "metadata": {...}
        },
        ...
    ]
    """
    logger.info(f"ğŸš€ RAG upload request received: {file.filename} ({file.size} bytes)")
    logger.info(f"ğŸ“‹ Request details: filename={file.filename}, content_type={file.content_type}")
    try:
        # Validate file type
        if not file.filename.endswith('.json'):
            raise HTTPException(
                status_code=400, 
                detail="Only JSON files are allowed"
            )
        
        # Read and parse JSON file
        logger.info(f"ğŸ“– Reading file contents...")
        contents = await file.read()
        logger.info(f"ğŸ“„ File read successfully: {len(contents)} bytes")
        
        try:
            # Handle UTF-8 BOM by trying utf-8-sig first, then fallback to utf-8
            try:
                text_content = contents.decode('utf-8-sig')
            except UnicodeDecodeError:
                text_content = contents.decode('utf-8')
            
            logger.info(f"ğŸ” Parsing JSON data...")
            json_data = json.loads(text_content)
            logger.info(f"âœ… JSON parsed successfully: {len(json_data)} documents")
            
            # ë¬¸ì„œ ìˆ˜ ê²€ì¦ì„ ìœ„í•œ ìƒì„¸ ì •ë³´
            if len(json_data) > 0:
                sample_doc = json_data[0]
                logger.info(f"ğŸ“‹ Sample document structure: {list(sample_doc.keys())}")
                logger.info(f"ğŸ“Š Total documents in JSON file: {len(json_data)}")
                
                # ë¬¸ì„œ í¬ê¸° ë¶„í¬ í™•ì¸
                doc_sizes = [len(str(doc.get('text', ''))) for doc in json_data]
                if doc_sizes:
                    avg_size = sum(doc_sizes) / len(doc_sizes)
                    max_size = max(doc_sizes)
                    min_size = min(doc_sizes)
                    logger.info(f"ğŸ“ Document size stats: avg={avg_size:.0f}, max={max_size}, min={min_size}")
        except (json.JSONDecodeError, UnicodeDecodeError) as e:
            logger.error(f"âŒ JSON parsing failed: {e}")
            raise HTTPException(
                status_code=400, 
                detail=f"Invalid JSON format: {str(e)}"
            )
        
        # Validate that it's a list
        if not isinstance(json_data, list):
            raise HTTPException(
                status_code=400, 
                detail="JSON file must contain an array of documents"
            )
        
        logger.info(f"ğŸ”„ Processing upload of {len(json_data)} documents from {file.filename}")
        
        # Process documents
        logger.info(f"ğŸš€ Starting RAG service processing...")
        result = await rag_service.upload_documents_from_json(json_data)
        logger.info(f"âœ… RAG service processing completed: {result}")
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing file upload: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"Failed to process file: {str(e)}"
        )


@router.post("/query", response_model=RagQueryResponse)
async def query_rag_documents(request: RagQueryRequest):
    """
    Query documents using RAG (Retrieval-Augmented Generation)
    
    This endpoint:
    1. Searches for relevant documents using vector similarity (Qdrant)
    2. Performs text search using OpenSearch
    3. Combines results and generates an answer using LLM
    """
    try:
        if not request.query.strip():
            raise HTTPException(
                status_code=400,
                detail="Query cannot be empty"
            )
        
        logger.info(f"Processing RAG query: {request.query[:100]}...")
        
        result = await rag_service.query_documents(request)
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing RAG query: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to process query: {str(e)}"
        )


@router.delete("/data")
async def delete_all_rag_data():
    """
    Delete all RAG data from both Qdrant and OpenSearch
    
    This will:
    1. Delete all documents from Qdrant collection
    2. Delete all documents from OpenSearch index
    3. Recreate empty collection/index structures
    """
    try:
        logger.info("Starting deletion of all RAG data")
        
        result = await rag_service.delete_all_data()
        
        if result.get("success", False):
            return JSONResponse(
                status_code=200,
                content={
                    "message": result["message"],
                    "processing_time": result["processing_time"],
                    "details": result["details"]
                }
            )
        else:
            return JSONResponse(
                status_code=500,
                content={
                    "error": result["message"],
                    "details": result.get("details", {})
                }
            )
            
    except Exception as e:
        logger.error(f"Error deleting RAG data: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to delete RAG data: {str(e)}"
        )


@router.get("/data/info")
async def get_rag_data_info():
    """
    Get information about stored RAG data
    
    Returns:
    - Number of documents in Qdrant
    - Number of documents in OpenSearch
    - Collection/index status
    """
    try:
        result = await rag_service.get_data_info()
        
        if result.get("success", False):
            return result
        else:
            raise HTTPException(
                status_code=500,
                detail=result.get("message", "Failed to get data info")
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting RAG data info: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get data info: {str(e)}"
        )




@router.get("/search")
async def search_documents(
    query: str,
    limit: int = 20,
    include_content: bool = True
):
    """ë¬¸ì„œ ê²€ìƒ‰ API - í”„ë¡ íŠ¸ì—”ë“œìš©"""
    try:
        qdrant_service, opensearch_service = rag_service._get_services()
        
        # Qdrant ê²€ìƒ‰ (ë²¡í„° ìœ ì‚¬ë„)
        vector_results = await qdrant_service.search_similar_documents(
            query=query,
            limit=limit,
            score_threshold=0.0
        )
        
        # OpenSearch ê²€ìƒ‰ (í…ìŠ¤íŠ¸ ê²€ìƒ‰)
        text_results = await opensearch_service.search_documents(
            query=query,
            limit=limit
        )
        
        # ê²°ê³¼ í†µí•© ë° ì •ê·œí™”
        all_results = []
        seen_ids = set()
        
        # ì ìˆ˜ ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€ê°’ ê³„ì‚°
        vector_scores = [r['score'] for r in vector_results]
        text_scores = [r['score'] for r in text_results]
        vector_max = max(vector_scores) if vector_scores else 1.0
        text_max = max(text_scores) if text_scores else 1.0
        
        # Qdrant ê²°ê³¼ ì²˜ë¦¬
        for result in vector_results:
            if result['id'] in seen_ids:
                continue
            seen_ids.add(result['id'])
            
            payload = result['payload']
            raw_score = result['score']
            # ì½”ì‚¬ì¸ ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ í›„ ì •ê·œí™”
            similarity = max(0.0, 1.0 - raw_score)
            normalized_score = max(0.0, min(1.0, similarity / vector_max if vector_max > 0 else similarity))
            
            all_results.append({
                'id': result['id'],
                'title': payload.get('title', ''),
                'url': payload.get('url', ''),
                'hierarchy': payload.get('hierarchy', []),
                'content': payload.get('content', '') if include_content else '',
                'content_preview': payload.get('content', '')[:200] + '...' if len(payload.get('content', '')) > 200 else payload.get('content', ''),
                'similarity_score': normalized_score,
                'search_type': 'vector',
                'metadata': payload.get('metadata', {})
            })
        
        # OpenSearch ê²°ê³¼ ì²˜ë¦¬
        for result in text_results:
            if result['id'] in seen_ids:
                continue
            seen_ids.add(result['id'])
            
            source = result['source']
            raw_score = result['score']
            # í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì ìˆ˜ ì •ê·œí™”
            normalized_score = max(0.0, min(1.0, raw_score / text_max if text_max > 0 else raw_score))
            
            all_results.append({
                'id': result['id'],
                'title': source.get('title', ''),
                'url': source.get('url', ''),
                'hierarchy': source.get('hierarchy', []),
                'content': source.get('content', '') if include_content else '',
                'content_preview': source.get('content', '')[:200] + '...' if len(source.get('content', '')) > 200 else source.get('content', ''),
                'similarity_score': normalized_score,
                'search_type': 'text',
                'metadata': source.get('metadata', {})
            })
        
        # ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        all_results.sort(key=lambda x: x['similarity_score'], reverse=True)
        
        return {
            "query": query,
            "total_results": len(all_results),
            "vector_results_count": len(vector_results),
            "text_results_count": len(text_results),
            "results": all_results[:limit]
        }
        
    except Exception as e:
        logger.error(f"Document search failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


