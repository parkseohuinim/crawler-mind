import logging
import re
import asyncio
import os
from typing import Dict, Any, List, Callable, Pattern, Tuple, Optional, Awaitable, Set
from urllib.parse import urlparse, unquote
import html
from bs4 import BeautifulSoup, NavigableString
from playwright.async_api import async_playwright, TimeoutError as AsyncTimeoutError
from markdownify import markdownify as md
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

 

# ê¸€ë¡œë²Œ íƒ€ì„ìŠ¤íƒ¬í”„ ë³€ìˆ˜ (preview.pyì—ì„œ ì„¤ì •)
CURRENT_TIMESTAMP = None

def set_current_timestamp(timestamp_str):
    """preview.pyì—ì„œ í˜¸ì¶œí•˜ì—¬ íƒ€ì„ìŠ¤íƒ¬í”„ ì„¤ì •"""
    global CURRENT_TIMESTAMP
    # Noneì´ ì•„ë‹Œ ê°’ë§Œ ì„¤ì • (Noneì€ ì´ˆê¸°í™” ëª©ì ì´ë¯€ë¡œ ì‹¤ì œë¡œëŠ” ê¸°ì¡´ê°’ ìœ ì§€)
    if timestamp_str is not None:
        CURRENT_TIMESTAMP = timestamp_str
    else:
        CURRENT_TIMESTAMP = None

def get_current_timestamp():
    """í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜ (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)"""
    from datetime import datetime
    global CURRENT_TIMESTAMP
    if CURRENT_TIMESTAMP is None:
        CURRENT_TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H%M%S')
        logging.info(f"ğŸ• ìƒˆë¡œìš´ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±: {CURRENT_TIMESTAMP}")
    else:
        # logging.info(f"ğŸ• ê¸°ì¡´ íƒ€ì„ìŠ¤íƒ¬í”„ ì‚¬ìš©: {CURRENT_TIMESTAMP}")
        pass
    return CURRENT_TIMESTAMP

# í˜ì´ì§€ë³„ ì²˜ë¦¬ í•¨ìˆ˜ íƒ€ì… íŒíŠ¸
PageHandlerFunc = Callable[[str, Any], Dict[str, Any]]
AsyncPageHandlerFunc = Callable[[str, Any], Awaitable[Dict[str, Any]]]

# URL íŒ¨í„´ê³¼ í•¸ë“¤ëŸ¬ í•¨ìˆ˜ë¥¼ ë§¤í•‘í•˜ëŠ” ê¸€ë¡œë²Œ ë ˆì§€ìŠ¤íŠ¸ë¦¬
URL_PATTERNS = []  # (ì»´íŒŒì¼ëœ ì •ê·œì‹ íŒ¨í„´, í•¸ë“¤ëŸ¬ í•¨ìˆ˜) íŠœí”Œ ëª©ë¡

# =========================
# 1. ê³µìš© ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =========================
def sanitize_filename(filename, max_length=100):
    """
    íŒŒì¼ëª…ì„ ì•ˆì „í•˜ê³  ê°€ë…ì„± ìˆê²Œ ë³€í™˜
    
    Args:
        filename: ì›ë³¸ íŒŒì¼ëª…
        max_length: ìµœëŒ€ ê¸¸ì´ (ê¸°ë³¸ê°’: 100)
    
    Returns:
        str: ë³€í™˜ëœ ì•ˆì „í•œ íŒŒì¼ëª…
    """
    # 1. ì¹´í…Œê³ ë¦¬ íƒœê·¸ ì²˜ë¦¬: [ê³µì§€] â†’ (ê³µì§€)
    sanitized = re.sub(r'\[([^]]+)\]', r'(\1)', filename)
    
    # 2. ì‹œê°„ í‘œê¸° ê°œì„ : 03/12(ìˆ˜) 01:00 ~ 08:00 â†’ 0312ìˆ˜(0100-0800)
    # ë‚ ì§œ/ì‹œê°„ íŒ¨í„´ ì°¾ê¸°
    datetime_pattern = r'(\d{1,2})/(\d{1,2})\(([^)]+)\)\s*(\d{1,2}):(\d{2})\s*~\s*(\d{1,2}):(\d{2})'
    datetime_match = re.search(datetime_pattern, sanitized)
    if datetime_match:
        month, day, weekday, start_hour, start_min, end_hour, end_min = datetime_match.groups()
        time_str = f"{month.zfill(2)}{day.zfill(2)}{weekday}({start_hour.zfill(2)}{start_min}-{end_hour.zfill(2)}{end_min})"
        sanitized = re.sub(datetime_pattern, time_str, sanitized)
    
    # 3. ê¸°íƒ€ ì‹œê°„ íŒ¨í„´ë“¤ ì²˜ë¦¬
    # HH:MM í˜•ì‹ â†’ HHMM
    sanitized = re.sub(r'(\d{1,2}):(\d{2})', r'\1\2', sanitized)
    
    # MM/DD í˜•ì‹ â†’ MMDD
    sanitized = re.sub(r'(\d{1,2})/(\d{1,2})', r'\1\2', sanitized)
    
    # 4. íŠ¹ìˆ˜ë¬¸ìë¥¼ ì˜ë¯¸ ìˆê²Œ ë³€í™˜
    # ~ (ë¬¼ê²°í‘œ) â†’ to
    sanitized = re.sub(r'\s*~\s*', 'to', sanitized)
    
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    sanitized = re.sub(r'\s+', ' ', sanitized)
    
    # 5. ê¸¸ì´ ì œí•œ
    if len(sanitized) > max_length:
        sanitized = sanitized[:max_length]
    
    # 6. íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
    sanitized = re.sub(r'[<>:"/\\|?*]', '', sanitized)
    
    return sanitized.strip()


def to_mshop_url(url: str) -> str:
    """KT Shop PC URLì„ ëª¨ë°”ì¼(https://m.shop.kt.com:444) í˜•íƒœë¡œ ë³€í™˜"""
    if not url or not url.startswith('http'):
        return ''
    return url.replace('https://shop.kt.com', 'https://m.shop.kt.com:444/m')

def to_mglobalroaming_url(url: str) -> str:
    """ê¸€ë¡œë²Œë¡œë° PC URLì„ ëª¨ë°”ì¼(https://m.globalroaming.kt.com) í˜•íƒœë¡œ ë³€í™˜"""
    if not url or not url.startswith('http'):
        return ''

    from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

    parsed = urlparse(url)
    query_params = parse_qs(parsed.query)
    mobile_netloc = parsed.netloc.replace('globalroaming.kt.com', 'm.globalroaming.kt.com')

    if parsed.path.startswith('/news/view.asp'):
        idx_val = query_params.get('idx', [''])[0]
        mobile_path = '/news/view.asp'
        mobile_query = urlencode({'idx': idx_val})
        return urlunparse((parsed.scheme, mobile_netloc, mobile_path, '', mobile_query, ''))

    return urlunparse((parsed.scheme, mobile_netloc, parsed.path, parsed.params, parsed.query, parsed.fragment))

def to_gigagenie_murl(url: str) -> str:
    """ê¸°ê°€ì§€ë‹ˆ ë¸”ë¡œê·¸ PC URLì„ ëª¨ë°”ì¼(https://gigagenie.kt.com/m/blog) í˜•íƒœë¡œ ë³€í™˜"""
    if not url or not url.startswith('http'):
        return ''
    return url.replace('https://gigagenie.kt.com/blog/', 'https://gigagenie.kt.com/m/blog/')

def format_date_show(date_str):
    """ê³µì—°ì˜ˆë§¤ ë‚ ì§œ í˜•ì‹ ë³€í™˜"""
    if not date_str:
        return ""
    
    # ê¸°ì¡´ íŒ¨í„´ë“¤
    patterns = [
        (r'(\d{4})\.(\d{1,2})\.(\d{1,2})', r'\1-\2-\3'),
        (r'(\d{4})-(\d{1,2})-(\d{1,2})', r'\1-\2-\3'),
        (r'(\d{1,2})/(\d{1,2})/(\d{4})', r'\3-\1-\2'),
    ]
    
    for pattern, replacement in patterns:
        match = re.search(pattern, date_str)
        if match:
            return re.sub(pattern, replacement, date_str)
    
    return date_str

def format_content(content):
    """ì½˜í…ì¸  í¬ë§·íŒ…"""
    if not content:
        return ""
    
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
    content = re.sub(r'^\s+|\s+$', '', content, flags=re.MULTILINE)
    
    return content

def create_markdown(title, date, content):
    """ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ìƒì„±"""
    return f"""# {title}

**ë‚ ì§œ:** {date}

---

{content}
"""

async def get_page_status_code(page):
    """í˜ì´ì§€ì˜ HTTP ìƒíƒœ ì½”ë“œë¥¼ ê°€ì ¸ì˜¤ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜"""
    try:
        # Playwrightì—ì„œ ì‘ë‹µ ê°ì²´ë¥¼ í†µí•´ ìƒíƒœ ì½”ë“œ í™•ì¸
        response = page.url
        # í˜„ì¬ í˜ì´ì§€ì˜ ì‘ë‹µ ì •ë³´ í™•ì¸
        if hasattr(page, '_response') and page._response:
            return page._response.status
        return None
    except Exception:
        return None

# =========================
# 2. í•¸ë“¤ëŸ¬ ë“±ë¡ ë° ë¼ìš°íŒ…
# =========================
def register_page_handler(url_pattern: str, handler_func: AsyncPageHandlerFunc):
    """
    URL íŒ¨í„´ê³¼ í•¸ë“¤ëŸ¬ í•¨ìˆ˜ë¥¼ ë“±ë¡
    
    Args:
        url_pattern: ì •ê·œì‹ íŒ¨í„´
        handler_func: í•¸ë“¤ëŸ¬ í•¨ìˆ˜ (ë¹„ë™ê¸°)
    """
    compiled_pattern = re.compile(url_pattern)
    URL_PATTERNS.append((compiled_pattern, handler_func))

def get_registered_handlers():
    """ë“±ë¡ëœ í•¸ë“¤ëŸ¬ ëª©ë¡ ë°˜í™˜"""
    return [(pattern.pattern, func.__name__) for pattern, func in URL_PATTERNS]

async def route_url(url: str, fclient, menu: str = None) -> Optional[Dict[str, Any]]:
    """
    URLì— ë§ëŠ” í•¸ë“¤ëŸ¬ë¥¼ ì°¾ì•„ì„œ ì‹¤í–‰
    
    Args:
        url: ì²˜ë¦¬í•  URL
        fclient: ìŠ¤í¬ë˜í•‘ í´ë¼ì´ì–¸íŠ¸
        menu: ë©”ë‰´ ì •ë³´
        
    Returns:
        Optional[Dict[str, Any]]: í•¸ë“¤ëŸ¬ ê²°ê³¼ ë˜ëŠ” ë¹ˆ ë”•ì…”ë„ˆë¦¬ (ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ìš©)
    """
    for pattern, handler_func in URL_PATTERNS:
        if pattern.match(url):
            try:
                # handle_gigagenie_faq_playwrightëŠ” 2ê°œ ì¸ìë§Œ ë°›ìŒ
                if handler_func.__name__ == "handle_gigagenie_faq_playwright":
                    result = await handler_func(url, fclient)
                else:
                    result = await handler_func(url, fclient, menu)
                return result
            except Exception as e:
                logging.error(f"âŒ í•¸ë“¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {handler_func.__name__} - {str(e)}")
                return None
    
    # í•¸ë“¤ëŸ¬ê°€ ì—†ëŠ” ê²½ìš° None ë°˜í™˜ (ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ì„ ìœ„í•´)
    logging.info(f"ğŸ” URLì— ë§ëŠ” í•¸ë“¤ëŸ¬ê°€ ì—†ì–´ ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ì„ ì‹¤í–‰í•©ë‹ˆë‹¤: {url}")
    return None


async def handle_membership_partner_list_playwright(url: str, fclient, menu=None) -> dict:
    """
    Playwright(ë¡œì»¬)ë¡œ KT ë©¤ë²„ì‹­ ì œíœ´ ë¸Œëœë“œ ëª©ë¡ í˜ì´ì§€ì—ì„œ ëª¨ë“  ë¸Œëœë“œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¸ë“¤ëŸ¬
    - ë”ë³´ê¸° ë²„íŠ¼ì´ display: none ë  ë•Œê¹Œì§€ ë°˜ë³µ í´ë¦­
    - #cfmClContents ì˜ì—­ë§Œ ì¶”ì¶œí•˜ì—¬ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
    """


    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        response = await page.goto(url, wait_until="networkidle")
        await page.wait_for_timeout(2000)
        
        # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸
        status_code = response.status if response else None

        # ë”ë³´ê¸° ë²„íŠ¼ì´ display:none ë  ë•Œê¹Œì§€ ë°˜ë³µ í´ë¦­
        for _ in range(50):  # ì•ˆì „í•˜ê²Œ ìµœëŒ€ 30íšŒ ì œí•œ
            try:
                display = await page.eval_on_selector(
                    "#btnMoreData",
                    "el => window.getComputedStyle(el).display"
                )
                if display == "none":
                    break
                btn = await page.query_selector("#btnMoreData button.btn.view-more")
                if btn and await btn.is_enabled() and await btn.is_visible():
                    await btn.click()
                    await page.wait_for_timeout(700)
                else:
                    break
            except Exception:
                break

        # #cfmClContents ì˜ì—­ë§Œ ì¶”ì¶œ
        content_html = await page.eval_on_selector("#cfmClContents", "el => el.outerHTML")
        await browser.close()

    # HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
    markdown_body = md(content_html or "(ì½˜í…ì¸  ì—†ìŒ)")
    markdown = markdown_body

    return {
        "url": url,
        "title": "KT ë©¤ë²„ì‹­ ì œíœ´ ë¸Œëœë“œ ëª©ë¡",
        "markdown": markdown,
        "html": content_html,
        "status_code": status_code,
        "special_processed": True,
        "playwright_processed": True
    }

register_page_handler(
    r'https?://membership\.kt\.com/discount/partner/PartnerList\.do',
    handle_membership_partner_list_playwright
)


# =========================
# 3. ê³µì—°ì˜ˆë§¤ ê´€ë ¨ í•¸ë“¤ëŸ¬
# =========================
async def handle_interpark_notice_main(url: str, fclient, menu=None) -> dict:
    from datetime import datetime, timedelta
    import re
    logging.info(f"ê³µì—°ì˜ˆë§¤ ê³µì§€ì‚¬í•­ ë©”ì¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹œì‘: {url}")
    cutoff_date = datetime.now() - timedelta(days=365)
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(viewport={'width': 1920, 'height': 1080}, user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36')
        page = await context.new_page()
        response = await page.goto(url, wait_until='domcontentloaded')
        await page.wait_for_timeout(2000)
        
        # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸
        status_code = response.status if response else None
        notice_data = await page.evaluate(r"""() => {
            const notices = [];
            
            // ë°©ë²• 1: ê¸°ì¡´ ë°©ì‹ (table.board.dir-vertical)
            let table = document.querySelector('table.board.dir-vertical');
            
            // ë°©ë²• 2: ì¼ë°˜ì ì¸ í…Œì´ë¸” ì°¾ê¸°
            if (!table) {
                const tables = document.querySelectorAll('table');
                for (let t of tables) {
                    // NoticeView ë§í¬ê°€ ìˆëŠ” í…Œì´ë¸” ì°¾ê¸°
                    if (t.querySelector('a[href*="NoticeView"]')) {
                        table = t;
                        break;
                    }
                }
            }
            
            // ë°©ë²• 3: NoticeView ë§í¬ë“¤ì„ ì§ì ‘ ì°¾ê¸°
            if (!table) {
                const links = document.querySelectorAll('a[href*="NoticeView"]');
                links.forEach((link, index) => {
                    // ë§í¬ ì£¼ë³€ì—ì„œ ë‚ ì§œ ì •ë³´ ì°¾ê¸°
                    let dateText = '';
                    let numberText = (index + 1).toString();
                    let viewsText = '';
                    
                    // ë¶€ëª¨ í–‰ì—ì„œ ë‚ ì§œ ì°¾ê¸°
                    const row = link.closest('tr');
                    if (row) {
                        const cells = row.querySelectorAll('td');
                        if (cells.length >= 3) {
                            numberText = cells[0] ? cells[0].textContent.trim() : numberText;
                            dateText = cells[2] ? cells[2].textContent.trim() : '';
                            viewsText = cells[3] ? cells[3].textContent.trim() : '';
                        }
                    }
                    
                    // ë‚ ì§œ íŒ¨í„´ìœ¼ë¡œ ìœ íš¨ì„± ê²€ì¦
                    if (!dateText || !/\d{4}[.\-]\d{1,2}[.\-]\d{1,2}/.test(dateText)) {
                        // ë§í¬ ì£¼ë³€ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì°¾ê¸°
                        const parentText = link.parentElement ? link.parentElement.textContent : '';
                        const dateMatch = parentText.match(/(\d{4}[.\-]\d{1,2}[.\-]\d{1,2})/);
                        if (dateMatch) {
                            dateText = dateMatch[1];
                        } else {
                            dateText = new Date().toISOString().split('T')[0]; // fallback
                        }
                    }
                    
                    notices.push({
                        number: numberText,
                        title: link.textContent.trim(),
                        date: dateText,
                        views: viewsText,
                        relativeHref: link.getAttribute('href'),
                        fullHref: link.href
                    });
                });
                
                return { notices: notices, method: 'direct_links' };
            }
            
            // í…Œì´ë¸”ì´ ìˆëŠ” ê²½ìš°ì˜ ì²˜ë¦¬
            const rows = table.querySelectorAll('tr:not(:first-child)'); // í—¤ë” ì œì™¸
            
            rows.forEach(row => {
                const cells = row.querySelectorAll('td');
                if (cells.length >= 3) {
                    // ë‘ ë²ˆì§¸ ì…€ì—ì„œ ë§í¬ ì°¾ê¸°
                    let link = cells[1] ? cells[1].querySelector('a[href*="NoticeView"]') : null;
                    
                    // ë‹¤ë¥¸ ì…€ì—ì„œë„ ë§í¬ ì°¾ê¸°
                    if (!link) {
                        for (let cell of cells) {
                            link = cell.querySelector('a[href*="NoticeView"]');
                            if (link) break;
                        }
                    }
                    
                    if (link) {
                        notices.push({
                            number: cells[0] ? cells[0].textContent.trim() : '',
                            title: link.textContent.trim(),
                            date: cells[2] ? cells[2].textContent.trim() : '',
                            views: cells[3] ? cells[3].textContent.trim() : '',
                            relativeHref: link.getAttribute('href'),
                            fullHref: link.href
                        });
                    }
                }
            });
            
            return { notices: notices, method: 'table', tableFound: !!table };
        }""")
        await browser.close()
    notices = notice_data.get('notices', [])
    method = notice_data.get('method', 'unknown')
    table_found = notice_data.get('tableFound', False)
    if not notices:
        logging.warning("ì¸í„°íŒŒí¬ ê³µì§€ì‚¬í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return {"message": "ì¸í„°íŒŒí¬ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤", "total_processed": 0}
    logging.info(f"ì´ {len(notices)}ê°œ ì¸í„°íŒŒí¬ ê³µì§€ì‚¬í•­ ë°œê²¬ (ì¶”ì¶œë°©ì‹: {method})")
    menus, datas = [], []
    total_processed = 0
    for notice in notices:
        try:
            date_str = notice['date']
            date_match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', date_str)
            if date_match:
                year, month, day = map(int, date_match.groups())
                post_date = datetime(year, month, day)
                if post_date < cutoff_date:
                    logging.info(f"ê²Œì‹œë¬¼ '{notice['title'][:30]}...' ë‚ ì§œ({post_date.strftime('%Y-%m-%d')})ê°€ ê¸°ì¤€ì¼ ì´ì „ì…ë‹ˆë‹¤")
                    break
            logging.info(f"[{total_processed+1}/{len(notices)}] ì²˜ë¦¬ ì¤‘: {notice['title'][:50]}...")
            result = await handle_show_notice(notice['fullHref'], fclient)
            if "error" in result:
                logging.warning(f"ê²Œì‹œë¬¼ ì²˜ë¦¬ ì‹¤íŒ¨: {result['error']}")
                continue
            # í´ë”ëª… ìƒì„±
            formatted_date = ''
            if result.get('date'):
                date_match = re.search(r'(\d{4})[.\-](\d{1,2})[.\-](\d{1,2})', result['date'])
                if date_match:
                    formatted_date = f"{date_match.group(1)[2:]}-{date_match.group(2).zfill(2)}-{date_match.group(3).zfill(2)}"
            title_clean = sanitize_filename(result.get('title', 'unknown'))
            last_folder = f"({formatted_date}){title_clean}" if formatted_date else title_clean
            menus.append({'menu': f"{menu}^{last_folder}" if menu else last_folder, 'url': notice['fullHref']})
            datas.append(result)
            total_processed += 1
            logging.info(f"ì²˜ë¦¬ ì™„ë£Œ: {total_processed}ê°œ")
        except Exception as e:
            logging.error(f"ê²Œì‹œë¬¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            continue
    return {
        "menus": menus,
        "datas": datas,
        "total_processed": total_processed,
        "status": "completed",
        "message": f"ì´ {total_processed}ê°œ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì™„ë£Œ",
        "status_code": status_code
    }

register_page_handler(
    r'https?://kt\.interpark\.com/Partner/KT/Event/NoticeList\.asp.*',
    handle_interpark_notice_main
)

async def handle_show_notice(url: str, fclient) -> dict:
    """
    ê³µì—°ì˜ˆë§¤ ê³µì§€ì‚¬í•­ ê°œë³„ ê²Œì‹œë¬¼ ì²˜ë¦¬ í•¸ë“¤ëŸ¬ (crawl4ai ì‚¬ìš©)
    - ì œëª©, ë‚ ì§œ, ë‹¤ìŒê¸€ ë§í¬ë§Œ selectorë¡œ ì¶”ì¶œ
    - ì „ì²´ ì»¨í…ì¸ ëŠ” crawl4aië¡œ ì²˜ë¦¬
    """
    from datetime import datetime
    
    # 1. ì œëª©, ë‚ ì§œ, ë‹¤ìŒê¸€ ë§í¬ë§Œ playwrightë¡œ ì¶”ì¶œ
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        
        # í˜ì´ì§€ ë¡œë”© ì‹œë„ - ë” ì—¬ìœ ë¡œìš´ ì˜µì…˜
        response = await page.goto(url, wait_until='networkidle', timeout=60000)  # networkidleë¡œ ë³€ê²½, íƒ€ì„ì•„ì›ƒ 60ì´ˆ
        await page.wait_for_timeout(5000)  # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ì„ 5ì´ˆë¡œ ëŠ˜ë¦¼
        
        # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
        status_code = response.status if response else None
        if status_code:
            if status_code >= 400:
                logging.error(f"âŒ ê³µì—°ì˜ˆë§¤ ê³µì§€ ({url}): HTTP {status_code} ì˜¤ë¥˜")
            elif status_code >= 300:
                logging.warning(f"âš ï¸ ê³µì—°ì˜ˆë§¤ ê³µì§€ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
            else:
                logging.info(f"âœ… ê³µì—°ì˜ˆë§¤ ê³µì§€ ({url}): HTTP {status_code} ì„±ê³µ")
        else:
            logging.debug(f"ğŸ” ê³µì—°ì˜ˆë§¤ ê³µì§€ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")
        
        # í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ì¶”ê°€ ëŒ€ê¸°
        try:
            await page.wait_for_load_state('domcontentloaded', timeout=30000)
            await page.wait_for_load_state('networkidle', timeout=30000)
        except Exception as e:
            logging.warning(f"ê³µì—°ì˜ˆë§¤ ê³µì§€ì‚¬í•­ í˜ì´ì§€ ë¡œë“œ ìƒíƒœ ëŒ€ê¸° ì¤‘ íƒ€ì„ì•„ì›ƒ: {str(e)}")
        
        # ì œëª©, ë‚ ì§œ, ë‹¤ìŒê¸€ ë§í¬, ì»¨í…ì¸ ë¥¼ ëª¨ë‘ ì¶”ì¶œ
        metadata = await page.evaluate("""() => {
            const title = document.querySelector('.sub-title06')?.textContent?.trim() || '';
            const date = document.querySelector('.reverse li:first-child')?.textContent?.trim() || '';
            const prevElement = document.querySelector('.inventory-list li:has(strong.next) div a');
            const prevLink = prevElement?.getAttribute('href') || '';
            const prevText = prevElement?.textContent || '';
            
            // ì¸í„°íŒŒí¬ ê³µì§€ì‚¬í•­ ë‚´ìš© ì¶”ì¶œ (ì‹¤ì œ ì…€ë ‰í„° ìš°ì„ )
            let contentHtml = '';
            const contentSelectors = ['.vip-detail-content', '.contents', '.content', '.detail-content', '.notice-content', 'main', '.main-content'];
            for (let selector of contentSelectors) {
                const contentDiv = document.querySelector(selector);
                if (contentDiv && contentDiv.innerHTML.trim()) {
                    contentHtml = contentDiv.innerHTML;
                    break;
                }
            }
            
            return {title, date, prevLink, prevText, contentHtml};
        }""")
        await browser.close()
    
    if not metadata['title'] or not metadata['date']:
        return {"error": "ì œëª© ë˜ëŠ” ë‚ ì§œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    # 2. ì¶”ì¶œëœ ì»¨í…ì¸  HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
    if metadata['contentHtml']:
        content = md(metadata['contentHtml'])
        logging.info(f"âœ… Playwrightë¡œ ê³µì—°ì˜ˆë§¤ ê³µì§€ì‚¬í•­ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ: {len(content)}ì")
    else:
        logging.warning("âš ï¸ ê³µì§€ì‚¬í•­ ë‚´ìš© ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, crawl4ai fallback ì‹œë„")
        # fallbackìœ¼ë¡œ crawl4ai ì‹œë„
        try:
            result = fclient.scrape_url(url)
            if result.success:
                content = result.markdown
                logging.info("âœ… Crawl4ai fallback ì„±ê³µ")
            else:
                content = "ì»¨í…ì¸  ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨"
                logging.error("âŒ Crawl4ai fallback ì‹¤íŒ¨")
        except Exception as e:
            logging.error(f"âŒ Crawl4ai fallbackë„ ì‹¤íŒ¨: {str(e)}")
            content = "ì»¨í…ì¸  ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨"
    
    # 3. ë‚ ì§œ ê²€ì¦ ë° í¬ë§·íŒ…
    formatted_date = format_date_show(metadata['date'])
    if not formatted_date:
        return {"error": "ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì‹¤íŒ¨"}
    
    # ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸  í¬ë§·íŒ…
    formatted_content = format_content(content)
    markdown_content = create_markdown(metadata['title'], metadata['date'].replace('ë‚ ì§œ', ''), formatted_content)
    
    # ë‹¤ìŒê¸€ URL ì²˜ë¦¬
    next_url = None
    if metadata['prevLink'] and "ì´ì „ê¸€ì´ ì—†ìŠµë‹ˆë‹¤" not in metadata['prevText']:
        base_url = url.split('/Partner/KT/Event/')[0]
        next_url = f"{base_url}/Partner/KT/Event/{metadata['prevLink']}"
    
    logging.info(f"ğŸ‰ ê³µì—°ì˜ˆë§¤ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì™„ë£Œ: '{metadata['title']}'")
    # startdate/enddate ê³„ì‚° (ê³µì§€: ê²Œì‹œì¼ë§Œ â†’ startdate)
    startdate_hyphen = "0000-00-00"
    enddate_hyphen = "9999-99-99"
    try:
        import re as _re
        dm = _re.search(r"(\\d{4})[.\\-](\\d{2})[.\\-](\\d{2})", metadata.get('date', ''))
        if dm:
            startdate_hyphen = f"{dm.group(1)}-{dm.group(2)}-{dm.group(3)}"
    except Exception:
        pass

    return {
        "url": url,
        "title": metadata['title'],
        "date": metadata['date'],
        "startdate": startdate_hyphen,
        "enddate": enddate_hyphen,
        "markdown": markdown_content,
        "html": metadata['contentHtml'] or content,
        "next_url": next_url,
        "special_processed": True,
        "playwright_processed": True
    }


# =========================
# 5. ë¡œë° ê´€ë ¨ í•¸ë“¤ëŸ¬
# =========================
async def handle_globalroaming_notice_main(url: str, fclient, menu=None) -> dict:
    from datetime import datetime, timedelta
    import re
    cutoff_date = datetime.now() - timedelta(days=365)
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(viewport={'width': 1920, 'height': 1080}, user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36')
        page = await context.new_page()
        response = await page.goto(url, wait_until='domcontentloaded')
        await page.wait_for_timeout(2000)
        
        # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
        status_code = response.status if response else None
        if status_code:
            if status_code >= 400:
                logging.error(f"âŒ ê¸€ë¡œë²Œë¡œë° ê³µì§€ ({url}): HTTP {status_code} ì˜¤ë¥˜")
            elif status_code >= 300:
                logging.warning(f"âš ï¸ ê¸€ë¡œë²Œë¡œë° ê³µì§€ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
            else:
                logging.info(f"âœ… ê¸€ë¡œë²Œë¡œë° ê³µì§€ ({url}): HTTP {status_code} ì„±ê³µ")
        else:
            logging.debug(f"ğŸ” ê¸€ë¡œë²Œë¡œë° ê³µì§€ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")
        notice_data = await page.evaluate(r"""() => {
            const table = document.querySelector('table.board.dir-vertical');
            if (!table) return { error: 'table not found' };
            
            const notices = [];
            const links = table.querySelectorAll('a[href*="view.asp"]');
            
            links.forEach(link => {
                // í…Œì´ë¸” í–‰ì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
                const row = link.closest('tr');
                if (row) {
                    const cells = row.querySelectorAll('td');
                    if (cells.length >= 4) {
                        notices.push({
                            title: link.textContent.trim(),
                            href: link.href,
                            number: cells[0] ? cells[0].textContent.trim() : '',
                            date: cells[2] ? cells[2].textContent.trim() : '',
                            views: cells[3] ? cells[3].textContent.trim() : ''
                        });
                    }
                }
            });
            
            return { notices: notices };
        }""")
        await browser.close()
    notices = notice_data.get('notices', [])
    if not notices:
        return {"message": "ë¡œë° ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤", "total_processed": 0}
    menus, datas = [], []
    total_processed = 0
    for notice in notices:
        try:
            date_str = notice['date']
            date_match = re.search(r'(\d{4})[\.\-](\d{1,2})[\.\-](\d{1,2})', date_str)
            if date_match:
                year, month, day = map(int, date_match.groups())
                post_date = datetime(year, month, day)
                if post_date < cutoff_date:
                    break
            # ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚ ì§œê°€ ì—†ì„ ìˆ˜ ìˆì–´ ëª©ë¡ì˜ ë“±ë¡ì¼ìë¥¼ ì „ë‹¬í•˜ì—¬ fallbackìœ¼ë¡œ ì‚¬ìš©
            result = await handle_roaming_notice(notice['href'], fclient, notice.get('date'))
            if "error" in result:
                logging.warning(f"âš ï¸ ê²Œì‹œë¬¼ ì²˜ë¦¬ ì‹¤íŒ¨: {result['error']}")
                continue
            formatted_date = ''
            if result.get('date'):
                date_match = re.search(r'(\d{4})[.\-](\d{1,2})[.\-](\d{1,2})', result['date'])
                if date_match:
                    formatted_date = f"{date_match.group(1)[2:]}-{date_match.group(2).zfill(2)}-{date_match.group(3).zfill(2)}"
            title_clean = sanitize_filename(result.get('title', 'unknown'))
            last_folder = title_clean
            menus.append({
                'menu': f"{menu}^{last_folder}" if menu else last_folder,
                'url': notice['href'],
                'murl': to_mglobalroaming_url(notice['href'])
            })

            if not result.get('murl'):
                result['murl'] = to_mglobalroaming_url(result.get('url', notice['href']))
            datas.append(result)
            total_processed += 1
            logging.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {total_processed}ê°œ")
        except Exception as e:
            logging.error(f"âŒ ê²Œì‹œë¬¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            continue
    return {
        "menus": menus,
        "datas": datas,
        "total_processed": total_processed,
        "status": "completed",
        "message": f"ì´ {total_processed}ê°œ ë¡œë° ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì™„ë£Œ"
    }
# ê¸€ë¡œë²Œë¡œë° ê³µì§€ì‚¬í•­ ë©”ì¸ ëª©ë¡ í˜ì´ì§€ ë“±ë¡
register_page_handler(
    r'https?://globalroaming\.kt\.com/news/list\.asp(?:\?.*)?$',
    handle_globalroaming_notice_main
)
async def handle_roaming_notice(url: str, fclient, list_date: str = None, list_title: str = None) -> dict:
    """
    ë¡œë° ê³µì§€ì‚¬í•­ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬
    """
    func_name = "handle_roaming_notice"
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(viewport={'width': 1920, 'height': 1080}, user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36')
        page = await context.new_page()
        
        try:
            # í˜ì´ì§€ ë¡œë”© ì‹œë„ - ë” ì—¬ìœ ë¡œìš´ ì˜µì…˜
            response = await page.goto(url, wait_until='networkidle', timeout=60000)  # networkidleë¡œ ë³€ê²½, íƒ€ì„ì•„ì›ƒ 60ì´ˆ
            await page.wait_for_timeout(5000)  # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ì„ 5ì´ˆë¡œ ëŠ˜ë¦¼
            
            # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
            status_code = response.status if response else None
            if status_code:
                if status_code >= 400:
                    logging.error(f"âŒ ë¡œë° ê³µì§€ ({url}): HTTP {status_code} ì˜¤ë¥˜")
                elif status_code >= 300:
                    logging.warning(f"âš ï¸ ë¡œë° ê³µì§€ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
                else:
                    logging.info(f"âœ… ë¡œë° ê³µì§€ ({url}): HTTP {status_code} ì„±ê³µ")
            else:
                logging.debug(f"ğŸ” ë¡œë° ê³µì§€ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")
            
            # í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ì¶”ê°€ ëŒ€ê¸°
            try:
                await page.wait_for_load_state('domcontentloaded', timeout=30000)
                await page.wait_for_load_state('networkidle', timeout=30000)
            except Exception as e:
                logging.warning(f"ë¡œë° ê³µì§€ì‚¬í•­ í˜ì´ì§€ ë¡œë“œ ìƒíƒœ ëŒ€ê¸° ì¤‘ íƒ€ì„ì•„ì›ƒ: {str(e)}")
            
            # 1. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = await page.evaluate("""() => {
                const getText = (sel) => {
                    const el = document.querySelector(sel);
                    return el ? (el.textContent || '').trim() : '';
                };

                let title = getText('.contents-title') ||
                            getText('.board-view .subject') ||
                            getText('h2.title') ||
                            getText('.title') ||
                            getText('h1');
                if (!title) {
                    const og = document.querySelector('meta[property="og:title"]')?.getAttribute('content')?.trim();
                    if (og) title = og;
                }

                const rawDate = getText('.reg_date') ||
                                getText('.date') ||
                                getText('.info .date') ||
                                getText('.board-info .date') ||
                                'ë‚ ì§œ ì •ë³´ ì—†ìŒ';

                const contentElement = document.querySelector('div.txt') ||
                                       document.querySelector('.board-content') ||
                                       document.querySelector('#cfmClContents') ||
                                       document.querySelector('.content') ||
                                       document.querySelector('.board-body');
                const contentHtml = contentElement ? contentElement.innerHTML : '';

                // ë‹¤ìŒê¸€ ë§í¬ í…ìŠ¤íŠ¸ ê¸°ë°˜ íƒì§€
                let nextLink = '';
                let nextText = '';
                const anchors = Array.from(document.querySelectorAll('a[href]'));
                for (const a of anchors) {
                    const t = (a.textContent || '').trim();
                    if (/ë‹¤ìŒê¸€|ë‹¤ìŒ|Next/i.test(t)) {
                        nextLink = a.getAttribute('href') || '';
                        nextText = t;
                        break;
                    }
                }

                return {
                    title: title || 'ì œëª© ì—†ìŒ',
                    rawDate,
                    contentHtml,
                    nextLink,
                    nextText
                };
            }""")
            
            await browser.close()
        except Exception as e:
            await browser.close()
            logging.error(f"âŒ Playwright ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {"error": f"Playwright ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"}
    
    # 2. ì¶”ì¶œëœ ì»¨í…ì¸  HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
    if metadata['contentHtml']:
        content = md(metadata['contentHtml'])
        logging.info(f"âœ… ë¡œë° ê³µì§€ì‚¬í•­ HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ ì„±ê³µ: ê¸¸ì´={len(content)}")
    else:
        # fallbackìœ¼ë¡œ crawl4ai ì‹œë„
        logging.info("âš ï¸ ë¡œë° ê³µì§€ì‚¬í•­ HTML ë‚´ìš©ì´ ì—†ì–´ crawl4ai fallback ì‹œë„")
        try:
            result = await fclient.scrape_single_url(url)
            if result.get("markdown"):
                content = result["markdown"]
                logging.info(f"âœ… ë¡œë° ê³µì§€ì‚¬í•­ crawl4ai fallback ì„±ê³µ: ê¸¸ì´={len(content)}")
            else:
                content = "ì»¨í…ì¸  ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨"
                logging.error("âŒ ë¡œë° ê³µì§€ì‚¬í•­ crawl4ai fallback ì‹¤íŒ¨: markdown ì—†ìŒ")
        except Exception as e:
            content = "ì»¨í…ì¸  ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨"
            logging.error(f"âŒ ë¡œë° ê³µì§€ì‚¬í•­ crawl4ai fallback ì‹¤íŒ¨: {str(e)}")
    
    # 3. ì¹´í…Œê³ ë¦¬ì™€ ë‚ ì§œ ë¶„ë¦¬ ì²˜ë¦¬
    category = ''
    category_date_match = re.search(r'^(.+?)\s*(\d{4}[\.\-]\d{2}[\.\-]\d{2})', metadata['rawDate'])
    if category_date_match:
        category = category_date_match.group(1).strip()
        actual_date = category_date_match.group(2).replace('-', '.')
        logging.info(f"âœ… ë¡œë° ê³µì§€ì‚¬í•­ ë‚ ì§œ íŒŒì‹± ì„±ê³µ: date='{actual_date}', category='{category}'")
    else:
        # ë¶„ë¦¬ ì‹¤íŒ¨ ì‹œ ì „ì²´ë¥¼ ë‚ ì§œë¡œ ì²˜ë¦¬ ì‹œë„
        date_only_match = re.search(r'(\d{4}[\.\-]\d{2}[\.\-]\d{2})', metadata['rawDate'])
        if date_only_match:
            actual_date = date_only_match.group(1).replace('-', '.')
            logging.info(f"âœ… ë¡œë° ê³µì§€ì‚¬í•­ ë‚ ì§œ íŒŒì‹± ì„±ê³µ (fallback): date='{actual_date}'")
        else:
            # ëª©ë¡ í˜ì´ì§€ì˜ ë“±ë¡ì¼ìë¥¼ fallbackìœ¼ë¡œ ì‚¬ìš© ì‹œë„
            if list_date:
                list_date_match = re.search(r'(\d{4}[\.\-]\d{2}[\.\-]\d{2})', list_date)
                if list_date_match:
                    actual_date = list_date_match.group(1).replace('-', '.')
                    logging.info(f"âœ… ë¡œë° ê³µì§€ì‚¬í•­ ë‚ ì§œ íŒŒì‹± ì„±ê³µ (list fallback): date='{actual_date}'")
                else:
                    logging.error(f"âŒ ë¡œë° ê³µì§€ì‚¬í•­ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ (list fallback ë¶ˆê°€): raw='{metadata['rawDate']}', list='{list_date}'")
                    actual_date = ''
            else:
                logging.error(f"âŒ ë¡œë° ê³µì§€ì‚¬í•­ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {metadata['rawDate']}")
                actual_date = ''
    
    # 4. ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸  í¬ë§·íŒ…
    formatted_content = format_content(content)
    # ì¹´í…Œê³ ë¦¬ ì •ë³´ë„ ë§ˆí¬ë‹¤ìš´ì— í¬í•¨
    date_display = f"{actual_date}" + (f" (ì¹´í…Œê³ ë¦¬: {category})" if category else "")
    
    # ì œëª© ìµœì¢… ê²°ì • (ìƒì„¸ê°€ ë¹„ë©´ ëª©ë¡ ì œëª© ì‚¬ìš©)
    final_title = metadata['title'] if metadata['title'] else (list_title or 'ì œëª© ì—†ìŒ')
    markdown_content = create_markdown(final_title, date_display, formatted_content)
    
    # ë‹¤ìŒê¸€ URL ì²˜ë¦¬
    next_url = None
    if metadata['nextLink']:
        base_url = url.split('/news/')[0]
        next_url = f"{base_url}/news/{metadata['nextLink']}"
    
    logging.info(f"ğŸ‰ ë¡œë° ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì™„ë£Œ: title='{final_title}', date='{actual_date}'")

    # startdate/enddate ê³„ì‚°
    def _normalize_hyphen_date(s: str) -> str:
        s = s.strip()
        m = re.search(r"(\d{4})[\.\-ë…„]\s*(\d{1,2})[\.\-ì›”]\s*(\d{1,2})", s)
        if not m:
            m = re.search(r"(\d{4})[\.\-](\d{1,2})[\.\-](\d{1,2})", s)
        if m:
            y, mo, d = m.group(1), m.group(2), m.group(3)
            return f"{y}-{mo.zfill(2)}-{d.zfill(2)}"
        return ""

    startdate_hyphen = "0000-00-00"
    enddate_hyphen = "9999-99-99"

    text_for_range = (metadata.get('contentHtml') or '') + ' ' + (content or '')
    inherit_left_year = False
    m_range = re.search(r"(\d{4}[ë…„\.\-]\s*\d{1,2}[ì›”\.\-]\s*\d{1,2}ì¼?)\s*[~\-â€“]\s*(\d{4}[ë…„\.\-]?\s*\d{1,2}[ì›”\.\-]\s*\d{1,2}ì¼?)", text_for_range)
    if not m_range:
        m_range = re.search(r"(\d{4}[ë…„\.\-]\s*\d{1,2}[ì›”\.\-]\s*\d{1,2}ì¼?)\s*[~\-â€“]\s*(\d{1,2}[ì›”\.\-]\s*\d{1,2}ì¼?)", text_for_range)
        if m_range:
            inherit_left_year = True

    if m_range:
        left = m_range.group(1)
        right = m_range.group(2)
        left_h = _normalize_hyphen_date(left)
        if inherit_left_year and left_h:
            ly = left_h.split('-')[0]
            m_right = re.search(r"(\d{1,2})[ì›”\.\-]\s*(\d{1,2})", right)
            if m_right:
                right_h = f"{ly}-{m_right.group(1).zfill(2)}-{m_right.group(2).zfill(2)}"
            else:
                right_h = _normalize_hyphen_date(right)
        else:
            right_h = _normalize_hyphen_date(right)
        if left_h:
            startdate_hyphen = left_h
        if right_h:
            enddate_hyphen = right_h
    else:
        if actual_date:
            startdate_hyphen = actual_date.replace('.', '-')

    return {
        "url": url,
        "murl": to_mglobalroaming_url(url),
        "title": final_title,
        "date": actual_date,
        "markdown": markdown_content,
        "html": metadata['contentHtml'] or content,
        "startdate": startdate_hyphen,
        "enddate": enddate_hyphen,
        "next_url": next_url,
        "special_processed": True,
        "playwright_processed": True
    }

# =========================
# 6. KT ê³µì§€/ë„¤íŠ¸ì›Œí¬/ì•ˆì „í•œ í†µì‹ ìƒí™œ ê´€ë ¨ í•¸ë“¤ëŸ¬
# =========================
async def handle_kt_notice_main(url: str, fclient, menu=None) -> dict:
    logging.info(f"KT ê³µì§€ì‚¬í•­ ë©”ì¸ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, menu={menu}")
    from datetime import datetime, timedelta
    import re
    cutoff_date = datetime.now() - timedelta(days=365)
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(viewport={'width': 1920, 'height': 1080}, user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36')
        page = await context.new_page()
        response = await page.goto(url, wait_until='domcontentloaded')
        first_notice_link = None
        
        # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
        status_code = response.status if response else None
        if status_code:
            if status_code >= 400:
                logging.error(f"âŒ KT ê³µì§€ ({url}): HTTP {status_code} ì˜¤ë¥˜")
            elif status_code >= 300:
                logging.warning(f"âš ï¸ KT ê³µì§€ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
            else:
                logging.info(f"âœ… KT ê³µì§€ ({url}): HTTP {status_code} ì„±ê³µ")
        else:
            logging.debug(f"ğŸ” KT ê³µì§€ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")
        for attempt in range(3):
            try:
                await page.wait_for_selector('a[data-bno]', timeout=10000)
            except Exception as e:
                pass
            await page.wait_for_timeout(2000)
            first_notice_link = await page.evaluate("""() => {
                const firstElement = document.querySelector('a[data-bno]');
                if (firstElement) {
                    const bno = firstElement.getAttribute('data-bno');
                    return `https://inside.kt.com/html/notice/notice_detail.html?bno=${bno}`;
                }
                return null;
            }""")
            if first_notice_link:
                break
            elif attempt < 2:
                pass
        await browser.close()
    if not first_notice_link:
        return {"error": "ì²« ë²ˆì§¸ ê³µì§€ì‚¬í•­ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
    total_processed = 0
    current_url = first_notice_link
    menus, datas = [], []
    logging.info(f"ì´ {total_processed}ê°œ ê³µì§€ì‚¬í•­ ê²Œì‹œë¬¼ ì²˜ë¦¬ ì‹œì‘")
    while current_url and total_processed < 1000:
        try:
            logging.info(f"ğŸ”„ {total_processed + 1}ë²ˆì§¸ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì‹œì‘: url={current_url}")
            result = await handle_kt_notice_detail(current_url, fclient, cutoff_date)
            if "error" in result:
                logging.warning(f"âŒ {total_processed + 1}ë²ˆì§¸ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì‹¤íŒ¨: {result['error']}")
                break
            if result.get("date_cutoff_reached"):
                logging.info(f"â° ë‚ ì§œ cutoff ë„ë‹¬: {result.get('date', 'unknown')}")
                break
            formatted_date = ''
            if result.get('date'):
                date_match = re.search(r'(\d{4})[.\-](\d{1,2})[.\-](\d{1,2})', result['date'])
                if date_match:
                    formatted_date = f"{date_match.group(1)[2:]}-{date_match.group(2).zfill(2)}-{date_match.group(3).zfill(2)}"
            title_clean = sanitize_filename(result.get('title', 'unknown'))
            last_folder = f"({formatted_date}){title_clean}" if formatted_date else title_clean
            menus.append({'menu': f"{menu}^{last_folder}" if menu else last_folder, 'url': current_url})
            datas.append(result)
            total_processed += 1
            logging.info(f"âœ… {total_processed}ë²ˆì§¸ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì™„ë£Œ: '{result.get('title', 'unknown')}' ({formatted_date})")
            current_url = result.get("next_url")
            if not current_url:
                logging.info("ğŸ”— ë‹¤ìŒ ê²Œì‹œë¬¼ ë§í¬ê°€ ì—†ì–´ ì²˜ë¦¬ ì¢…ë£Œ")
                break
        except Exception as e:
            logging.error(f"âŒ {total_processed + 1}ë²ˆì§¸ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            break
    logging.info(f"ğŸ‰ KT ê³µì§€ì‚¬í•­ ë©”ì¸ ì²˜ë¦¬ ì™„ë£Œ: ì´ {total_processed}ê°œ ê²Œì‹œë¬¼ ì²˜ë¦¬ë¨")
    return {
        "menus": menus,
        "datas": datas,
        "total_processed": total_processed,
        "status": "completed",
        "message": f"ì´ {total_processed}ê°œ ê²Œì‹œë¬¼ ì²˜ë¦¬ë¨"
    }

async def handle_kt_notice_detail(url: str, fclient, cutoff_date=None) -> dict:
    """
    KT ê³µì§€ì‚¬í•­ ê°œë³„ ê²Œì‹œë¬¼ ì²˜ë¦¬ í•¸ë“¤ëŸ¬ (crawl4ai ì‚¬ìš©)
    - ì œëª©, ë‚ ì§œ(ì¹´í…Œê³ ë¦¬), ë‹¤ìŒê¸€ ë§í¬ë§Œ selectorë¡œ ì¶”ì¶œ
    - ì „ì²´ ì»¨í…ì¸ ëŠ” crawl4aië¡œ ì²˜ë¦¬
    """
    from datetime import datetime
    import re
    
    if cutoff_date is None:
        from datetime import timedelta
        cutoff_date = datetime.now() - timedelta(days=365)
    
    logging.info(f"ğŸ”„ KT ê³µì§€ì‚¬í•­ ê°œë³„ ê²Œì‹œë¬¼ ì²˜ë¦¬: {url}")
    
    # 1. ì œëª©, ë‚ ì§œ, ë‹¤ìŒê¸€ ë§í¬ë§Œ playwrightë¡œ ì¶”ì¶œ
    max_retries = 3
    metadata = None
    
    for attempt in range(max_retries):
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
                )
                page = await context.new_page()
                
                # ë¡œë”© ì „ëµì„ ì‹œë„ë³„ë¡œ ë‹¤ë¥´ê²Œ ì ìš©
                if attempt == 0:
                    response = await page.goto(url, wait_until='domcontentloaded', timeout=30000)
                    await page.wait_for_timeout(3000)
                elif attempt == 1:
                    response = await page.goto(url, wait_until='load', timeout=40000)
                    await page.wait_for_timeout(5000)
                else:
                    response = await page.goto(url, wait_until='networkidle', timeout=50000)
                    await page.wait_for_timeout(7000)
                
                # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
                status_code = response.status if response else None
                if status_code:
                    if status_code >= 400:
                        logging.error(f"âŒ KT ê³µì§€ ìƒì„¸ ({url}): HTTP {status_code} ì˜¤ë¥˜")
                    elif status_code >= 300:
                        logging.warning(f"âš ï¸ KT ê³µì§€ ìƒì„¸ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
                    else:
                        logging.info(f"âœ… KT ê³µì§€ ìƒì„¸ ({url}): HTTP {status_code} ì„±ê³µ")
                else:
                    logging.debug(f"ğŸ” KT ê³µì§€ ìƒì„¸ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")
                
                # ì œëª©, ë‚ ì§œ, ë‹¤ìŒê¸€ ë§í¬, ì»¨í…ì¸ ë¥¼ ëª¨ë‘ ì¶”ì¶œ
                metadata = await page.evaluate("""() => {
                    const title = document.querySelector('h1.title');
                    const dateElement = document.querySelector('.desc');
                    const contentDiv = document.querySelector('.txt-content');
                    
                    // ë‹¤ìŒê¸€ ë§í¬ ì°¾ê¸° - data-bno ì†ì„± ê¸°ë°˜
                    let nextLink = '';
                    
                    // ë°©ë²• 1: data-bno ì†ì„±ì´ ìˆê³  "ë‹¤ìŒê¸€" ê´€ë ¨ ìš”ì†Œ
                    const nextElement = document.querySelector('a[data-bno].next-area');
                    if (nextElement) {
                        const nextBno = nextElement.getAttribute('data-bno');
                        if (nextBno) {
                            const currentUrl = window.location.href;
                            const baseUrl = currentUrl.split('?')[0];
                            nextLink = `${baseUrl}?bno=${nextBno}`;
                        }
                    }
                    
                    // ë°©ë²• 2: "ë‹¤ìŒê¸€" í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ìš”ì†Œì—ì„œ data-bno ì°¾ê¸°
                    if (!nextLink) {
                        const allElements = document.querySelectorAll('*');
                        for (let elem of allElements) {
                            if (elem.textContent && elem.textContent.includes('ë‹¤ìŒê¸€')) {
                                const parent = elem.closest('a[data-bno]');
                                if (parent) {
                                    const nextBno = parent.getAttribute('data-bno');
                                    if (nextBno) {
                                        const currentUrl = window.location.href;
                                        const baseUrl = currentUrl.split('?')[0];
                                        nextLink = `${baseUrl}?bno=${nextBno}`;
                                        break;
                                    }
                                }
                            }
                        }
                    }
                    
                    // ë°©ë²• 3: ê¸°ì¡´ ë°©ì‹ (hrefì— bnoê°€ ìˆëŠ” ê²½ìš°)
                    if (!nextLink) {
                        const nextLinks = document.querySelectorAll('a[href*="bno="]');
                        for (let link of nextLinks) {
                            if (link.textContent.includes('ë‹¤ìŒê¸€') || link.textContent.includes('ë‹¤ìŒ')) {
                                nextLink = link.href;
                                break;
                            }
                        }
                    }
                    
                    return {
                        title: title ? title.textContent.trim() : '',
                        rawDate: dateElement ? dateElement.textContent.trim() : '',
                        nextLink: nextLink,
                        contentHtml: contentDiv ? contentDiv.innerHTML : ''
                    };
                }""")
                
                await browser.close()
                
                # ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
                if metadata['title'] and metadata['rawDate']:
                    if attempt > 0:
                        logging.info(f"âœ… ì¬ì‹œë„ {attempt + 1}íšŒì°¨ì—ì„œ ì„±ê³µ")
                    break
                elif attempt < max_retries - 1:
                    logging.warning(f"âš ï¸ ì‹œë„ {attempt + 1}íšŒì°¨ ì‹¤íŒ¨ - ì œëª©/ë‚ ì§œ ì •ë³´ ì—†ìŒ, ì¬ì‹œë„ ì¤‘...")
                    continue
                else:
                    return {"error": "ì œëª© ë˜ëŠ” ë‚ ì§œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
                    
        except Exception as e:
            if attempt < max_retries - 1:
                logging.warning(f"âš ï¸ ì‹œë„ {attempt + 1}íšŒì°¨ì—ì„œ ì—ëŸ¬ ë°œìƒ: {str(e)} - ì¬ì‹œë„ ì¤‘...")
                continue
            else:
                logging.error(f"âŒ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨: {str(e)}")
                return {"error": f"í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {str(e)}"}
    
    # 2. ì¶”ì¶œëœ ì»¨í…ì¸  HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
    if metadata['contentHtml']:
        content = md(metadata['contentHtml'])
        logging.info(f"âœ… Playwrightë¡œ ê³µì§€ì‚¬í•­ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ: {len(content)}ì")
    else:
        logging.warning("âš ï¸ '.txt-content' ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, crawl4ai fallback ì‹œë„")
        # fallbackìœ¼ë¡œ crawl4ai ì‹œë„
        try:
            result = fclient.scrape_url(url)
            if result.success:
                content = result.markdown
                logging.info("âœ… Crawl4ai fallback ì„±ê³µ")
            else:
                content = "ì»¨í…ì¸  ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨"
                logging.error("âŒ Crawl4ai fallback ì‹¤íŒ¨")
        except Exception as e:
            logging.error(f"âŒ Crawl4ai fallbackë„ ì‹¤íŒ¨: {str(e)}")
            content = "ì»¨í…ì¸  ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨"
    
    # 3. ì¹´í…Œê³ ë¦¬ì™€ ë‚ ì§œ ë¶„ë¦¬ ì²˜ë¦¬
    category = ""
    actual_date = ""
    
    category_date_match = re.match(r'^(.+?)(\d{4}\.\d{2}\.\d{2})$', metadata['rawDate'])
    if category_date_match:
        category = category_date_match.group(1).strip()
        actual_date = category_date_match.group(2)
        logging.info(f"âœ… ì¹´í…Œê³ ë¦¬ ë¶„ë¦¬ ì„±ê³µ: ì¹´í…Œê³ ë¦¬='{category}', ë‚ ì§œ='{actual_date}'")
    else:
        # ë¶„ë¦¬ ì‹¤íŒ¨ ì‹œ ì „ì²´ë¥¼ ë‚ ì§œë¡œ ì²˜ë¦¬ ì‹œë„
        date_only_match = re.search(r'(\d{4}\.\d{2}\.\d{2})', metadata['rawDate'])
        if date_only_match:
            actual_date = date_only_match.group(1)
            logging.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¦¬ ì‹¤íŒ¨, ë‚ ì§œë§Œ ì¶”ì¶œ: '{actual_date}'")
        else:
            logging.warning(f"âŒ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {metadata['rawDate']}")
            return {"error": f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {metadata['rawDate']}"}
    
    # ë‚ ì§œ cutoff ì²´í¬
    date_match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', actual_date)
    if date_match:
        year, month, day = map(int, date_match.groups())
        post_date = datetime(year, month, day)
        
        if post_date < cutoff_date:
            logging.info(f"â° ê²Œì‹œë¬¼ ë‚ ì§œ({post_date.strftime('%Y-%m-%d')})ê°€ ê¸°ì¤€ì¼ ì´ì „ì…ë‹ˆë‹¤")
            return {"date_cutoff_reached": True}
    else:
        logging.warning(f"âŒ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {actual_date}")
    
    # 4. ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸  í¬ë§·íŒ…
    formatted_content = format_content(content)
    # ì¹´í…Œê³ ë¦¬ ì •ë³´ë„ ë§ˆí¬ë‹¤ìš´ì— í¬í•¨
    date_display = f"{actual_date}" + (f" (ì¹´í…Œê³ ë¦¬: {category})" if category else "")
    markdown_content = create_markdown(metadata['title'], date_display, formatted_content)
    
    # ë‹¤ìŒê¸€ URL ì²˜ë¦¬
    next_url = None
    if metadata['nextLink'] and 'bno=' in metadata['nextLink']:
        next_url = metadata['nextLink']
    
    # ëª¨ë°”ì¼ URL ìƒì„± (inside.kt.com -> m.kt.com)
    mobile_url = url.replace('inside.kt.com', 'm.kt.com') if 'inside.kt.com' in url else None
    
    logging.info(f"ğŸ‰ KT ê³µì§€ì‚¬í•­ ìƒì„¸ ì²˜ë¦¬ ì™„ë£Œ: title='{metadata['title']}', date='{actual_date}'")
    # startdate/enddate (ê³µì§€: ê²Œì‹œì¼ë§Œ â†’ startdateì— ì €ì¥)
    startdate_hyphen = "0000-00-00"
    enddate_hyphen = "9999-99-99"
    try:
        dm = re.search(r"(\d{4})[.\-](\d{2})[.\-](\d{2})", actual_date)
        if dm:
            startdate_hyphen = f"{dm.group(1)}-{dm.group(2)}-{dm.group(3)}"
    except Exception:
        pass

    return {
        "url": url,
        "mobile_url": mobile_url,
        "murl": mobile_url or '',
        "title": metadata['title'],
        "category": category,
        "date": actual_date,
        "raw_date": metadata['rawDate'],
        "startdate": startdate_hyphen,
        "enddate": enddate_hyphen,
        "markdown": markdown_content,
        "html": metadata['contentHtml'] or content,
        "next_url": next_url,
        "special_processed": True,
        "playwright_processed": True
    }

register_page_handler(
    r'https?://inside\.kt\.com/html/notice/notice_list\.html',
    handle_kt_notice_main
)

async def handle_network_notice_main(url: str, fclient, menu=None) -> dict:
    logging.info(f"ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ë©”ì¸ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, menu={menu}")
    from datetime import datetime, timedelta
    import re
    cutoff_date = datetime.now() - timedelta(days=365)
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(viewport={'width': 1920, 'height': 1080}, user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36')
        page = await context.new_page()
        response = await page.goto(url, wait_until='domcontentloaded')
        first_bno = None
        
        # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
        status_code = response.status if response else None
        if status_code:
            if status_code >= 400:
                logging.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ê³µì§€ ({url}): HTTP {status_code} ì˜¤ë¥˜")
            elif status_code >= 300:
                logging.warning(f"âš ï¸ ë„¤íŠ¸ì›Œí¬ ê³µì§€ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
            else:
                logging.info(f"âœ… ë„¤íŠ¸ì›Œí¬ ê³µì§€ ({url}): HTTP {status_code} ì„±ê³µ")
        else:
            logging.debug(f"ğŸ” ë„¤íŠ¸ì›Œí¬ ê³µì§€ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")
        for attempt in range(3):
            try:
                await page.wait_for_selector('a[data-bno]', timeout=10000)
            except Exception as e:
                pass
            await page.wait_for_timeout(2000)
            first_bno = await page.evaluate("""() => {
                const firstLink = document.querySelector('a[data-bno]');
                return firstLink ? firstLink.getAttribute('data-bno') : null;
            }""")
            if first_bno:
                break
            elif attempt < 2:
                pass
        await browser.close()
    if not first_bno:
        return {"error": "ì²« ë²ˆì§¸ ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
    first_url = f"https://inside.kt.com/html/notice/net_notice_detail.html?bno={first_bno}"
    current_url = first_url
    total_processed = 0
    menus, datas = [], []
    max_iterations = 1000
    logging.info(f"ì´ {total_processed}ê°œ ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ê²Œì‹œë¬¼ ì²˜ë¦¬ ì‹œì‘")
    for i in range(max_iterations):
        if not current_url:
            break
        try:
            logging.info(f"ğŸ”„ {total_processed + 1}ë²ˆì§¸ ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì‹œì‘: url={current_url}")
            result = await handle_network_notice_detail(current_url, fclient, cutoff_date)
            if "error" in result:
                logging.warning(f"âŒ {total_processed + 1}ë²ˆì§¸ ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì‹¤íŒ¨: {result['error']}")
                break
            elif result.get("date_cutoff_reached"):
                logging.info(f"â° ë‚ ì§œ cutoff ë„ë‹¬: {result.get('date', 'unknown')}")
                break
            else:
                formatted_date = ''
                if result.get('date'):
                    date_match = re.search(r'(\d{4})[.\-](\d{1,2})[.\-](\d{1,2})', result['date'])
                    if date_match:
                        formatted_date = f"{date_match.group(1)[2:]}-{date_match.group(2).zfill(2)}-{date_match.group(3).zfill(2)}"
                title_clean = sanitize_filename(result.get('title', 'unknown'))
                last_folder = f"({formatted_date}){title_clean}" if formatted_date else title_clean
                menus.append({'menu': f"{menu}^{last_folder}" if menu else last_folder, 'url': current_url, 'murl': result.get('murl')})
                datas.append(result)
                total_processed += 1
                logging.info(f"âœ… {total_processed}ë²ˆì§¸ ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì™„ë£Œ: '{result.get('title', 'unknown')}' ({formatted_date})")
                current_url = result.get("next_url")
                if not current_url:
                    logging.info("ğŸ”— ë‹¤ìŒ ê²Œì‹œë¬¼ ë§í¬ê°€ ì—†ì–´ ì²˜ë¦¬ ì¢…ë£Œ")
                    break
        except Exception as e:
            logging.error(f"âŒ {total_processed + 1}ë²ˆì§¸ ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            break
    logging.info(f"ğŸ‰ ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ë©”ì¸ ì²˜ë¦¬ ì™„ë£Œ: ì´ {total_processed}ê°œ ê²Œì‹œë¬¼ ì²˜ë¦¬ë¨")
    return {
        "menus": menus,
        "datas": datas,
        "total_processed": total_processed,
        "status": "completed",
        "message": f"ì´ {total_processed}ê°œ ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì™„ë£Œ"
    }
async def handle_network_notice_detail(url: str, fclient, cutoff_date=None) -> dict:
    logging.info(f"ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ìƒì„¸ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, cutoff_date={cutoff_date}")
    """
    ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ê°œë³„ ê²Œì‹œë¬¼ ì²˜ë¦¬ í•¸ë“¤ëŸ¬ (crawl4ai ì‚¬ìš©)
    - ì œëª©, ë‚ ì§œ(ì¹´í…Œê³ ë¦¬), ë‹¤ìŒê¸€ ë§í¬ë§Œ selectorë¡œ ì¶”ì¶œ
    - ì „ì²´ ì»¨í…ì¸ ëŠ” crawl4aië¡œ ì²˜ë¦¬
    """
    from datetime import datetime
    func_name = "handle_network_notice_detail"
    import re
    
    if cutoff_date is None:
        from datetime import timedelta
        cutoff_date = datetime.now() - timedelta(days=365)
    
    # 1. ì œëª©, ë‚ ì§œ, ë‹¤ìŒê¸€ ë§í¬ë§Œ playwrightë¡œ ì¶”ì¶œ
    metadata = None
    try:
        logging.info(f"ğŸ”„ ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ìƒì„¸ í˜ì´ì§€ ì§„ì…: url={url}")
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
            )
            page = await context.new_page()
            
            # í˜ì´ì§€ ë¡œë”© ì‹œë„ - ë” ì—¬ìœ ë¡œìš´ ì˜µì…˜
            response = await page.goto(url, wait_until='networkidle', timeout=60000)  # networkidleë¡œ ë³€ê²½, íƒ€ì„ì•„ì›ƒ 60ì´ˆ
            await page.wait_for_timeout(5000)  # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ì„ 5ì´ˆë¡œ ëŠ˜ë¦¼
            
            # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
            status_code = response.status if response else None
            if status_code:
                if status_code >= 400:
                    logging.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ê³µì§€ ìƒì„¸ ({url}): HTTP {status_code} ì˜¤ë¥˜")
                elif status_code >= 300:
                    logging.warning(f"âš ï¸ ë„¤íŠ¸ì›Œí¬ ê³µì§€ ìƒì„¸ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
                else:
                    logging.info(f"âœ… ë„¤íŠ¸ì›Œí¬ ê³µì§€ ìƒì„¸ ({url}): HTTP {status_code} ì„±ê³µ")
            else:
                logging.debug(f"ğŸ” ë„¤íŠ¸ì›Œí¬ ê³µì§€ ìƒì„¸ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")
            
            # í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ì¶”ê°€ ëŒ€ê¸°
            try:
                await page.wait_for_load_state('domcontentloaded', timeout=30000)
                await page.wait_for_load_state('networkidle', timeout=30000)
            except Exception as e:
                logging.warning(f"ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ í˜ì´ì§€ ë¡œë“œ ìƒíƒœ ëŒ€ê¸° ì¤‘ íƒ€ì„ì•„ì›ƒ: {str(e)}")
            
            # ì œëª©, ë‚ ì§œ, ë‹¤ìŒê¸€ ë§í¬ ì¶”ì¶œ
            title = await page.evaluate("""() => {
                const t = document.querySelector('h1.title');
                return t ? t.textContent.trim() : '';
            }""")
            raw_date = await page.evaluate("""() => {
                const d = document.querySelector('.desc');
                return d ? d.textContent.trim() : '';
            }""")
            
            # ì„±ê³µ ì¡°ê±´ ì²´í¬: ì œëª©ê³¼ ë‚ ì§œê°€ ëª¨ë‘ ìˆìœ¼ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
            if title and raw_date:
                logging.info(f"âœ… ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ìƒì„¸ í˜ì´ì§€ ì§„ì… ì„±ê³µ: title='{title}', date='{raw_date}'")
                # ë³¸ë¬¸ ì—¬ëŸ¬ í›„ë³´ ì…€ë ‰í„° ìˆœì°¨ ì‹œë„
                content_html = ""
                for selector in ['.txt-content', '.contents', '.content', '.detail-content', '.notice-content', 'main', '.main-content']:
                    content_div = await page.query_selector(selector)
                    if content_div:
                        html = await content_div.inner_html()
                        if html.strip():
                            content_html = html
                            logging.info(f"âœ… ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: selector='{selector}', ê¸¸ì´={len(html)}")
                            break
                if not content_html:
                    logging.warning("âš ï¸ ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: ëª¨ë“  ì…€ë ‰í„°ì—ì„œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
                # ë‹¤ìŒê¸€ ë§í¬ ì¶”ì¶œ
                next_link = await page.evaluate("""() => {
                    let nextLink = '';
                    const nextElement = document.querySelector('a[data-bno].next-area');
                    if (nextElement) {
                        const nextBno = nextElement.getAttribute('data-bno');
                        if (nextBno) {
                            const currentUrl = window.location.href;
                            const baseUrl = currentUrl.split('?')[0];
                            nextLink = `${baseUrl}?bno=${nextBno}`;
                        }
                    }
                    if (!nextLink) {
                        const allElements = document.querySelectorAll('*');
                        for (let elem of allElements) {
                            if (elem.textContent && elem.textContent.includes('ë‹¤ìŒê¸€')) {
                                const parent = elem.closest('a[data-bno]');
                                if (parent) {
                                    const nextBno = parent.getAttribute('data-bno');
                                    if (nextBno) {
                                        const currentUrl = window.location.href;
                                        const baseUrl = currentUrl.split('?')[0];
                                        nextLink = `${baseUrl}?bno=${nextBno}`;
                                        break;
                                    }
                                }
                            }
                        }
                    }
                    if (!nextLink) {
                        const nextLinks = document.querySelectorAll('a[href*="bno="]');
                        for (let link of nextLinks) {
                            if (link.textContent.includes('ë‹¤ìŒê¸€') || link.textContent.includes('ë‹¤ìŒ')) {
                                nextLink = link.href;
                                break;
                            }
                        }
                    }
                    return nextLink;
                }""")
                if next_link:
                    logging.info(f"ğŸ”— ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ë‹¤ìŒê¸€ ë§í¬ ë°œê²¬: {next_link}")
                else:
                    logging.info("ğŸ”— ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ë‹¤ìŒê¸€ ë§í¬ ì—†ìŒ")
                await browser.close()
                metadata = {
                    'title': title,
                    'rawDate': raw_date,
                    'nextLink': next_link,
                    'contentHtml': content_html
                }
            else:
                logging.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ìƒì„¸ í˜ì´ì§€ ì§„ì… ì‹¤íŒ¨: ì œëª©='{title}', ë‚ ì§œ='{raw_date}'")
                await browser.close()
                return {"error": "ì œëª© ë˜ëŠ” ë‚ ì§œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    except Exception as e:
        logging.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ìƒì„¸ í˜ì´ì§€ ì§„ì… ì‹¤íŒ¨: {str(e)}")
        return {"error": f"í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {str(e)}"}
    
    # 2. ì¶”ì¶œëœ ì»¨í…ì¸  HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
    if metadata['contentHtml']:
        content = md(metadata['contentHtml'])
        logging.info(f"âœ… ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ ì„±ê³µ: ê¸¸ì´={len(content)}")
    else:
        # fallbackìœ¼ë¡œ crawl4ai ì‹œë„
        logging.info("âš ï¸ ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ HTML ë‚´ìš©ì´ ì—†ì–´ crawl4ai fallback ì‹œë„")
        try:
            result = await fclient.scrape_single_url(url)
            if result.get("markdown"):
                content = result["markdown"]
                logging.info(f"âœ… ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ crawl4ai fallback ì„±ê³µ: ê¸¸ì´={len(content)}")
            else:
                content = "ì»¨í…ì¸  ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨"
                logging.error("âŒ ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ crawl4ai fallback ì‹¤íŒ¨: markdown ì—†ìŒ")
        except Exception as e:
            content = "ì»¨í…ì¸  ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨"
            logging.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ crawl4ai fallback ì‹¤íŒ¨: {str(e)}")
    
    # 3. ì¹´í…Œê³ ë¦¬ì™€ ë‚ ì§œ ë¶„ë¦¬ ì²˜ë¦¬ (ì •ê·œì‹ìœ¼ë¡œ robustí•˜ê²Œ)
    raw_date = metadata.get('rawDate', '')
    date_only_match = re.search(r'(\d{4}[.\-]\d{2}[.\-]\d{2})', raw_date)
    if date_only_match:
        actual_date = date_only_match.group(1)
        category = raw_date[:raw_date.find(actual_date)].strip() if raw_date.find(actual_date) > 0 else ""
        logging.info(f"âœ… ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ë‚ ì§œ íŒŒì‹± ì„±ê³µ: date='{actual_date}', category='{category}'")
    else:
        actual_date = ""
        category = ""
        logging.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: raw_date='{raw_date}'")
        return {"error": f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {raw_date}"}
    
    # ë‚ ì§œ cutoff ì²´í¬
    date_match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', actual_date)
    if date_match:
        year, month, day = map(int, date_match.groups())
        post_date = datetime(year, month, day)
        if post_date < cutoff_date:
            logging.info(f"â° ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ë‚ ì§œ cutoff ë„ë‹¬: {actual_date} < {cutoff_date.strftime('%Y.%m.%d')}")
            return {"date_cutoff_reached": True}
    else:
        pass
    
    # 4. ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸  í¬ë§·íŒ…
    formatted_content = format_content(content)
    date_display = f"{actual_date}" + (f" (ì¹´í…Œê³ ë¦¬: {category})" if category else "")
    markdown_content = create_markdown(metadata['title'], date_display, formatted_content)
    logging.info(f"âœ… ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ë§ˆí¬ë‹¤ìš´ ìƒì„± ì™„ë£Œ: title='{metadata['title']}', ê¸¸ì´={len(markdown_content)}")
    
    # ë‹¤ìŒê¸€ URL ì²˜ë¦¬
    next_url = None
    if metadata['nextLink'] and 'bno=' in metadata['nextLink']:
        next_url = metadata['nextLink']
    
    # ëª¨ë°”ì¼ URL ìƒì„± (inside.kt.com -> m.kt.com)
    mobile_url = url.replace('inside.kt.com', 'm.kt.com') if 'inside.kt.com' in url else None
    
    logging.info(f"ğŸ‰ ë„¤íŠ¸ì›Œí¬ ê³µì§€ì‚¬í•­ ìƒì„¸ ì²˜ë¦¬ ì™„ë£Œ: title='{metadata['title']}', date='{actual_date}'")
    # startdate/enddate (ê³µì§€: ê²Œì‹œì¼ë§Œ â†’ startdateì— ì €ì¥)
    startdate_hyphen = "0000-00-00"
    enddate_hyphen = "9999-99-99"
    try:
        dm = re.search(r"(\d{4})[.\-](\d{2})[.\-](\d{2})", actual_date)
        if dm:
            startdate_hyphen = f"{dm.group(1)}-{dm.group(2)}-{dm.group(3)}"
    except Exception:
        pass

    return {
        "url": url,
        "mobile_url": mobile_url,
        "murl": mobile_url or '',
        "title": metadata['title'],
        "category": category,
        "date": actual_date,
        "raw_date": metadata['rawDate'],
        "startdate": startdate_hyphen,
        "enddate": enddate_hyphen,
        "markdown": markdown_content,
        "html": metadata['contentHtml'] or content,
        "next_url": next_url,
        "special_processed": True,
        "playwright_processed": True
    }

register_page_handler(
    r'https?://inside\.kt\.com/html/notice/net_notice_list\.html',
    handle_network_notice_main
)

async def handle_safety_notice_main(url: str, fclient, menu=None) -> dict:
    logging.info(f"ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ ë©”ì¸ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, menu={menu}")
    """
    ì•ˆì „í•œ í†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ë©”ì¸ í˜ì´ì§€ ì²˜ë¦¬ í•¸ë“¤ëŸ¬ (input_urlsì˜ menu ì»¬ëŸ¼ ì‚¬ìš©)
    - data-bno ì†ì„±ì„ ê°€ì§„ ë§í¬ë“¤ ì¶”ì¶œ (ê¸°ì¡´ KT ê³µì§€ì‚¬í•­ê³¼ ë™ì¼ êµ¬ì¡°)
    - ê°€ì¥ ìµœì‹  ê²Œì‹œë¬¼ë¶€í„° 1ë…„ ì „ê¹Œì§€ ìˆœì°¨ ì²˜ë¦¬
    - ê° ê²Œì‹œë¬¼ì˜ menuëŠ” input_urlsì—ì„œ ìƒìœ„ì—ì„œ ë°›ì•„ì˜´
    - menus, datas ë¦¬ìŠ¤íŠ¸ë¡œ ëˆ„ì /ë°˜í™˜
    """
    from datetime import datetime, timedelta
    
    # 1ë…„ ì „ ë‚ ì§œ ê³„ì‚°
    cutoff_date = datetime.now() - timedelta(days=365)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        response = await page.goto(url, wait_until='domcontentloaded')
        first_notice_link = None
        
        # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
        status_code = response.status if response else None
        if status_code:
            if status_code >= 400:
                logging.error(f"âŒ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ ({url}): HTTP {status_code} ì˜¤ë¥˜")
            elif status_code >= 300:
                logging.warning(f"âš ï¸ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
            else:
                logging.info(f"âœ… ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ ({url}): HTTP {status_code} ì„±ê³µ")
        else:
            logging.debug(f"ğŸ” ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")
        for attempt in range(3):
            try:
                await page.wait_for_selector('a[data-bno]', timeout=10000)
            except Exception as e:
                logging.warning(f"'a[data-bno]' ìš”ì†Œ ëŒ€ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/3): {str(e)}")
            await page.wait_for_timeout(2000)
            first_notice_link = await page.evaluate("""() => {
                const firstElement = document.querySelector('a[data-bno]');
                if (firstElement) {
                    const bno = firstElement.getAttribute('data-bno');
                    // ì•ˆì „í•œ í†µì‹ ìƒí™œì€ safety_detail.htmlì„ ì‚¬ìš©í•  ê²ƒìœ¼ë¡œ ì¶”ì •
                    return `https://inside.kt.com/html/safety/notice_detail.html?bno=${bno}`;
                }
                return null;
            }""")
            if first_notice_link:
                break
            elif attempt < 2:
                pass
        await browser.close()
    
    if not first_notice_link:
        return {"error": "ì²« ë²ˆì§¸ ì•ˆì „í•œ í†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
    
    # ì²« ë²ˆì§¸ ê²Œì‹œë¬¼ë¶€í„° ìˆœì°¨ ì²˜ë¦¬ ì‹œì‘
    total_processed = 0
    current_url = first_notice_link
    menus = []
    datas = []
    logging.info(f"ì´ {total_processed}ê°œ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ê²Œì‹œë¬¼ ì²˜ë¦¬ ì‹œì‘")

    while current_url and total_processed < 1000:  # ì•ˆì „ì¥ì¹˜: ìµœëŒ€ 1000ê°œ
        try:
            logging.info(f"ğŸ”„ {total_processed + 1}ë²ˆì§¸ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì‹œì‘: url={current_url}")
            # ìƒì„¸ í•¸ë“¤ëŸ¬ í˜¸ì¶œ (menuëª…ì„ ìƒìœ„ì—ì„œ ì£¼ì…)
            result = await handle_safety_notice_detail(current_url, fclient)

            if "error" in result:
                logging.warning(f"âŒ {total_processed + 1}ë²ˆì§¸ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì‹¤íŒ¨: {result['error']}")
                break

            if result.get("date_cutoff_reached"):
                logging.info(f"â° ë‚ ì§œ cutoff ë„ë‹¬: {result.get('date', 'unknown')}")
                break

            # === ê²Œì‹œë¬¼ ì •ë³´ ì¶”ê°€ ===
            import re
            from datetime import datetime
            # ë‚ ì§œ íŒŒì‹± ë° yy-mm-dd ìƒì„±
            formatted_date = ''
            if result.get('date'):
                date_match = re.search(r'(\d{4})[.\-](\d{1,2})[.\-](\d{1,2})', result['date'])
                if date_match:
                    formatted_date = f"{date_match.group(1)[2:]}-{date_match.group(2).zfill(2)}-{date_match.group(3).zfill(2)}"
            # ì œëª© ì •ì œ
            title_clean = sanitize_filename(result.get('title', 'unknown'))
            # ë§ˆì§€ë§‰ í´ë”ëª…: (yy-mm-dd){title}
            last_folder = f"({formatted_date}){title_clean}" if formatted_date else title_clean

            # menus, datas ë¦¬ìŠ¤íŠ¸ì— ì •ë³´ ì¶”ê°€
            menus.append({'menu': f"{menu}^{last_folder}" if menu else last_folder, 'url': current_url, 'murl': result.get('murl')})
            datas.append(result)

            total_processed += 1
            logging.info(f"âœ… {total_processed}ë²ˆì§¸ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì™„ë£Œ: '{result.get('title', 'unknown')}' ({formatted_date})")
            current_url = result.get("next_url")

            if not current_url:
                logging.info("ğŸ”— ë‹¤ìŒ ê²Œì‹œë¬¼ ë§í¬ê°€ ì—†ì–´ ì²˜ë¦¬ ì¢…ë£Œ")
                break

        except Exception as e:
            logging.error(f"âŒ {total_processed + 1}ë²ˆì§¸ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            break

    return {
        "menus": menus,
        "datas": datas,
        "total_processed": total_processed,
        "status": "completed",
        "message": f"ì´ {total_processed}ê°œ ì•ˆì „í•œ í†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ì²˜ë¦¬ ì™„ë£Œ"
    }

register_page_handler(
    r'https?://inside\.kt\.com/html/safety/notice_list\.html',
    handle_safety_notice_main
)

async def handle_safety_notice_detail(url: str, fclient, cutoff_date=None) -> dict:
    logging.info(f"ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ ìƒì„¸ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, cutoff_date={cutoff_date}")
    """
    ì•ˆì „í•œ í†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ê°œë³„ ê²Œì‹œë¬¼ ì²˜ë¦¬ í•¸ë“¤ëŸ¬ (crawl4ai ì‚¬ìš©)
    - ì œëª©, ë‚ ì§œ(ì¹´í…Œê³ ë¦¬), ë‹¤ìŒê¸€ ë§í¬ë§Œ selectorë¡œ ì¶”ì¶œ
    - ì „ì²´ ì»¨í…ì¸ ëŠ” crawl4aië¡œ ì²˜ë¦¬
    """
    from datetime import datetime
    func_name = "handle_safety_notice_detail"
    import re
    
    if cutoff_date is None:
        from datetime import timedelta
        cutoff_date = datetime.now() - timedelta(days=365)
    
    # 1. ì œëª©, ë‚ ì§œ, ë‹¤ìŒê¸€ ë§í¬ë§Œ playwrightë¡œ ì¶”ì¶œ
    metadata = None
    try:
        logging.info(f"ğŸ”„ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ìƒì„¸ í˜ì´ì§€ ì§„ì…: url={url}")
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
            )
            page = await context.new_page()
            
            # í˜ì´ì§€ ë¡œë”© ì‹œë„ - ë” ì—¬ìœ ë¡œìš´ ì˜µì…˜
            response = await page.goto(url, wait_until='networkidle', timeout=60000)  # networkidleë¡œ ë³€ê²½, íƒ€ì„ì•„ì›ƒ 60ì´ˆ
            await page.wait_for_timeout(5000)  # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ì„ 5ì´ˆë¡œ ëŠ˜ë¦¼
            
            # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
            status_code = response.status if response else None
            if status_code:
                if status_code >= 400:
                    logging.error(f"âŒ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ ìƒì„¸ ({url}): HTTP {status_code} ì˜¤ë¥˜")
                elif status_code >= 300:
                    logging.warning(f"âš ï¸ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ ìƒì„¸ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
                else:
                    logging.info(f"âœ… ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ ìƒì„¸ ({url}): HTTP {status_code} ì„±ê³µ")
            else:
                logging.debug(f"ğŸ” ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ ìƒì„¸ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")
            
            # í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ì¶”ê°€ ëŒ€ê¸°
            try:
                await page.wait_for_load_state('domcontentloaded', timeout=30000)
                await page.wait_for_load_state('networkidle', timeout=30000)
            except Exception as e:
                logging.warning(f"ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ í˜ì´ì§€ ë¡œë“œ ìƒíƒœ ëŒ€ê¸° ì¤‘ íƒ€ì„ì•„ì›ƒ: {str(e)}")
            
            # ì œëª©, ë‚ ì§œ, ë‹¤ìŒê¸€ ë§í¬ ì¶”ì¶œ
            title = await page.evaluate("""() => {
                const t = document.querySelector('h1.title');
                return t ? t.textContent.trim() : '';
            }""")
            raw_date = await page.evaluate("""() => {
                const d = document.querySelector('.desc');
                return d ? d.textContent.trim() : '';
            }""")
            
            # ì„±ê³µ ì¡°ê±´ ì²´í¬: ì œëª©ê³¼ ë‚ ì§œê°€ ëª¨ë‘ ìˆìœ¼ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
            if title and raw_date:
                logging.info(f"âœ… ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ìƒì„¸ í˜ì´ì§€ ì§„ì… ì„±ê³µ: title='{title}', date='{raw_date}'")
                # ë³¸ë¬¸ ì—¬ëŸ¬ í›„ë³´ ì…€ë ‰í„° ìˆœì°¨ ì‹œë„
                content_html = ""
                for selector in ['.txt-content', '.contents', '.content', '.detail-content', '.notice-content', 'main', '.main-content']:
                    content_div = await page.query_selector(selector)
                    if content_div:
                        html = await content_div.inner_html()
                        if html.strip():
                            content_html = html
                            logging.info(f"âœ… ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: selector='{selector}', ê¸¸ì´={len(html)}")
                            break
                if not content_html:
                    logging.warning("âš ï¸ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: ëª¨ë“  ì…€ë ‰í„°ì—ì„œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
                # ë‹¤ìŒê¸€ ë§í¬ ì¶”ì¶œ
                next_link = await page.evaluate("""() => {
                    let nextLink = null;
                    const nextLinks = document.querySelectorAll('a[href*="bno="]');
                    for (let link of nextLinks) {
                        if (link.textContent.includes('ë‹¤ìŒê¸€') || link.textContent.includes('ë‹¤ìŒ')) {
                            nextLink = link.href;
                            break;
                        }
                    }
                    return nextLink;
                }""")
                if next_link:
                    logging.info(f"ğŸ”— ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ë‹¤ìŒê¸€ ë§í¬ ë°œê²¬: {next_link}")
                else:
                    logging.info("ğŸ”— ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ë‹¤ìŒê¸€ ë§í¬ ì—†ìŒ")
                await browser.close()
                metadata = {
                    'title': title,
                    'rawDate': raw_date,
                    'nextLink': next_link,
                    'contentHtml': content_html
                }
            else:
                logging.error(f"âŒ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ìƒì„¸ í˜ì´ì§€ ì§„ì… ì‹¤íŒ¨: ì œëª©='{title}', ë‚ ì§œ='{raw_date}'")
                await browser.close()
                return {"error": f"í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: ì œëª© ë˜ëŠ” ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"}

    except Exception as e:
        logging.error(f"âŒ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ìƒì„¸ í˜ì´ì§€ ì§„ì… ì‹¤íŒ¨: {str(e)}")
        return {"error": f"í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {str(e)}"}
    
    # 2. ì¶”ì¶œëœ ì»¨í…ì¸  HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
    if metadata['contentHtml']:
        content = md(metadata['contentHtml'])
        logging.info(f"âœ… ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ ì„±ê³µ: ê¸¸ì´={len(content)}")
    else:
        # fallbackìœ¼ë¡œ crawl4ai ì‹œë„
        logging.info("âš ï¸ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ HTML ë‚´ìš©ì´ ì—†ì–´ crawl4ai fallback ì‹œë„")
        try:
            result = await fclient.scrape_single_url(url)
            if result.get("markdown"):
                content = result["markdown"]
                logging.info(f"âœ… ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ crawl4ai fallback ì„±ê³µ: ê¸¸ì´={len(content)}")
            else:
                content = "ì»¨í…ì¸  ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨"
                logging.error("âŒ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ crawl4ai fallback ì‹¤íŒ¨: markdown ì—†ìŒ")
        except Exception as e:
            content = "ì»¨í…ì¸  ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨"
            logging.error(f"âŒ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ crawl4ai fallback ì‹¤íŒ¨: {str(e)}")
    
    # 3. ì¹´í…Œê³ ë¦¬ì™€ ë‚ ì§œ ë¶„ë¦¬ ì²˜ë¦¬ (ì •ê·œì‹ìœ¼ë¡œ robustí•˜ê²Œ)
    raw_date = metadata.get('rawDate', '')
    date_only_match = re.search(r'(\d{4}[.\-]\d{2}[.\-]\d{2})', raw_date)
    if date_only_match:
        actual_date = date_only_match.group(1)
        category = raw_date[:raw_date.find(actual_date)].strip() if raw_date.find(actual_date) > 0 else ""
        logging.info(f"âœ… ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ë‚ ì§œ íŒŒì‹± ì„±ê³µ: date='{actual_date}', category='{category}'")
    else:
        actual_date = ""
        category = ""
        logging.error(f"âŒ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: raw_date='{raw_date}'")
        return {"error": f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {raw_date}"}
    
    # ë‚ ì§œ cutoff ì²´í¬
    date_match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', actual_date)
    if date_match:
        year, month, day = map(int, date_match.groups())
        post_date = datetime(year, month, day)
        if post_date < cutoff_date:
            logging.info(f"â° ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ë‚ ì§œ cutoff ë„ë‹¬: {actual_date} < {cutoff_date.strftime('%Y.%m.%d')}")
            return {"date_cutoff_reached": True}
    else:
        pass
    
    # 4. ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸  í¬ë§·íŒ…
    formatted_content = format_content(content)
    date_display = f"{actual_date}" + (f" (ì¹´í…Œê³ ë¦¬: {category})" if category else "")
    markdown_content = create_markdown(metadata['title'], date_display, formatted_content)
    logging.info(f"âœ… ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ë§ˆí¬ë‹¤ìš´ ìƒì„± ì™„ë£Œ: title='{metadata['title']}', ê¸¸ì´={len(markdown_content)}")
    
    # ë‹¤ìŒê¸€ URL ì²˜ë¦¬
    next_url = None
    if metadata['nextLink'] and 'bno=' in metadata['nextLink']:
        next_url = metadata['nextLink']
    
    # ëª¨ë°”ì¼ URL ìƒì„± (inside.kt.com -> m.kt.com)
    mobile_url = url.replace('inside.kt.com', 'm.kt.com') if 'inside.kt.com' in url else None
    
    # startdate/enddate (ê³µì§€: ê²Œì‹œì¼ë§Œ â†’ startdateì— ì €ì¥)
    startdate_hyphen = "0000-00-00"
    enddate_hyphen = "9999-99-99"
    try:
        dm = re.search(r"(\d{4})[.\-](\d{2})[.\-](\d{2})", actual_date)
        if dm:
            startdate_hyphen = f"{dm.group(1)}-{dm.group(2)}-{dm.group(3)}"
    except Exception:
        pass

    logging.info(f"ğŸ‰ ì•ˆì „í•œí†µì‹ ìƒí™œ ê³µì§€ì‚¬í•­ ìƒì„¸ ì²˜ë¦¬ ì™„ë£Œ: title='{metadata['title']}', date='{actual_date}'")
    return {
        "url": url,
        "mobile_url": mobile_url,
        "murl": mobile_url or '',
        "title": metadata['title'],
        "category": category,
        "date": actual_date,
        "raw_date": metadata['rawDate'],
        "startdate": startdate_hyphen,
        "enddate": enddate_hyphen,
        "markdown": markdown_content,
        "html": metadata['contentHtml'] or content,
        "next_url": next_url,
        "special_processed": True,
        "playwright_processed": True
    }
    
async def handle_product_detail(url: str, fclient=None, menu=None) -> dict:
    logging.info(f"ìƒí’ˆ ìƒì„¸ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, menu={menu}")
    """
    ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ í•¸ë“¤ëŸ¬
    - title íƒì§€ -> í´ë¦­ -> ì¶”ì¶œì´ ì£¼ìš” ëª©ì 
    - URLì—ì„œ ItemCode ì¶”ì¶œí•˜ì—¬ handler ì ê²©ì„± íŒë‹¨
    - menu_nameì€ input_urlsì˜ menu ì»¬ëŸ¼ ê°’ ì‚¬ìš©
    - ì¼ë°˜ ìƒí’ˆê³¼ soho ìƒí’ˆ ëª¨ë‘ ë™ì¼í•œ êµ¬ì¡°ë¡œ ì²˜ë¦¬
    - íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í¬í•¨
    """
    logging.info(f"ìƒí’ˆ ìƒì„¸ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, menu={menu}")
    
    # URLì—ì„œ ItemCode ì¶”ì¶œ (handler ì ê²©ì„± íŒë‹¨ìš©)
    m = re.search(r'ItemCode=(\d+)', url)
    if not m:
        return None  # ì¼ë°˜ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ fallback
    
    item_code = m.group(1)

    # ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ì„¤ì •
    max_retries = 3
    base_timeout = 60000  # 60ì´ˆ ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ
    
    for attempt in range(max_retries):
        try:
            logging.info(f"ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ ì§„ì… ì‹œë„ {attempt + 1}/{max_retries}: url={url}, item_code={item_code}")
            
            # ì‹œë„ë³„ë¡œ ë‹¤ë¥¸ ë¡œë”© ì „ëµ ì ìš©
            if attempt == 0:
                wait_until = "domcontentloaded"
                timeout = 30000
                extra_wait = 3000
            elif attempt == 1:
                wait_until = "load"
                timeout = 45000
                extra_wait = 5000
            else:
                wait_until = "networkidle"
                timeout = base_timeout
                extra_wait = 7000
            
            # ì™„ì „íˆ ìƒˆë¡œìš´ ë¸Œë¼ìš°ì € ì„¸ì…˜ìœ¼ë¡œ ì‹œì‘
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                
                # í˜ì´ì§€ ë¡œë“œ
                response = await page.goto(url, wait_until=wait_until, timeout=timeout)
                await page.wait_for_timeout(extra_wait)
                
                # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
                status_code = response.status if response else None
                if status_code:
                    if status_code >= 400:
                        logging.error(f"âŒ ìƒí’ˆ ìƒì„¸ ({url}): HTTP {status_code} ì˜¤ë¥˜")
                    elif status_code >= 300:
                        logging.warning(f"âš ï¸ ìƒí’ˆ ìƒì„¸ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
                    else:
                        logging.info(f"âœ… ìƒí’ˆ ìƒì„¸ ({url}): HTTP {status_code} ì„±ê³µ")
                else:
                    logging.debug(f"ğŸ” ìƒí’ˆ ìƒì„¸ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")
                
                # ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ ë¡œë“œ ëŒ€ê¸°
                try:
                    await page.wait_for_selector("#cfmClContents", timeout=10000)
                    logging.info("ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ ë¡œë“œ ì™„ë£Œ")
                except:
                    logging.warning("ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ ë¡œë“œ ì‹¤íŒ¨, ê³„ì† ì§„í–‰")
                
                # í˜ì´ì§€ ì œëª© ì¶”ì¶œ
                title = await page.evaluate("""
                    () => {
                        const titleEl = document.querySelector('h1') || document.querySelector('.product-title') || document.querySelector('h2');
                        return titleEl ? titleEl.textContent.trim() : 'No title found';
                    }
                """)
                
                logging.info(f"ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ ì œëª© ì¶”ì¶œ: '{title}'")
                
                # ì•„ì½”ë””ì–¸ íŠ¸ë¦¬ê±° íƒì§€
                accordion_triggers = await page.evaluate("""
                    () => {
                        const triggers = [];
                        for (let i = 1; i <= 10; i++) {
                            const trigger = document.querySelector(`#title${i}`);
                            if (trigger) {
                                triggers.push({
                                    id: `title${i}`,
                                    text: trigger.textContent.trim(),
                                    visible: trigger.offsetParent !== null
                                });
                            }
                        }
                        return triggers;
                    }
                """)
                
                logging.info(f"ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ ì•„ì½”ë””ì–¸ íŠ¸ë¦¬ê±° {len(accordion_triggers)}ê°œ ë°œê²¬")
                
                # ëª¨ë“  ì•„ì½”ë””ì–¸ íŠ¸ë¦¬ê±° í´ë¦­í•˜ì—¬ ìˆ¨ê²¨ì§„ ë‚´ìš© ëª¨ë‘ í‘œì‹œ
                if accordion_triggers:
                    for i, trigger in enumerate(accordion_triggers, 1):
                        try:
                            trigger_id = trigger['id']
                            trigger_text = trigger['text']
                            
                            logging.info(f"ìƒí’ˆ ìƒì„¸ ì•„ì½”ë””ì–¸ í´ë¦­ {i}/{len(accordion_triggers)}: {trigger_id} - '{trigger_text}'")
                            await page.click(f"#{trigger_id}", timeout=5000)
                            await page.wait_for_timeout(1000)
                            logging.info(f"ìƒí’ˆ ìƒì„¸ ì•„ì½”ë””ì–¸ í´ë¦­ ì„±ê³µ: {trigger_id}")
                            
                        except Exception as e:
                            logging.warning(f"ìƒí’ˆ ìƒì„¸ ì•„ì½”ë””ì–¸ í´ë¦­ ì‹¤íŒ¨ {i}/{len(accordion_triggers)}: {trigger_id}, ì—ëŸ¬: {str(e)}")
                            continue
                
                # ì¶”ì²œ ì»¨í…ì¸  ì¶”ì¶œ (recommendations)
                combined_html = ""
                markdown_text = ""
                additional_details = []  # N-pdt-compare-column ìì„¸íˆ ë³´ê¸°ë¡œ ì¶”ì¶œëœ í•˜ìœ„ ìƒí’ˆë“¤
                
                try:
                    logging.info(f"ìƒí’ˆ ìƒì„¸ recommendations ì¶”ì¶œ ì‹œì‘: url={url}")
                    
                    # ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„
                    await page.wait_for_timeout(3000)
                    
                    # JSì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  Pythonì—ì„œ ì •ì œ
                    raw_reco = await page.evaluate("""() => {
                        const abs = (u) => {
                            try { const a = document.createElement('a'); a.href = u; return a.href; } catch(e){ return u; }
                        };
                        // top
                        const top = Array.from(document.querySelectorAll('ul.three-list li a')).map(a => ({
                            title: (a.textContent||'').trim(),
                            url: abs(a.getAttribute('href')||a.href||'')
                        })).filter(x => x.title && x.url);

                        // bundle_option
                        const bundle = [];
                        ['#trigger1-1-1', '#trigger1-1-2'].forEach(sel => {
                            const root = document.querySelector(sel);
                            if (!root) return;
                            root.querySelectorAll('.bxslider li a').forEach(a => {
                                const title = ((a.querySelector('p')?.textContent) || a.textContent || '').trim();
                                const url = abs(a.getAttribute('href')||a.href||'');
                                const main = (a.querySelector('.recommend-main-info')?.textContent||'').trim();
                                const sub = (a.querySelector('.recommend-sub-info')?.textContent||'').trim();
                                const desc = [main, sub].filter(Boolean).join(' ');
                                if (title && url) bundle.push({ title, url, desc });
                            });
                        });

                        // plan_variant (ê³µìœ  ì•„ì´ì½˜ ì œì™¸, javascript ì œì™¸)
                        const planVariant = Array.from(document.querySelectorAll('.N-head-btn-area a'))
                            .filter(a => !a.classList.contains('icon'))
                            .map(a => ({
                                title: (a.textContent||'').trim(),
                                url: abs(a.getAttribute('href')||a.href||'')
                            }))
                            .filter(x => x.title && x.url && !x.url.startsWith('javascript:'));

                        // other_plan / extra_service
                        const otherPlan = [];
                        const extraService = [];
                        Array.from(document.querySelectorAll('ul.N-compare-suggest-list li a')).forEach(a => {
                            const title = ((a.querySelector('strong.tit')?.textContent) || a.textContent || '').trim();
                            const url = abs(a.getAttribute('href')||a.href||'');
                            const onclick = (a.getAttribute('onclick')||'');
                            if (title && url) {
                                if (onclick.includes('ì¶”ì²œë¶€ê°€ì„œë¹„ìŠ¤')) extraService.push({ title, url });
                                else otherPlan.push({ title, url });
                            }
                        });

                        return { top, bundle, planVariant, otherPlan, extraService };
                    }""")
                            
                    # ë°ì´í„° ê²€ì¦ ë° ë¡œê¹…
                    if raw_reco and any([
                        len(raw_reco.get('top', [])) > 0,
                        len(raw_reco.get('bundle', [])) > 0,
                        len(raw_reco.get('planVariant', [])) > 0,
                        len(raw_reco.get('otherPlan', [])) > 0,
                        len(raw_reco.get('extraService', [])) > 0
                    ]):
                        logging.info(f"âœ… Recommendations ì¶”ì¶œ ì„±ê³µ: url={url}, top={len(raw_reco.get('top', []))}, bundle={len(raw_reco.get('bundle', []))}, planVariant={len(raw_reco.get('planVariant', []))}, otherPlan={len(raw_reco.get('otherPlan', []))}, extraService={len(raw_reco.get('extraService', []))}")
                    else:
                        # ì‹¤íŒ¨ ì›ì¸ ë¶„ì„ì„ ìœ„í•œ ë””ë²„ê¹… ì •ë³´ ìˆ˜ì§‘
                        debug_info = await page.evaluate("""() => {
                            const debug = {};
                            
                            // ê° ì„ íƒìë³„ ìš”ì†Œ ê°œìˆ˜ í™•ì¸
                            debug.top_count = document.querySelectorAll('ul.three-list li a').length;
                            debug.bundle_trigger1 = document.querySelector('#trigger1-1-1') ? document.querySelector('#trigger1-1-1').querySelectorAll('.bxslider li a').length : 0;
                            debug.bundle_trigger2 = document.querySelector('#trigger1-1-2') ? document.querySelector('#trigger1-1-2').querySelectorAll('.bxslider li a').length : 0;
                            debug.plan_variant_count = document.querySelectorAll('.N-head-btn-area a').length;
                            debug.plan_variant_icon_count = document.querySelectorAll('.N-head-btn-area a.icon').length;
                            debug.compare_list_count = document.querySelectorAll('ul.N-compare-suggest-list li a').length;
                            
                            // ì‹¤ì œ HTML êµ¬ì¡° í™•ì¸ (ì²« ë²ˆì§¸ ìš”ì†Œë§Œ)
                            debug.top_sample = document.querySelector('ul.three-list li a') ? document.querySelector('ul.three-list li a').outerHTML.substring(0, 200) : 'ì—†ìŒ';
                            debug.bundle_sample = document.querySelector('#trigger1-1-1 .bxslider li a') ? document.querySelector('#trigger1-1-1 .bxslider li a').outerHTML.substring(0, 200) : 'ì—†ìŒ';
                            
                            return debug;
                        }""")
                        
                        logging.warning(f"âš ï¸ Recommendations ì¶”ì¶œ ì‹¤íŒ¨: url={url}")
                        logging.warning(f"   - top: {debug_info.get('top_count', 0)}ê°œ, bundle: {debug_info.get('bundle_trigger1', 0)}+{debug_info.get('bundle_trigger2', 0)}ê°œ")
                        logging.warning(f"   - plan_variant: {debug_info.get('plan_variant_count', 0)}ê°œ (icon ì œì™¸: {debug_info.get('plan_variant_count', 0) - debug_info.get('plan_variant_icon_count', 0)}ê°œ)")
                        logging.warning(f"   - compare_list: {debug_info.get('compare_list_count', 0)}ê°œ")
                        logging.warning(f"   - top ìƒ˜í”Œ: {debug_info.get('top_sample', 'N/A')}")
                        logging.warning(f"   - bundle ìƒ˜í”Œ: {debug_info.get('bundle_sample', 'N/A')}")
                        
                        raw_reco = {'top': [], 'bundle': [], 'planVariant': [], 'otherPlan': [], 'extraService': []}

                    # Python ì¸¡ ì •ì œ ë° í¬ë§· í†µì¼
                    def to_abs(u: str) -> str:
                        if not u:
                            return ''
                        if u.startswith('http'):
                            return u
                        if u.startswith('/'):
                            return f"https://product.kt.com{u}"
                        return u

                    def to_murl(u: str) -> str:
                        if not u or not u.startswith('http'):
                            return ''
                        m = u.replace('https://product.kt.com', 'https://m.product.kt.com')
                        # wDic ê²½ë¡œë¥¼ mDicë¡œ ë³€í™˜
                        m = m.replace('/wDic/', '/mDic/')
                        return m

                    recommendations_list = []

                    # top
                    top_count = 0
                    top_raw = raw_reco.get('top', [])
                    for item in top_raw[:10]:
                        url_abs = to_abs(item.get('url', ''))
                        if url_abs:
                            recommendations_list.append({
                                'kind': 'top',
                                'name': item.get('title', ''),
                                'desc': '',
                                'url': url_abs,
                                'murl': to_murl(url_abs)
                            })
                            top_count += 1

                    # bundle_option (ì¤‘ë³µ ì œê±° by url)
                    seen = set()
                    bundle_count = 0
                    bundle_raw = raw_reco.get('bundle', [])
                    for item in bundle_raw[:20]:
                        url_abs = to_abs(item.get('url', ''))
                        if not url_abs or url_abs in seen:
                            continue
                        seen.add(url_abs)
                        desc = item.get('desc') or ''
                        recommendations_list.append({
                            'kind': 'bundle_option',
                            'name': item.get('title', ''),
                            'desc': desc,
                            'url': url_abs,
                            'murl': to_murl(url_abs)
                        })
                        bundle_count += 1

                    # plan_variant (ê³µìœ  ë§í¬ ì œê±°ë¨)
                    plan_variant_count = 0
                    plan_variant_raw = raw_reco.get('planVariant', [])
                    for item in plan_variant_raw[:10]:
                        url_abs = to_abs(item.get('url', ''))
                        if url_abs and not url_abs.startswith('javascript:'):
                            recommendations_list.append({
                                'kind': 'plan_variant',
                                'name': item.get('title', ''),
                                'desc': '',
                                'url': url_abs,
                                'murl': to_murl(url_abs)
                            })
                            plan_variant_count += 1

                    # other_plan
                    other_plan_count = 0
                    other_plan_raw = raw_reco.get('otherPlan', [])
                    for item in other_plan_raw[:10]:
                        url_abs = to_abs(item.get('url', ''))
                        if url_abs:
                            recommendations_list.append({
                                'kind': 'other_plan',
                                'name': item.get('title', ''),
                                'desc': '',
                                'url': url_abs,
                                'murl': to_murl(url_abs)
                            })
                            other_plan_count += 1

                    # extra_service
                    extra_service_count = 0
                    extra_service_raw = raw_reco.get('extraService', [])
                    for item in extra_service_raw[:10]:
                        url_abs = to_abs(item.get('url', ''))
                        if url_abs:
                            recommendations_list.append({
                                'kind': 'extra_service',
                                'name': item.get('title', ''),
                                'desc': '',
                                'url': url_abs,
                                'murl': to_murl(url_abs)
                            })
                            extra_service_count += 1

                    recommendations = recommendations_list
                    logging.info(f"ìƒí’ˆ ìƒì„¸ ì´ recommendations: {len(recommendations)}ê°œ (ì²˜ë¦¬ë¨)")
                    
                except Exception as e:
                    logging.error(f"ìƒí’ˆ ìƒì„¸ recommendations ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    recommendations = []

                # N-pdt-compare-column ë‚´ "ìì„¸íˆ ë³´ê¸°" ë§í¬ ì¶”ì¶œ ë° ì²˜ë¦¬
                try:
                    logging.info(f"ìƒí’ˆ ìƒì„¸ N-pdt-compare-column ìì„¸íˆ ë³´ê¸° ì¶”ì¶œ ì‹œì‘: url={url}")
                    
                    # N-pdt-compare-columnì—ì„œ "ìì„¸íˆ ë³´ê¸°" ë§í¬ ì¶”ì¶œ
                    detail_links = await page.evaluate("""() => {
                        const abs = (u) => {
                            try { const a = document.createElement('a'); a.href = u; return a.href; } catch(e){ return u; }
                        };
                        
                        const results = [];
                        const columns = document.querySelectorAll('.N-pdt-compare-column');
                        
                        columns.forEach(col => {
                            // btn-reduced ë§í¬ ì°¾ê¸°
                            const link = col.querySelector('a.btn-reduced');
                            if (!link) return;
                            
                            // ë§í¬ í…ìŠ¤íŠ¸ê°€ "ìì„¸íˆ ë³´ê¸°"ì¸ì§€ í™•ì¸
                            const linkText = (link.textContent || '').trim();
                            if (linkText !== 'ìì„¸íˆ ë³´ê¸°') return;
                            
                            // ìƒí’ˆëª… ì¶”ì¶œ (strong.name)
                            const nameEl = col.querySelector('strong.name');
                            if (!nameEl) return;
                            
                            const name = (nameEl.textContent || '').trim();
                            const href = abs(link.getAttribute('href') || link.href || '');
                            
                            if (name && href && href.startsWith('http')) {
                                results.push({ name, href });
                            }
                        });
                        
                        return results;
                    }""")
                    
                    if detail_links and len(detail_links) > 0:
                        logging.info(f"âœ… N-pdt-compare-column ìì„¸íˆ ë³´ê¸° ë§í¬ {len(detail_links)}ê°œ ë°œê²¬: url={url}")
                        
                        # ì¶”ì¶œëœ ë§í¬ë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ handle_product_detailì— ì „ë‹¬
                        for link_info in detail_links:
                            try:
                                # ì´ë¦„ ì •ì œ: ì¤„ë°”ê¿ˆ, íŠ¹ìˆ˜ë¬¸ì ì œê±°
                                clean_name = link_info['name']
                                # ì¤„ë°”ê¿ˆ ì œê±°
                                clean_name = re.sub(r'[\r\n]+', ' ', clean_name)
                                # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
                                clean_name = re.sub(r'\s+', ' ', clean_name)
                                # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, ìŠ¬ë˜ì‹œë§Œ í—ˆìš©)
                                clean_name = re.sub(r'[^\w\sã„±-ã…ã…-ã…£ê°€-í£/\-\(\)]', '', clean_name)
                                clean_name = clean_name.strip()
                                
                                detail_url = link_info['href']
                                
                                logging.info(f"ìƒí’ˆ ìƒì„¸ ì¬ê·€ í˜¸ì¶œ: name='{clean_name}', url={detail_url}")
                                
                                # ì¬ê·€ì ìœ¼ë¡œ handle_product_detail í˜¸ì¶œ
                                sub_result = await handle_product_detail(detail_url, fclient=fclient, menu=menu)
                                
                                if sub_result:
                                    # ì›ë˜ ì´ë¦„ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ì¶”ê°€
                                    sub_result['parent_product_name'] = clean_name
                                    sub_result['parent_url'] = url
                                    additional_details.append(sub_result)
                                    logging.info(f"âœ… ìƒí’ˆ ìƒì„¸ ì¬ê·€ í˜¸ì¶œ ì„±ê³µ: '{clean_name}'")
                                else:
                                    logging.warning(f"âš ï¸ ìƒí’ˆ ìƒì„¸ ì¬ê·€ í˜¸ì¶œ ì‹¤íŒ¨: '{clean_name}'")
                                    
                            except Exception as e:
                                logging.error(f"âŒ ìƒí’ˆ ìƒì„¸ ì¬ê·€ í˜¸ì¶œ ì˜¤ë¥˜: name='{link_info.get('name', 'N/A')}', error={str(e)}")
                                continue
                        
                        # ì¶”ê°€ëœ ìƒì„¸ ì •ë³´ê°€ ìˆìœ¼ë©´ ë¡œê¹…
                        if additional_details:
                            logging.info(f"âœ… ìƒí’ˆ ìƒì„¸ ì¬ê·€ í˜¸ì¶œ ì™„ë£Œ: {len(additional_details)}ê°œ ì²˜ë¦¬ë¨")
                            # ì¶”ê°€ëœ ì •ë³´ë¥¼ ê²°ê³¼ì— í¬í•¨ (ë‚˜ì¤‘ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡)
                            # ì´ ì •ë³´ëŠ” ë°˜í™˜ê°’ì˜ ì¼ë¶€ë¡œ ì €ì¥ë©ë‹ˆë‹¤
                        
                    else:
                        logging.debug(f"N-pdt-compare-column ìì„¸íˆ ë³´ê¸° ë§í¬ ì—†ìŒ: url={url}")
                    
                except Exception as e:
                    logging.error(f"ìƒí’ˆ ìƒì„¸ N-pdt-compare-column ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

                # ëª¨ë“  ì•„ì½”ë””ì–¸ í´ë¦­ í›„ ì „ì²´ cfmClContents ë‚´ìš© ìˆ˜ì§‘
                try:
                    logging.info("ìƒí’ˆ ìƒì„¸ ì½˜í…ì¸  ìˆ˜ì§‘ ì‹œì‘")
                    combined_html = await page.evaluate("""
                        () => {
                            const mainContent = document.querySelector('#cfmClContents');
                            if (!mainContent) return '';
                            
                            // ì œì™¸í•  ìš”ì†Œë“¤ ì œê±°
                            const excludeSelectors = [
                                '#cfmClHeader', '#cfmClFooter', '#cfmClSkip', 
                                'form', '.header', '.footer', '.nav', ".swiper-controls-wrapper",".opage-hashtag-arrow", ".swiper-button-next", ".swiper-button-prev",
                                ".icon.kakao", ".icon.facebook", ".icon.twitter", ".icon.youtube",
                                ".location", ".sns-area", ".opener", "a[onclick*='KT_trackClicks']", '.together-recommend-area',
                                ".N-compare-suggest-list", ".top-three-box", ".tabs",
                            ];
                            
                            // ë³µì‚¬ë³¸ ìƒì„±í•˜ì—¬ ì›ë³¸ ë³€ê²½ ë°©ì§€
                            const contentClone = mainContent.cloneNode(true);
                            
                            // ì œì™¸ ìš”ì†Œë“¤ ì œê±°
                            excludeSelectors.forEach(selector => {
                                const elements = contentClone.querySelectorAll(selector);
                                elements.forEach(el => el.remove());
                            });
                            
                            return contentClone.outerHTML;
                        }
                    """)
                    
                    if combined_html:
                        markdown_text = md(combined_html)
                        logging.info(f"ìƒí’ˆ ìƒì„¸ ì½˜í…ì¸  ìˆ˜ì§‘ ì„±ê³µ: HTML ê¸¸ì´={len(combined_html)}, ë§ˆí¬ë‹¤ìš´ ê¸¸ì´={len(markdown_text)}")
                    else:
                        # fallback
                        logging.warning("ìƒí’ˆ ìƒì„¸ ë©”ì¸ ì½˜í…ì¸  ì—†ìŒ, body ì „ì²´ë¡œ fallback")
                        combined_html = await page.eval_on_selector("body", "el => el.outerHTML")
                        markdown_text = md(combined_html)
                        logging.info(f"ìƒí’ˆ ìƒì„¸ fallback ì½˜í…ì¸  ìˆ˜ì§‘ ì™„ë£Œ: HTML ê¸¸ì´={len(combined_html)}, ë§ˆí¬ë‹¤ìš´ ê¸¸ì´={len(markdown_text)}")
                        
                except Exception as e:
                    logging.error(f"ìƒí’ˆ ìƒì„¸ ì½˜í…ì¸  ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
                    combined_html = ""
                    markdown_text = "ì½˜í…ì¸  ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

                # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ê²½ìš° ê²°ê³¼ ë°˜í™˜
                logging.info(f"ìƒí’ˆ ìƒì„¸ ì²˜ë¦¬ ì™„ë£Œ: title='{title}', accordion_count={len(accordion_triggers)}, content_length={len(combined_html) if combined_html else 0}, additional_details={len(additional_details)}")
                return {
                    "url": url,
                    "murl": to_murl(url),
                    "title": title,
                    "markdown": markdown_text,
                    "html": combined_html or "",
                    "item_code": item_code,
                    "accordion_count": len(accordion_triggers),
                    "content_length": len(combined_html) if combined_html else 0,
                    "recommendations": recommendations or [],
                    "additional_details": additional_details or [],  # N-pdt-compare-column ìì„¸íˆ ë³´ê¸°ë¡œ ì¶”ì¶œëœ í•˜ìœ„ ìƒí’ˆë“¤
                    "special_processed": True,
                    "playwright_processed": True
                }
                
        except Exception as e:
            if attempt < max_retries - 1:
                logging.warning(f"ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {str(e)} - ì¬ì‹œë„ ì¤‘...")
                await asyncio.sleep(5)  # ì¬ì‹œë„ ì „ 5ì´ˆ ëŒ€ê¸°
                continue
            else:
                logging.error(f"ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ìµœì¢… ì‹¤íŒ¨: {str(e)}")
                return None  # ì¼ë°˜ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ fallback

register_page_handler(
    r'https?://product\.kt\.com/wDic/(soho/)?productDetail\.do\?ItemCode=.*',
    handle_product_detail
)


async def handle_wdic_mobile_list(url: str, fclient, menu=None) -> dict:
    """
    KT ìƒí’ˆì‚¬ì „(wDic) ì¹´í…Œê³ ë¦¬ ëª©ë¡ í•¸ë“¤ëŸ¬ (ì¼ë°˜/ì†Œìƒê³µì¸ ëª¨ë‘ ì§€ì›)
    ì§€ì› í˜ì´ì§€:
    - ì¼ë°˜ ìƒí’ˆ: product.kt.com/wDic/index.do?CateCode=6002/6003 ë“±
    - ì†Œìƒê³µì¸: product.kt.com/wDic/soho/index.do?CateCode=7002 ë“±
    
    ìš”êµ¬ì‚¬í•­:
    - íƒ­ ìˆœíšŒ: ul.ui-tab-list ë˜ëŠ” ul.red-select ('ì¶”ì²œ' íƒ­ ì œì™¸)
    - .type-sub-itemì´ ìˆìœ¼ë©´ ëª¨ë“  ì„œë¸Œ í•„í„° ìˆœíšŒ
    - 'ë”ë³´ê¸°(.btn-more)'ê°€ ì‚¬ë¼ì§ˆ ë•Œê¹Œì§€ í´ë¦­í•´ ì „ì²´ í¼ì¹¨
    - ìƒì„¸ URLì€ '.btns a[href*="productDetail"]'ë§Œ ì‚¬ìš© (ItemCode ê¸°ë°˜)
    - ëª¨ë“  íƒ­ ìˆœíšŒ í›„ ItemCode ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    - ë©”ë‰´ êµ¬ì¡°: {menu}^{íƒ­ëª…}^{ì„œë¸Œí•„í„°ëª…}^{ì•„ì´í…œ ì œëª©}
    - ìˆ˜ì§‘ëœ ìƒì„¸ URLë“¤ì„ handle_product_detailë¡œ ì „ë‹¬í•˜ì—¬ datas êµ¬ì„±
    """
    import logging
    from urllib.parse import urljoin
    from playwright.async_api import async_playwright
    from markdownify import markdownify as md

    base_host = 'https://product.kt.com'

    def _to_murl(u: str) -> str:
        if not u or not u.startswith('http'):
            return ''
        m = u.replace('https://product.kt.com', 'https://m.product.kt.com')
        m = m.replace('/wDic/', '/mDic/')
        return m

    async def _capture_list_snapshot(page, base_menu: str = "", tab_text: str = "", sub_filter_text: str = ""):
        """í˜„ì¬ ëª©ë¡ í™”ë©´ì˜ ë³¸ë¬¸(html, markdown)ì„ ìº¡ì²˜í•˜ì—¬ datas/menusì— ì¶”ê°€"""
        try:
            # ë³¸ë¬¸ ì˜ì—­ ìš°ì„ , ì—†ìœ¼ë©´ body ì „ì²´
            html = await page.evaluate("""
                () => {
                    const root = document.querySelector('#cfmClContents') || document.body;
                    const clone = root.cloneNode(true);
                    // ë¶ˆí•„ìš” ìš”ì†Œ ì œê±° (í—¤ë”/í‘¸í„°/ìŠ¤í‚µ, ìœ„ì¹˜, SNS, íŒŒì¸ë“œì„¼í„° ë“±)
                    const removeSelectors = [
                        '#cfmClHeader', '#cfmClFooter', '#cfmClSkip',
                        '.location', '.sns-area', '.find-center'
                    ];
                    removeSelectors.forEach(sel => {
                        clone.querySelectorAll(sel).forEach(el => el.remove());
                    });
                    return clone.outerHTML;
                }
            """)
            markdown_text = md(html) if html else ""
            # ë©”ë‰´ëª… êµ¬ì„±: menu ^ íƒ­ ^ ì„œë¸Œí•„í„°
            final_menu = (base_menu or "").strip()
            if tab_text:
                final_menu = f"{final_menu}^{tab_text}" if final_menu else tab_text
            if sub_filter_text:
                final_menu = f"{final_menu}^{sub_filter_text}" if final_menu else sub_filter_text
            # ëª©ë¡ ì ‘ë¯¸ì‚¬ ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            # ê²°ê³¼ ì¶”ê°€
            menus.append({ 'menu': final_menu, 'url': page.url, 'murl': _to_murl(page.url) })
            datas.append({
                "url": page.url,
                "murl": _to_murl(page.url),
                "title": final_menu,
                "markdown": markdown_text,
                "html": html or "",
                "special_processed": True,
                "playwright_processed": True,
                "is_list_snapshot": True
            })
            logging.info(f"ëª©ë¡ ìŠ¤ëƒ…ìƒ· ìº¡ì²˜ ì™„ë£Œ: menu='{final_menu}', html_len={len(html) if html else 0}")
        except Exception as e:
            logging.warning(f"ëª©ë¡ ìŠ¤ëƒ…ìƒ· ìº¡ì²˜ ì‹¤íŒ¨: {str(e)}")

    async def _click_more_until_exhausted(page) -> int:
        """MCP ê²€ì¦ ë¡œì§ê³¼ ì™„ì „ ë™ì¼: JavaScriptë¡œ ì§ì ‘ í´ë¦­"""
        clicks = 0
        guard = 0
        while guard < 50:
            guard += 1
            try:
                # í´ë¦­ ì „ li ê°œìˆ˜
                before = await page.evaluate("document.querySelectorAll('.plan-list-area .plan-list li').length")
                
                # MCPì™€ ë™ì¼: JavaScriptë¡œ ë²„íŠ¼ ì²´í¬ ë° ì§ì ‘ í´ë¦­
                clicked = await page.evaluate(r"""
                    () => {
                        const btn = document.querySelector('.btn-more');
                        if (!btn) return false;
                        const style = btn.getAttribute('style') || '';
                        const css = getComputedStyle(btn);
                        const visible = btn.offsetParent !== null && css.display !== 'none' && css.visibility !== 'hidden' && !/display:\s*none/i.test(style);
                        if (!visible) return false;
                        btn.click();
                        return true;
                    }
                """)
                
                if not clicked:
                    break
                
                clicks += 1
                await page.wait_for_timeout(1200)

                # í´ë¦­ í›„ li ê°œìˆ˜ í™•ì¸
                after = await page.evaluate("document.querySelectorAll('.plan-list-area .plan-list li').length")

                # ì¦ê°€ ì—†ìœ¼ë©´ ì¶”ê°€ ëŒ€ê¸° í›„ ì¬í™•ì¸ (MCPì™€ ë™ì¼)
                if after <= before:
                    await page.wait_for_timeout(1500)
                    after = await page.evaluate("document.querySelectorAll('.plan-list-area .plan-list li').length")

                # ì—¬ì „íˆ ì¦ê°€ ì—†ê³  ë²„íŠ¼ì´ ì•ˆ ë³´ì´ë©´ ì¢…ë£Œ
                if after <= before:
                    btn_check = await page.evaluate(r"""
                        () => {
                            const b = document.querySelector('.btn-more');
                            if (!b) return false;
                            const s = b.getAttribute('style')||'';
                            const c = getComputedStyle(b);
                            return b.offsetParent !== null && c.display !== 'none' && c.visibility !== 'hidden' && !/display:\s*none/i.test(s);
                        }
                    """)
                    if not btn_check:
                        break
            except Exception as e:
                logging.warning(f"ë”ë³´ê¸° í´ë¦­ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                break
        return clicks

    async def _ensure_filter_all(page):
        try:
            changed = await page.evaluate(r"""
                () => {
                    function isVisible(el){
                        if(!el) return false;
                        const style = getComputedStyle(el);
                        return el.offsetParent !== null && style.display !== 'none' && style.visibility !== 'hidden';
                    }
                    const cands = Array.from(document.querySelectorAll('a, button, label'));
                    for(const el of cands){
                        const txt = (el.textContent||'').replace(/\s+/g,'').trim();
                        if (txt.includes('ì „ì²´') && isVisible(el)) {
                            try { el.click(); return true; } catch(e) { return false; }
                        }
                    }
                    return false;
                }
            """)
            if changed:
                await page.wait_for_timeout(600)
        except Exception:
            pass

    async def _ensure_type_sub_all(page):
        try:
            changed = await page.evaluate(r"""
                () => {
                    function isVisible(el){
                        if(!el) return false;
                        const style = getComputedStyle(el);
                        return el.offsetParent !== null && style.display !== 'none' && style.visibility !== 'hidden';
                    }
                    const root = document.querySelector('.type-sub-item');
                    if (!root) return false;
                    const cands = Array.from(root.querySelectorAll('a, button, label'));
                    for (const el of cands){
                        const txt = (el.textContent||'').replace(/\s+/g,'').trim();
                        if (txt.includes('ì „ì²´') && isVisible(el)){
                            try { el.click(); return true; } catch(e) { return false; }
                        }
                    }
                    return false;
                }
            """)
            if changed:
                await page.wait_for_timeout(600)
        except Exception:
            pass

    async def _extract_items(page) -> list:
        """MCP ê²€ì¦ ë¡œì§ê³¼ ë™ì¼: .plan-list-area .btns a[href*='productDetail']ë§Œ ìˆ˜ì§‘"""
        items = await page.evaluate("""
            () => {
                const results = [];
                // MCPì™€ ë™ì¼: .plan-list-areaì—ì„œë§Œ ìˆ˜ì§‘
                const anchors = Array.from(document.querySelectorAll('.plan-list-area .btns a[href*="productDetail"]'));

                function normRel(href){
                    try{
                        const a = document.createElement('a');
                        a.href = href;
                        const rel = `${a.pathname}${a.search||''}`;
                        return rel.startsWith('/wDic/') ? rel : (rel.startsWith('/') ? rel : `/wDic/${rel}`);
                    }catch(e){
                        return href.startsWith('/wDic/') ? href : (href.startsWith('/') ? href : `/wDic/${href}`);
                    }
                }

                function getNearestTitle(anchor){
                    const titleSelector = '.title, .plan_tit, .tit, .name, strong, span.two-line';
                    
                    // span íƒœê·¸ë¥¼ ì œì™¸í•˜ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
                    function extractTextWithoutSpan(element) {
                        if (!element) return '';
                        
                        // .title í´ë˜ìŠ¤ë¥¼ ê°€ì§„ thë‚˜ divì¸ ê²½ìš°, span ì œì™¸í•˜ê³  strong ë“±ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                        if (element.classList && element.classList.contains('title')) {
                            let text = '';
                            for (const child of element.childNodes) {
                                if (child.nodeType === Node.TEXT_NODE) {
                                    text += child.textContent;
                                } else if (child.nodeType === Node.ELEMENT_NODE && child.tagName !== 'SPAN') {
                                    text += child.textContent;
                                }
                            }
                            return text.trim();
                        }
                        
                        return (element.textContent || '').trim();
                    }
                    
                    let el = anchor.closest('li, tr, .plan-list li, .prd-list li, .result-list li, .list-item, .card, .box');
                    if (el){
                        const t = el.querySelector(titleSelector);
                        if (t) return extractTextWithoutSpan(t);
                    }
                    // ì´ì „ í˜•ì œ ê²€ìƒ‰
                    let cur = anchor.parentElement;
                    for (let depth=0; depth<5 && cur; depth++){
                        let prev = cur.previousElementSibling;
                        let hops = 0;
                        while(prev && hops < 10){
                            const t = prev.querySelector(titleSelector);
                            if (t) return extractTextWithoutSpan(t);
                            prev = prev.previousElementSibling;
                            hops++;
                        }
                        cur = cur.parentElement;
                    }
                    // ìƒìœ„ì—ì„œ ê²€ìƒ‰
                    let parent = anchor.parentElement;
                    for (let i=0; i<6 && parent; i++){
                        const t = parent.querySelector(titleSelector);
                        if (t) return extractTextWithoutSpan(t);
                        parent = parent.parentElement;
                    }
                    const at = (anchor.textContent||'').trim();
                    if (!/ìƒì„¸|ìì„¸íˆ/.test(at)) return at;
                    return '';
                }

                anchors.forEach(a => {
                    const href = a.getAttribute('href') || a.href || '';
                    if (!href || href === '#' || href.startsWith('javascript:')) return;
                    const rel = normRel(href);
                    const title = getNearestTitle(a);
                    results.push({ title: title || '', relHref: rel });
                });

                return results;
            }
        """)
        return items or []

    menus, datas = [], []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        response = await page.goto(url, wait_until='domcontentloaded', timeout=60000)
        await page.wait_for_timeout(1200)

        status_code = response.status if response else None
        if status_code and status_code >= 400:
            logging.error(f"âŒ wDic ëª©ë¡ ({url}): HTTP {status_code} ì˜¤ë¥˜")

        # íƒ­ ë¡œë“œ ëŒ€ê¸° (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
        try:
            await page.wait_for_selector('ul.ui-tab-list, ul.red-select', timeout=10000)
        except Exception:
            pass

        # íƒ­ ìˆ˜ì§‘ (ì—¬ëŸ¬ ì„ íƒì ëŒ€ì‘: ui-tab-list ë˜ëŠ” red-select)
        # 'ì¶”ì²œ' íƒ­ ì œì™¸
        tabs = await page.evaluate("""
            () => {
                const arr = [];
                // ì—¬ëŸ¬ íƒ­ ì»¨í…Œì´ë„ˆ ì„ íƒì ì‹œë„
                const selectors = ['ul.ui-tab-list li a', 'ul.red-select li a'];
                let anchors = [];
                for (const sel of selectors) {
                    anchors = Array.from(document.querySelectorAll(sel));
                    if (anchors.length > 0) break;
                }
                
                let idx = 0;
                if (anchors.length === 0) {
                    arr.push({ index: -1, text: 'ì „ì²´' });
                } else {
                    anchors.forEach((a, originalIdx) => {
                        const text = (a.textContent||'').trim();
                        // 'ì¶”ì²œ' íƒ­ ì œì™¸
                        if (text === 'ì¶”ì²œ') return;
                        arr.push({ index: originalIdx, text });
                    });
                }
                return arr;
            }
        """)
        if not tabs:
            tabs = [{ 'index': -1, 'text': 'ì „ì²´' }]

        detail_targets = []

        # ì´ˆê¸°(ê¸°ë³¸) ëª©ë¡ í˜ì´ì§€ë„ ìº¡ì²˜
        try:
            await _capture_list_snapshot(page, base_menu=(menu or "").strip())
        except Exception:
            pass

        for tab in tabs:
            try:
                # íƒ­ í´ë¦­ (ì—¬ëŸ¬ ì„ íƒì ëŒ€ì‘: JavaScriptë¡œ ì§ì ‘ í´ë¦­)
                if tab.get('index', -1) >= 0:
                    tab_clicked = await page.evaluate(f"""
                        () => {{
                            // ì—¬ëŸ¬ íƒ­ ì„ íƒì ì‹œë„
                            const selectors = ['ul.ui-tab-list li a', 'ul.red-select li a'];
                            let tabs = [];
                            for (const sel of selectors) {{
                                tabs = Array.from(document.querySelectorAll(sel));
                                if (tabs.length > 0) break;
                            }}
                            
                            if (tabs.length > {tab['index']}) {{
                                tabs[{tab['index']}].click();
                                return true;
                            }}
                            return false;
                        }}
                    """)
                    if tab_clicked:
                        # ë„¤íŠ¸ì›Œí¬ê°€ ì•ˆì •ë  ë•Œê¹Œì§€ ëŒ€ê¸°
                        try:
                            await page.wait_for_load_state('networkidle', timeout=5000)
                        except Exception:
                            await page.wait_for_timeout(1200)

                # ìƒìœ„ í•„í„° 'ì „ì²´' ë³´ì •
                await _ensure_filter_all(page)
                await page.wait_for_timeout(800)

                # type-sub-item í™•ì¸ ë° ì²˜ë¦¬ (ëª¨ë“  ì„œë¸Œ í•„í„° ìˆœíšŒ)
                sub_filters = await page.evaluate("""
                    () => {
                        const root = document.querySelector('.type-sub-item');
                        if (!root) return [];
                        const filters = Array.from(root.querySelectorAll('a, button, label'));
                        const result = [];
                        filters.forEach((el, idx) => {
                            const text = (el.textContent||'').trim();
                            if (text) result.push({ index: idx, text });
                        });
                        return result;
                    }
                """)

                if sub_filters and len(sub_filters) > 0:
                    # ì„œë¸Œ í•„í„°ê°€ ìˆìœ¼ë©´ ëª¨ë“  í•„í„° ìˆœíšŒ
                    logging.info(f"íƒ­ '{tab.get('text','')}': ì„œë¸Œ í•„í„° {len(sub_filters)}ê°œ ë°œê²¬, ëª¨ë‘ ìˆœíšŒ")
                    for sub_filter in sub_filters:
                        try:
                            # ì„œë¸Œ í•„í„° í´ë¦­ ì „ í˜„ì¬ ë¦¬ìŠ¤íŠ¸ ê°œìˆ˜ ê¸°ë¡
                            prev_count = await page.evaluate("document.querySelectorAll('.plan-list-area .plan-list li').length")
                            
                            # ì„œë¸Œ í•„í„° í´ë¦­
                            sub_clicked = await page.evaluate(f"""
                                () => {{
                                    const root = document.querySelector('.type-sub-item');
                                    if (!root) return false;
                                    const filters = Array.from(root.querySelectorAll('a, button, label'));
                                    if (filters.length > {sub_filter['index']}) {{
                                        filters[{sub_filter['index']}].click();
                                        return true;
                                    }}
                                    return false;
                                }}
                            """)
                            if sub_clicked:
                                # ë„¤íŠ¸ì›Œí¬ê°€ ì•ˆì •ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 5ì´ˆ)
                                try:
                                    await page.wait_for_load_state('networkidle', timeout=5000)
                                except Exception:
                                    pass
                                
                                # ì¶”ê°€ë¡œ ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ í™•ì¸ (ìµœëŒ€ 3ì´ˆ)
                                for _ in range(6):
                                    await page.wait_for_timeout(500)
                                    new_count = await page.evaluate("document.querySelectorAll('.plan-list-area .plan-list li').length")
                                    if new_count > 0 and new_count != prev_count:
                                        break

                            clicks = await _click_more_until_exhausted(page)
                            items = await _extract_items(page)
                            
                            # í˜„ì¬ íƒ­+ì„œë¸Œí•„í„°ì˜ ëª©ë¡ í™”ë©´ë„ ìº¡ì²˜
                            try:
                                await _capture_list_snapshot(
                                    page,
                                    base_menu=(menu or "").strip(),
                                    tab_text=tab.get('text', '').strip(),
                                    sub_filter_text=sub_filter.get('text', '').strip()
                                )
                            except Exception:
                                pass

                            li_count = await page.evaluate("document.querySelectorAll('.plan-list-area .plan-list li').length")
                            
                            # ìƒì„¸ë§í¬ 0ê°œì¼ ë•Œ ë°©ì–´ ë¡œì§: ì¬ì‹œë„
                            if len(items) == 0:
                                if clicks == 0 and li_count == 0:
                                    # ë”ë³´ê¸° í´ë¦­ì´ 0íšŒì´ê³  ë¦¬ìŠ¤íŠ¸ë„ ì—†ëŠ” ê²½ìš°: í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ í›„ ì¬ì‹œë„
                                    logging.warning(f"âš ï¸  ë”ë³´ê¸° í´ë¦­ 0íšŒ, ìƒì„¸ë§í¬ 0ê°œ (li={li_count}) - í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ í›„ ì¬ì‹œë„ ì¤‘...")
                                    await page.reload(timeout=10000)
                                    await page.wait_for_timeout(2000)
                                    try:
                                        await page.wait_for_load_state('networkidle', timeout=5000)
                                    except Exception:
                                        pass
                                    # ì„œë¸Œ í•„í„° ë‹¤ì‹œ í´ë¦­
                                    if sub_clicked:
                                        sub_clicked = await page.evaluate(f"""
                                            () => {{
                                                const root = document.querySelector('.type-sub-item');
                                                if (!root) return false;
                                                const filters = Array.from(root.querySelectorAll('a, button, label'));
                                                if (filters.length > {sub_filter['index']}) {{
                                                    filters[{sub_filter['index']}].click();
                                                    return true;
                                                }}
                                                return false;
                                            }}
                                        """)
                                        if sub_clicked:
                                            await page.wait_for_timeout(2000)
                                    clicks = await _click_more_until_exhausted(page)
                                    items = await _extract_items(page)
                                    li_count = await page.evaluate("document.querySelectorAll('.plan-list-area .plan-list li').length")
                                elif li_count > 0:
                                    # ë¦¬ìŠ¤íŠ¸ëŠ” ìˆì§€ë§Œ ìƒì„¸ë§í¬ê°€ ì—†ëŠ” ê²½ìš°: í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° í›„ ì¬ì‹œë„
                                    logging.warning(f"âš ï¸  ìƒì„¸ë§í¬ 0ê°œ ê°ì§€ (li={li_count}), í˜ì´ì§€ ë¡œë“œ ì¬ì‹œë„ ì¤‘...")
                                    await page.wait_for_timeout(2000)
                                    try:
                                        await page.wait_for_load_state('networkidle', timeout=5000)
                                    except Exception:
                                        pass
                                    items = await _extract_items(page)
                                
                                if len(items) == 0:
                                    logging.error(f"âŒ ì¬ì‹œë„ í›„ì—ë„ ìƒì„¸ë§í¬ 0ê°œ: íƒ­='{tab.get('text','')}', ì„œë¸Œí•„í„°='{sub_filter.get('text','')}', clicks={clicks}, li={li_count}")
                            
                            logging.info(f"íƒ­ '{tab.get('text','')}' > ì„œë¸Œí•„í„° '{sub_filter.get('text','')}' ë”ë³´ê¸° í´ë¦­ {clicks}íšŒ, li={li_count}, ìƒì„¸ë§í¬={len(items)}ê°œ ìˆ˜ì§‘")

                            for it in items:
                                if not it.get('relHref'):
                                    continue
                                detail_targets.append({
                                    'tab': tab.get('text', ''),
                                    'sub_filter': sub_filter.get('text', ''),
                                    'title': it.get('title', '').strip() or '(ì œëª© ì—†ìŒ)',
                                    'relHref': it['relHref']
                                })
                        except Exception as e:
                            logging.warning(f"ì„œë¸Œ í•„í„° '{sub_filter.get('text','')}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                            continue
                else:
                    # ì„œë¸Œ í•„í„° ì—†ìœ¼ë©´ ê¸°ì¡´ ë¡œì§
                    clicks = await _click_more_until_exhausted(page)
                    items = await _extract_items(page)
                    
                    # í˜„ì¬ íƒ­ì˜ ëª©ë¡ í™”ë©´ë„ ìº¡ì²˜
                    try:
                        await _capture_list_snapshot(
                            page,
                            base_menu=(menu or "").strip(),
                            tab_text=tab.get('text', '').strip()
                        )
                    except Exception:
                        pass

                    li_count = await page.evaluate("document.querySelectorAll('.plan-list-area .plan-list li').length")
                    
                    # ìƒì„¸ë§í¬ 0ê°œì¼ ë•Œ ë°©ì–´ ë¡œì§: ì¬ì‹œë„
                    if len(items) == 0:
                        if clicks == 0 and li_count == 0:
                            # ë”ë³´ê¸° í´ë¦­ì´ 0íšŒì´ê³  ë¦¬ìŠ¤íŠ¸ë„ ì—†ëŠ” ê²½ìš°: í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ í›„ ì¬ì‹œë„
                            logging.warning(f"âš ï¸  ë”ë³´ê¸° í´ë¦­ 0íšŒ, ìƒì„¸ë§í¬ 0ê°œ (li={li_count}) - í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ í›„ ì¬ì‹œë„ ì¤‘...")
                            await page.reload(timeout=10000)
                            await page.wait_for_timeout(2000)
                            try:
                                await page.wait_for_load_state('networkidle', timeout=5000)
                            except Exception:
                                pass
                            clicks = await _click_more_until_exhausted(page)
                            items = await _extract_items(page)
                            li_count = await page.evaluate("document.querySelectorAll('.plan-list-area .plan-list li').length")
                        elif li_count > 0:
                            # ë¦¬ìŠ¤íŠ¸ëŠ” ìˆì§€ë§Œ ìƒì„¸ë§í¬ê°€ ì—†ëŠ” ê²½ìš°: í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° í›„ ì¬ì‹œë„
                            logging.warning(f"âš ï¸  ìƒì„¸ë§í¬ 0ê°œ ê°ì§€ (li={li_count}), í˜ì´ì§€ ë¡œë“œ ì¬ì‹œë„ ì¤‘...")
                            await page.wait_for_timeout(2000)
                            try:
                                await page.wait_for_load_state('networkidle', timeout=5000)
                            except Exception:
                                pass
                            items = await _extract_items(page)
                        
                        if len(items) == 0:
                            logging.error(f"âŒ ì¬ì‹œë„ í›„ì—ë„ ìƒì„¸ë§í¬ 0ê°œ: íƒ­='{tab.get('text','')}', clicks={clicks}, li={li_count}")
                    
                    logging.info(f"íƒ­ '{tab.get('text','')}' ë”ë³´ê¸° í´ë¦­ {clicks}íšŒ, li={li_count}, ìƒì„¸ë§í¬={len(items)}ê°œ ìˆ˜ì§‘")

                    for it in items:
                        if not it.get('relHref'):
                            continue
                        detail_targets.append({
                            'tab': tab.get('text', ''),
                            'title': it.get('title', '').strip() or '(ì œëª© ì—†ìŒ)',
                            'relHref': it['relHref']
                        })
            except Exception as e:
                logging.warning(f"íƒ­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue

        # ì¤‘ë³µ ì œê±°: ItemCode ê¸°ì¤€ìœ¼ë¡œ (URLì—ì„œ ì¶”ì¶œ)
        import re
        seen_itemcodes = set()
        unique_targets = []
        for target in detail_targets:
            # URLì—ì„œ ItemCode ì¶”ì¶œ
            match = re.search(r'ItemCode=(\d+)', target['relHref'])
            if match:
                itemcode = match.group(1)
                if itemcode in seen_itemcodes:
                    continue
                seen_itemcodes.add(itemcode)
            unique_targets.append(target)
        
        logging.info(f"ì¤‘ë³µ ì œê±°: ì „ì²´ {len(detail_targets)}ê°œ â†’ ìœ ë‹ˆí¬ {len(unique_targets)}ê°œ")
        detail_targets = unique_targets

        # ìƒì„¸ ì²˜ë¦¬ (ì§ë ¬)
        for i, target in enumerate(detail_targets, 1):
            detail_url = urljoin(base_host, target['relHref'])
            try:
                result = await handle_product_detail(detail_url, fclient, menu)
                if not result:
                    logging.warning(f"ìƒì„¸ ì²˜ë¦¬ ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ê²°ê³¼: {detail_url}")
                    continue

                base_menu = (menu or '').strip()
                tab_prefix = target.get('tab', '').strip()
                sub_filter_name = target.get('sub_filter', '').strip()
                title_suffix = target.get('title', '').strip()
                final_menu = base_menu
                if tab_prefix:
                    final_menu = f"{final_menu}^{tab_prefix}" if final_menu else tab_prefix
                if sub_filter_name:
                    final_menu = f"{final_menu}^{sub_filter_name}" if final_menu else sub_filter_name
                if title_suffix:
                    final_menu = f"{final_menu}^{title_suffix}" if final_menu else title_suffix

                menus.append({ 'menu': final_menu or (result.get('title') or ''), 'url': detail_url })
                datas.append(result)
                logging.info(f"[{i}/{len(detail_targets)}] ìƒì„¸ ì²˜ë¦¬ ì™„ë£Œ: {detail_url}")
            except Exception as e:
                logging.error(f"ìƒì„¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {detail_url} - {str(e)}")
                continue

        await browser.close()

    return {
        'menus': menus,
        'datas': datas,
        'metadata': {
            'url': url,
            'total_items': len(datas),
            'source': 'wdic_list',
            'special_processed': True,
            'playwright_processed': True
        }
    }

register_page_handler(
    r'https?://product\.kt\.com/wDic/.*index\.do\?CateCode=\d+',
    handle_wdic_mobile_list
)


async def handle_gigagenie_detail(url: str, fclient=None, menu=None) -> dict:
    """
    ê¸°ê°€ì§€ë‹ˆ ì„œë¹„ìŠ¤ ìƒì„¸ í˜ì´ì§€(2ëìŠ¤ íƒ­/ë²„íŠ¼ ë™ì  ìˆœíšŒ) í¬ë¡¤ë§ ë° ë§ˆí¬ë‹¤ìš´/HTML ë°˜í™˜
    - 2ëìŠ¤ ë²„íŠ¼(ul#depth2Level li button)ë“¤ì„ ëª¨ë‘ ìˆœíšŒí•˜ë©° í´ë¦­
    - ê° ë²„íŠ¼ í´ë¦­ í›„ ë³¸ë¬¸(div.fjbInnerTabBox.fjbTabCon*.on) ë‚´ìš©ì„ ì¶”ì¶œ
    - # {íƒ­ëª…}\n... í˜•ì‹ìœ¼ë¡œ ë§ˆí¬ë‹¤ìš´ ëˆ„ì 
    - depth2Levelì´ ì—†ëŠ” ê²½ìš°, ê¸°ë³¸ ì½˜í…ì¸ ë§Œ ì¶”ì¶œ
    - ë°˜í™˜ê°’: {url, markdown, html, special_processed, playwright_processed}
    """
    import re
    from markdownify import markdownify as md
    import logging
    from playwright.async_api import async_playwright

    def clean_img_alt(md_text):
        # altì— <ê°€ í¬í•¨ëœ ê²½ìš° altë¥¼ ë¹„ì›€ (ì¤„ë°”ê¿ˆ í¬í•¨)
        def repl(match):
            alt = match.group(1)
            url = match.group(2)
            if '<' in alt:
                return f"![]({url})"
            else:
                return match.group(0)
        return re.sub(r'!\[(.*?)\]\((.*?)\)', repl, md_text, flags=re.DOTALL)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        response = await page.goto(url, wait_until="domcontentloaded", timeout=40000)
        await page.wait_for_timeout(3000)
        
        # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
        status_code = response.status if response else None
        if status_code:
            if status_code >= 400:
                logging.error(f"âŒ ê¸°ê°€ì§€ë‹ˆ ìƒì„¸ ({url}): HTTP {status_code} ì˜¤ë¥˜")
            elif status_code >= 300:
                logging.warning(f"âš ï¸ ê¸°ê°€ì§€ë‹ˆ ìƒì„¸ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
            else:
                logging.info(f"âœ… ê¸°ê°€ì§€ë‹ˆ ìƒì„¸ ({url}): HTTP {status_code} ì„±ê³µ")
        else:
            logging.debug(f"ğŸ” ê¸°ê°€ì§€ë‹ˆ ìƒì„¸ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")

        # 2ëìŠ¤ ë²„íŠ¼ ëª©ë¡ ì¶”ì¶œ
        buttons = await page.query_selector_all("#depth2Level li button")
        markdown_content = ""
        html_content = ""
        if buttons and len(buttons) > 0:
            tab_infos = []
            for btn in buttons:
                # íƒ­ëª… ì¶”ì¶œ (span í…ìŠ¤íŠ¸)
                span = await btn.query_selector("span")
                tab_name = (await span.inner_text()).strip() if span else (await btn.inner_text()).strip()
                tab_infos.append({"button": btn, "tab_name": tab_name})

            for tab in tab_infos:
                btn = tab["button"]
                tab_name = tab["tab_name"]
                try:
                    await btn.click()
                    await page.wait_for_timeout(1200)
                    # ë³¸ë¬¸ ì¶”ì¶œ: div.fjbInnerTabBox.fjbTabCon*.on (on í´ë˜ìŠ¤ê°€ ë¶™ì€ ê²ƒë§Œ)
                    content_div = await page.query_selector("div.fjbInnerTabBox[class*='fjbTabCon'][class~='on']")
                    if content_div:
                        html = await content_div.inner_html()
                        md_text = md(html)
                        md_text = clean_img_alt(md_text)
                        markdown_content += f"# {tab_name}\n\n{md_text}\n\n"
                        html_content += f"<h1>{tab_name}</h1>\n{html}\n\n"
                    else:
                        markdown_content += f"# {tab_name}\n\n(ë‚´ìš© ì—†ìŒ)\n\n"
                        html_content += f"<h1>{tab_name}</h1>\n(ë‚´ìš© ì—†ìŒ)\n\n"
                except Exception as e:
                    logging.warning(f"íƒ­ '{tab_name}' í´ë¦­/ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    markdown_content += f"# {tab_name}\n\n(íƒ­ ì¶”ì¶œ ì‹¤íŒ¨)\n\n"
                    html_content += f"<h1>{tab_name}</h1>\n(íƒ­ ì¶”ì¶œ ì‹¤íŒ¨)\n\n"
        else:
            # depth2Levelì´ ì—†ëŠ” ê²½ìš°: ê¸°ë³¸ ì½˜í…ì¸ ë§Œ ì¶”ì¶œ
            content_div = await page.query_selector("div.fjbInnerTabBox[class*='fjbTabCon'][class~='on']")
            if not content_div:
                # onì´ ì—†ìœ¼ë©´ fjbTabCon* ì¤‘ ì²« ë²ˆì§¸ ì‚¬ìš©
                content_divs = await page.query_selector_all("div.fjbInnerTabBox[class*='fjbTabCon']")
                content_div = content_divs[0] if content_divs else None
            if content_div:
                html = await content_div.inner_html()
                md_text = md(html)
                md_text = clean_img_alt(md_text)
                markdown_content += f"# ê¸°ë³¸ ì½˜í…ì¸ \n\n{md_text}\n\n"
                html_content += f"<h1>ê¸°ë³¸ ì½˜í…ì¸ </h1>\n{html}\n\n"
            else:
                markdown_content += f"# ê¸°ë³¸ ì½˜í…ì¸ \n\n(ë‚´ìš© ì—†ìŒ)\n\n"
                html_content += f"<h1>ê¸°ë³¸ ì½˜í…ì¸ </h1>\n(ë‚´ìš© ì—†ìŒ)\n\n"
        # ì „ì²´ í˜ì´ì§€ HTMLë„ ì €ì¥
        page_html = await page.content()
        await browser.close()

    return {
        "url": url,
        "markdown": markdown_content.strip(),
        "html": html_content.strip(),
        "special_processed": True,
        "playwright_processed": True
    }

register_page_handler(
    r'https?://gigagenie\.kt\.com/whyGenieServiceDetail\.do\?serviceCate=.*',
    handle_gigagenie_detail
)

async def handle_gigagenie_faq_playwright(url: str, fclient) -> dict:
    """
    Playwrightë¡œ ê¸°ê°€ì§€ë‹ˆ ìì£¼í•˜ëŠ”ì§ˆë¬¸ ì „ì²´ í˜ì´ì§€(ìƒí’ˆë³„ ë²„íŠ¼, í˜ì´ì§€ë„¤ì´ì…˜ í¬í•¨) Q/A ì¶”ì¶œ í•¸ë“¤ëŸ¬
    - ìƒí’ˆë³„ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ê° ìƒí’ˆì˜ FAQ ì¶”ì¶œ
    - selectFaqList() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
    - ì§ˆë¬¸/ë‹µë³€ êµ¬ì¡°: ul#faqList li > a.fjbQuestion (í´ë¦­) + div.fjbAnser
    - íƒ€ì„ì•„ì›ƒ ì‹œ ì™„ì „í•œ ë¸Œë¼ìš°ì € ì„¸ì…˜ ì¬ì‹œì‘ ë©”ì»¤ë‹ˆì¦˜ í¬í•¨
    """
    logging.info(f"ê¸°ê°€ì§€ë‹ˆ FAQ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}")
    
    # FAQëŠ” ìƒíƒœ ë³´ì¡´ì´ ì¤‘ìš”í•˜ë¯€ë¡œ ë‹¨ì¼ ì‹œë„ë¡œ ì²˜ë¦¬
    # íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œì—ë§Œ ì™„ì „ ì¬ì‹œì‘
    logging.info(f"ê¸°ê°€ì§€ë‹ˆ FAQ í˜ì´ì§€ ì§„ì…: url={url}")
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        # FAQ í˜ì´ì§€ ë¡œë“œ (ì¶©ë¶„í•œ íƒ€ì„ì•„ì›ƒ ì„¤ì •)
        response = await page.goto(url, wait_until="domcontentloaded", timeout=60000)
        await page.wait_for_timeout(8000)  # ì¶©ë¶„í•œ ëŒ€ê¸° ì‹œê°„
        
        # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
        status_code = response.status if response else None
        if status_code:
            if status_code >= 400:
                logging.error(f"âŒ ê¸°ê°€ì§€ë‹ˆ FAQ ({url}): HTTP {status_code} ì˜¤ë¥˜")
            elif status_code >= 300:
                logging.warning(f"âš ï¸ ê¸°ê°€ì§€ë‹ˆ FAQ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
            else:
                logging.info(f"âœ… ê¸°ê°€ì§€ë‹ˆ FAQ ({url}): HTTP {status_code} ì„±ê³µ")
        else:
            logging.debug(f"ğŸ” ê¸°ê°€ì§€ë‹ˆ FAQ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")
        
        # í˜ì´ì§€ ë¡œë”© ìƒíƒœ í™•ì¸
        try:
            await page.wait_for_selector("button[class*='fjbCard']", timeout=30000)
            logging.info("FAQ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            logging.warning(f"FAQ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹¤íŒ¨: {e}, ê³„ì† ì§„í–‰")

        markdown_body = ""
        all_qa_list = []
        
        # ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ (FAQ ì œì™¸) - Playwright ì‚¬ìš©
        try:
            logging.info("ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì‹œì‘")
            from markdownify import markdownify as md
            
            # FAQ ê´€ë ¨ ìš”ì†Œë“¤ ë° ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            faq_selectors = [
                # FAQ ê´€ë ¨
                'ul#faqList', '.faqList', 
                '.accordion-area', '.accordion',
                '.faq_box', '.faq', '.faq-list', '.faq-item', 
                '.inquiry', '.answer', '.faqClass',
                'img[src*="faq"]', 'img[src*="FAQ"]',
                'a[href*="faq"]', 'a[href*="FAQ"]',
                # Header/Footer/Navigation
                'header', 'footer', 
                '.header', '.footer',
                '#header', '#footer',
                '#cfmClHeader', '#cfmClFooter',
                '.inner', 'nav', '.navigation'
            ]
            
            for selector in faq_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    for element in elements:
                        await element.evaluate('element => element.remove()')
                except:
                    pass  # ì…€ë ‰í„°ê°€ ìœ íš¨í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
            
            page_html = await page.content()
            page_markdown = md(page_html) if page_html else ""
            logging.info(f"ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì™„ë£Œ: ë§ˆí¬ë‹¤ìš´ {len(page_markdown)}ì, HTML {len(page_html)}ì")
        except Exception as e:
            logging.error(f"ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            page_markdown = ""
            page_html = ""
        
        # FAQ ì¶”ì¶œì„ ìœ„í•´ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
        await page.reload()
        await page.wait_for_timeout(3000)
        
        # 1. ìƒí’ˆë³„ ë²„íŠ¼ ëª©ë¡ ì¶”ì¶œ (ë‹¤ì–‘í•œ í´ë˜ìŠ¤ëª… ê³ ë ¤)
        product_buttons = await page.query_selector_all("button[class*='fjbCard']")
        logging.info(f"ì´ {len(product_buttons)}ê°œ ìƒí’ˆ ë²„íŠ¼ ë°œê²¬. FAQ ì¶”ì¶œ ì‹œì‘")
        
        for product_idx in range(len(product_buttons)):
            try:
                # í•­ìƒ ìµœì‹  ë²„íŠ¼ í•¸ë“¤ë¡œ ì¬ì¡°íšŒ
                product_buttons = await page.query_selector_all("button[class*='fjbCard']")
                button = product_buttons[product_idx]
                # ìƒí’ˆëª… ì¶”ì¶œ
                product_name = await button.get_attribute("id-name")
                if not product_name:
                    product_name = await button.inner_text()
                    product_name = product_name.replace('\n', ' ').strip()
                
                logging.info(f"ìƒí’ˆ {product_idx + 1}/{len(product_buttons)} ì²˜ë¦¬ ì‹œì‘: {product_name}")
                
                # ìƒí’ˆ ë²„íŠ¼ í´ë¦­
                await button.click()
                await page.wait_for_timeout(2000)
                
                # 2. í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬ (selectFaqList í•¨ìˆ˜ ì‚¬ìš©)
                page_num = 1
                while True:
                    logging.info(f"  {product_name} - í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘...")
                    # ì˜¬ë°”ë¥¸ ì…€ë ‰í„° ì‚¬ìš©: ul#faqList li
                    qa_items = await page.query_selector_all("ul#faqList li")
                    
                    # FAQê°€ ì—†ìœ¼ë©´ ìƒˆë¡œê³ ì¹¨ í›„ ë™ì¼ ìƒí’ˆ/ë™ì¼ í˜ì´ì§€ë¡œ ë³µì›í•´ì„œ í•œ ë²ˆ ë” ì‹œë„
                    if not qa_items:
                        logging.warning(f"  í˜ì´ì§€ {page_num}ì—ì„œ FAQ í•­ëª©ì´ ì—†ìŒ. ìƒˆë¡œê³ ì¹¨ í›„ ì¬ì‹œë„")
                        try:
                            await page.reload()
                            await page.wait_for_timeout(3000)
                            # ìƒí’ˆ ë²„íŠ¼ ë‹¤ì‹œ í´ë¦­ (ìƒˆë¡œê³ ì¹¨ í›„)
                            product_buttons = await page.query_selector_all("button[class*='fjbCard']")
                            # ìƒˆë¡œê³ ì¹¨ í›„ product_buttons ê¸¸ì´ ì²´í¬
                            if product_idx >= len(product_buttons):
                                logging.warning(f"  ìƒˆë¡œê³ ì¹¨ í›„ ìƒí’ˆ ë²„íŠ¼ ê°œìˆ˜ê°€ ì¤„ì–´ë“¦. ì›ë˜: {product_idx + 1}ê°œ, í˜„ì¬: {len(product_buttons)}ê°œ. í•´ë‹¹ ìƒí’ˆ ê±´ë„ˆëœ€")
                                break
                            button = product_buttons[product_idx]
                            await button.click()
                            await page.wait_for_timeout(2000)
                            # í•´ë‹¹ í˜ì´ì§€ë¡œ ì´ë™
                            if page_num > 1:
                                await page.evaluate(f"selectFaqList({page_num})")
                                await page.wait_for_timeout(2000)
                            # ë‹¤ì‹œ FAQ ë¦¬ìŠ¤íŠ¸ ì¿¼ë¦¬
                            qa_items = await page.query_selector_all("ul#faqList li")
                            if not qa_items:
                                logging.warning(f"  ìƒˆë¡œê³ ì¹¨ í›„ì—ë„ í˜ì´ì§€ {page_num} FAQ ì—†ìŒ. ë‹¤ìŒìœ¼ë¡œ ì´ë™")
                                break
                        except Exception as e:
                            logging.error(f"  ìƒˆë¡œê³ ì¹¨ í›„ ë³µì› ì‹¤íŒ¨: {str(e)}. í•´ë‹¹ ìƒí’ˆ ê±´ë„ˆëœ€")
                            break
                    
                    logging.info(f"  í˜ì´ì§€ {page_num}ì—ì„œ {len(qa_items)}ê°œ FAQ í•­ëª© ë°œê²¬")
                    
                    for qa_idx, qa_item in enumerate(qa_items):
                        try:
                            # ì§ˆë¬¸ ë§í¬ ì°¾ê¸°
                            question_link = await qa_item.query_selector("a.fjbQuestion")
                            if not question_link:
                                logging.warning(f"    FAQ {qa_idx + 1}: ì§ˆë¬¸ ë§í¬ ì—†ìŒ")
                                continue
                            
                            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                            category_elem = await question_link.query_selector("span.fjbCategory")
                            category = await category_elem.inner_text() if category_elem else ""
                            
                            # ì§ˆë¬¸ ì œëª© ì¶”ì¶œ
                            title_elem = await question_link.query_selector("span.fjbTit")
                            question_title = await title_elem.inner_text() if title_elem else ""
                            
                            # ì „ì²´ ì§ˆë¬¸ í…ìŠ¤íŠ¸ (ì¹´í…Œê³ ë¦¬ + ì œëª©)
                            if category and question_title:
                                question = f"[{category}] {question_title}"
                            else:
                                question = question_title or (await question_link.inner_text())
                            
                            # ë‹µë³€ ìš”ì†Œ í™•ì¸ (ì•„ì½”ë””ì–¸ í´ë¦­ ì—†ì´ ë°”ë¡œ ì¶”ì¶œ)
                            answer_elem = await qa_item.query_selector("div.fjbAnser")
                            answer = ""
                            if answer_elem:
                                answer_p = await answer_elem.query_selector("p")
                                if answer_p:
                                    answer = await answer_p.inner_text()
                                else:
                                    answer = await answer_elem.inner_text()
                                # HTML ì—”í‹°í‹° ì •ë¦¬
                                answer = answer.replace('&gt;', '>').replace('&lt;', '<').replace('&nbsp;', ' ')
                                # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
                                answer = re.sub(r'\s+', ' ', answer).strip()
                            else:
                                logging.warning(f"    FAQ {qa_idx + 1}: ë‹µë³€ ìš”ì†Œ ì—†ìŒ")
                            
                            # ìœ íš¨í•œ Q/Aë§Œ ì¶”ê°€
                            if question.strip() and answer.strip():
                                # êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œë§Œ ì¶”ê°€ (ë§ˆí¬ë‹¤ìš´ ì œê±°)
                                all_qa_list.append({
                                    "product": product_name,
                                    "category": category,
                                    "question": question.strip(),
                                    "answer": answer.strip(),
                                    "page": page_num
                                })
                                
                                logging.info(f"    FAQ {qa_idx + 1} ì¶”ì¶œ ì™„ë£Œ: {question[:50]}...")
                            else:
                                logging.warning(f"    FAQ {qa_idx + 1} ì¶”ì¶œ ì‹¤íŒ¨: ì§ˆë¬¸='{question[:30]}', ë‹µë³€='{answer[:30]}'")
                        except Exception as e:
                            logging.error(f"  FAQ í•­ëª© {qa_idx + 1} ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                            continue
                        
                        # TEST CODE
                        break
                    
                    # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸ ë° ì´ë™ (selectFaqList í•¨ìˆ˜ ì‚¬ìš©)
                    page_num += 1
                    next_page_selector = f"a[onclick*='selectFaqList({page_num})']"
                    try:
                        next_page_link = await page.query_selector(next_page_selector)
                        if next_page_link and await next_page_link.is_visible():
                            logging.info(f"  í˜ì´ì§€ {page_num}ë¡œ ì´ë™ (selectFaqList)")
                            await next_page_link.click()
                            await page.wait_for_timeout(10000)  # í˜ì´ì§€ë„¤ì´ì…˜ í´ë¦­ í›„ ì¶©ë¶„íˆ ëŒ€ê¸°
                        else:
                            # JavaScript í•¨ìˆ˜ ì§ì ‘ ì‹¤í–‰
                            try:
                                await page.evaluate(f"selectFaqList({page_num})")
                                await page.wait_for_timeout(10000)  # JSë¡œ ì´ë™ í›„ì—ë„ ì¶©ë¶„íˆ ëŒ€ê¸°
                                # ì‹¤ì œë¡œ í˜ì´ì§€ê°€ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
                                new_qa_items = await page.query_selector_all("ul#faqList li")
                                if new_qa_items:
                                    logging.info(f"  í˜ì´ì§€ {page_num}ë¡œ ì´ë™ ì„±ê³µ (JavaScript ì§ì ‘ ì‹¤í–‰)")
                                else:
                                    logging.info(f"  í˜ì´ì§€ {page_num}ê°€ ì—†ì–´ ë‹¤ìŒ ìƒí’ˆìœ¼ë¡œ ì´ë™")
                                    break
                            except Exception as e:
                                logging.info(f"  í˜ì´ì§€ {page_num} JavaScript ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
                                break
                    except Exception as e:
                        logging.info(f"  í˜ì´ì§€ {page_num} ì´ë™ ì‹¤íŒ¨: {str(e)}")
                        break
                
                product_qa_count = len([qa for qa in all_qa_list if qa['product'] == product_name])
                logging.info(f"ìƒí’ˆ {product_idx + 1}/{len(product_buttons)} ì²˜ë¦¬ ì™„ë£Œ: {product_name} (FAQ {product_qa_count}ê°œ)")
            except Exception as e:
                logging.error(f"ìƒí’ˆ {product_idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                continue

        await browser.close()

    logging.info(f"ê¸°ê°€ì§€ë‹ˆ FAQ ì „ì²´ ì¶”ì¶œ ì™„ë£Œ: ì´ ìƒí’ˆ {len(product_buttons)}ê°œ, ì´ FAQ {len(all_qa_list)}ê°œ")
    logging.info(f"qa_list ì¤€ë¹„ ì™„ë£Œ: {len(all_qa_list)}ê°œ FAQ")
    
    return {
        "url": url,  # URL í•„ë“œ ì¶”ê°€ (url.txt ìƒì„±ìš©)
        "markdown": page_markdown,  # FAQ ì œì™¸í•œ ì¼ë°˜ í˜ì´ì§€ ë‚´ìš©
        "html": page_html,
        "qa_list": all_qa_list,  # FAQ ë°ì´í„°ë§Œ ë³„ë„ ì €ì¥
        "total_products": len(product_buttons) if 'product_buttons' in locals() else 0,
        "total_qa": len(all_qa_list),
        "special_processed": True,
        "playwright_processed": True
    }

register_page_handler(
    r'https?://gigagenie\.kt\.com/whyGenieFaq\.do',
    handle_gigagenie_faq_playwright
)
async def handle_gigagenie_news_list(url: str, fclient, menu: str = None) -> dict:
    """
    ê¸°ê°€ì§€ë‹ˆ ì§€ë‹ˆì†Œì‹ ëª©ë¡ Playwright í•¸ë“¤ëŸ¬
    - "ë”ë³´ê¸°" ë²„íŠ¼ì„ ëê¹Œì§€ í´ë¦­í•´ ì „ì²´ ê²Œì‹œë¬¼ì„ ë…¸ì¶œ
    - ëª©ë¡ì—ì„œ seq, ì œëª©ì„ ì¶”ì¶œí•´ ìƒì„¸ URLì„ êµ¬ì„±
    - ì…ë ¥ menu ê°’ì— ê²Œì‹œë¬¼ ì œëª©ì„ ë¶™ì—¬ menu^{title} í˜•íƒœë¡œ ë©”ë‰´ ê²½ë¡œ êµ¬ì„±
    - ê° ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œëª©, ë‚ ì§œ(startdate), ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ì—¬ Markdown/HTML ìƒì„±
    """
    import logging
    import re
    import asyncio
    from markdownify import markdownify as md
    from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError

    logging.info(f"ê¸°ê°€ì§€ë‹ˆ ì§€ë‹ˆì†Œì‹ ëª©ë¡ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, menu={menu}")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        response = await page.goto(url, wait_until="domcontentloaded", timeout=40000)
        await page.wait_for_timeout(4000)

        status_code = response.status if response else None
        if status_code:
            if status_code >= 400:
                logging.error(f"âŒ ê¸°ê°€ì§€ë‹ˆ ì§€ë‹ˆì†Œì‹ ëª©ë¡ ({url}): HTTP {status_code} ì˜¤ë¥˜")
            elif status_code >= 300:
                logging.warning(f"âš ï¸ ê¸°ê°€ì§€ë‹ˆ ì§€ë‹ˆì†Œì‹ ëª©ë¡ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
            else:
                logging.info(f"âœ… ê¸°ê°€ì§€ë‹ˆ ì§€ë‹ˆì†Œì‹ ëª©ë¡ ({url}): HTTP {status_code} ì„±ê³µ")
        else:
            logging.debug(f"ğŸ” ê¸°ê°€ì§€ë‹ˆ ì§€ë‹ˆì†Œì‹ ëª©ë¡ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")

        load_more_selector = "button#btn_more"
        try:
            while True:
                load_more_button = await page.query_selector(load_more_selector)
                if not load_more_button:
                    logging.info("ë”ë³´ê¸° ë²„íŠ¼ì´ ì—†ì–´ ëª¨ë“  ê²Œì‹œë¬¼ì´ ë…¸ì¶œëœ ê²ƒìœ¼ë¡œ íŒë‹¨")
                    break
                # is_visible ì²´í¬ë¥¼ ì¶”ê°€í•˜ì—¬ display:none ìƒíƒœì—ì„œ ë¶ˆí•„ìš”í•œ í´ë¦­ ì‹œë„ë¥¼ ë°©ì§€
                if not await load_more_button.is_visible():
                    logging.info("ë”ë³´ê¸° ë²„íŠ¼ì´ ìˆ¨ê²¨ì ¸ ìˆì–´ ë¡œë”© ì™„ë£Œë¡œ íŒë‹¨")
                    break
                if not await load_more_button.is_enabled():
                    logging.info("ë”ë³´ê¸° ë²„íŠ¼ì´ ë¹„í™œì„±í™”ë˜ì–´ ë¡œë”© ì™„ë£Œ")
                    break
                try:
                    await load_more_button.click()
                except PlaywrightTimeoutError:
                    logging.warning("ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì¤‘ íƒ€ì„ì•„ì›ƒ ë°œìƒ â†’ ë¡œë”© ì™„ë£Œë¡œ íŒë‹¨")
                    break
                logging.info("ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ â†’ ì¶”ê°€ ê²Œì‹œë¬¼ ë¡œë”© ëŒ€ê¸°")
                await page.wait_for_timeout(2000)
        except PlaywrightTimeoutError as timeout_err:
            logging.warning(f"ë”ë³´ê¸° ë²„íŠ¼ ì²˜ë¦¬ ì¤‘ íƒ€ì„ì•„ì›ƒ ë°œìƒ: {str(timeout_err)}")
        except Exception as e:
            logging.warning(f"ë”ë³´ê¸° ë²„íŠ¼ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")

        card_selector = "ul#bloglist li"
        try:
            await page.wait_for_selector(card_selector, timeout=5000)
        except PlaywrightTimeoutError:
            logging.warning("ì§€ë‹ˆì†Œì‹ ì¹´ë“œê°€ ì¼ì • ì‹œê°„ ë‚´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (íƒ€ì„ì•„ì›ƒ)")
        except Exception as wait_err:
            logging.warning(f"ì§€ë‹ˆì†Œì‹ ì¹´ë“œ ëŒ€ê¸° ì¤‘ ì˜ˆì™¸ ë°œìƒ: {wait_err}")
        cards = await page.query_selector_all(card_selector)
        logging.info(f"ì§€ë‹ˆì†Œì‹ ì¹´ë“œ {len(cards)}ê°œ ë°œê²¬")

        base_menu = menu or "ì§€ë‹ˆì†Œì‹"
        datas = []
        menus = []

        semaphore = asyncio.Semaphore(5)

        async def process_detail(detail_url: str, parent_menu: str, original_idx: int):
            async with semaphore:
                detail_page = await browser.new_page()

                try:
                    detail_response = await detail_page.goto(detail_url, wait_until="domcontentloaded", timeout=40000)
                    await detail_page.wait_for_timeout(3000)

                    detail_status = detail_response.status if detail_response else None
                    if detail_status:
                        if detail_status >= 400:
                            logging.error(f"âŒ ì§€ë‹ˆì†Œì‹ ìƒì„¸ ({detail_url}): HTTP {detail_status} ì˜¤ë¥˜")
                        elif detail_status >= 300:
                            logging.warning(f"âš ï¸ ì§€ë‹ˆì†Œì‹ ìƒì„¸ ({detail_url}): HTTP {detail_status} ë¦¬ë‹¤ì´ë ‰íŠ¸")
                        else:
                            logging.info(f"âœ… ì§€ë‹ˆì†Œì‹ ìƒì„¸ ({detail_url}): HTTP {detail_status} ì„±ê³µ")

                    title_selector = "h3.cfmOllehNewsTitle div.inner"
                    date_selector = "h3.cfmOllehNewsTitle div.inner span.date"

                    title_element = await detail_page.query_selector(title_selector)
                    raw_title = (await title_element.inner_text()) if title_element else ""
                    title_clean = re.sub(r"\s+", " ", raw_title).strip()

                    date_element = await detail_page.query_selector(date_selector)
                    raw_date = (await date_element.inner_text()) if date_element else ""
                    date_text = raw_date.strip()

                    startdate = "0000-00-00"
                    if date_text:
                        m = re.match(r"(\d{2})\.(\d{2})\.(\d{2})", date_text)
                        if m:
                            year = int(m.group(1))
                            year += 2000 if year < 70 else 1900
                            startdate = f"{year}-{m.group(2)}-{m.group(3)}"

                    content_selectors = [
                        "div.cfmOllehNewsCont",
                        "div.fjbNewsArea",
                        "div[style*='background']"
                    ]
                    inner_html = ""
                    for selector in content_selectors:
                        elem = await detail_page.query_selector(selector)
                        if elem:
                            inner_html = await elem.inner_html()
                            if inner_html and inner_html.strip():
                                break

                    if not inner_html:
                        logging.warning(f"ë³¸ë¬¸ ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {detail_url}")

                    markdown_content = md(inner_html, heading_style="ATX") if inner_html else ""
                    html_content = inner_html or ""

                    title_for_menu = sanitize_filename(title_clean) if title_clean else "ì§€ë‹ˆì†Œì‹"
                    final_menu = f"{parent_menu}^{title_for_menu}" if parent_menu else title_for_menu

                    datas.append({
                        "url": detail_url,
                        "title": title_clean,
                        "date": date_text,
                        "startdate": startdate,
                        "markdown": markdown_content,
                        "html": html_content,
                        "status_code": detail_status,
                        "special_processed": True,
                        "playwright_processed": True,
                        "murl": to_gigagenie_murl(detail_url),
                        "original_index": original_idx
                    })

                    menus.append({
                        "menu": final_menu,
                        "url": detail_url,
                        "mobile_url": detail_url,
                        "murl": to_gigagenie_murl(detail_url),
                        "original_index": original_idx
                    })

                    logging.info(f"ì§€ë‹ˆì†Œì‹ ìƒì„¸ ì¶”ì¶œ ì™„ë£Œ: title='{title_clean}', startdate='{startdate}'")

                except Exception as detail_err:
                    logging.error(f"ì§€ë‹ˆì†Œì‹ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ({detail_url}): {str(detail_err)}")
                    datas.append({
                        "url": detail_url,
                        "title": "",
                        "date": "",
                        "startdate": "0000-00-00",
                        "markdown": "",
                        "html": "",
                        "error": str(detail_err),
                        "special_processed": True,
                        "playwright_processed": True,
                        "murl": to_gigagenie_murl(detail_url),
                        "original_index": original_idx
                    })
                    menus.append({
                        "menu": parent_menu,
                        "url": detail_url,
                        "mobile_url": detail_url,
                        "murl": to_gigagenie_murl(detail_url),
                        "original_index": original_idx
                    })
                finally:
                    await detail_page.close()

        for idx, card in enumerate(cards):
            try:
                thumbnail_link = await card.query_selector("a.thumbnail")
                if not thumbnail_link:
                    logging.warning("ì¸ë„¤ì¼ ë§í¬ê°€ ì—†ì–´ ì¹´ë“œ ê±´ë„ˆëœ€")
                    continue

                onclick_attr = await thumbnail_link.get_attribute("onclick") or ""
                seq_match = re.search(r"goDetPage\((\d+)\)", onclick_attr)
                seq = seq_match.group(1) if seq_match else None

                if seq:
                    detail_url = f"https://gigagenie.kt.com/blog/detail.do?seq={seq}"
                else:
                    href_attr = await thumbnail_link.get_attribute("href") or ""
                    if href_attr.startswith("http"):
                        detail_url = href_attr
                    else:
                        detail_url = f"https://gigagenie.kt.com{href_attr}" if href_attr else ""

                if not detail_url:
                    logging.warning("ìƒì„¸ URLì„ êµ¬ì„±í•  ìˆ˜ ì—†ì–´ ì¹´ë“œ ê±´ë„ˆëœ€")
                    continue

                await process_detail(detail_url, base_menu, idx)

            except Exception as card_err:
                logging.error(f"ì§€ë‹ˆì†Œì‹ ì¹´ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(card_err)}")
                continue
        await browser.close()

    logging.info(f"ê¸°ê°€ì§€ë‹ˆ ì§€ë‹ˆì†Œì‹ ëª©ë¡ ì²˜ë¦¬ ì™„ë£Œ: ì´ {len(datas)}ê°œ ê²Œì‹œë¬¼")

    return {
        "menus": menus,
        "datas": datas,
        "total_processed": len(datas),
        "status": "completed",
        "message": f"ì´ {len(datas)}ê°œ ì§€ë‹ˆì†Œì‹ ê²Œì‹œë¬¼ ì²˜ë¦¬ ì™„ë£Œ",
        "special_processed": True,
        "playwright_processed": True
    }

register_page_handler(
    r'https?://gigagenie\.kt\.com/whyGenieNews\.do',
    handle_gigagenie_news_list
)
# =========================
# 8. ê³ ê°ë¬¸ì˜ FAQ ì „ì²´ í˜ì´ì§€ ì¶”ì¶œ í•¸ë“¤ëŸ¬
# =========================

async def handle_membership_faq_all_playwright(url: str, fclient, menu=None) -> dict:
    """
    KT ë©¤ë²„ì‹­ FAQ í˜ì´ì§€ì—ì„œ iframeì„ í†µí•´ ëª¨ë“  FAQ Q/Aë¥¼ ì¶”ì¶œí•˜ëŠ” handler
    ë©”ì¸ í˜ì´ì§€ -> iframe ì ‘ê·¼ -> FAQ ë°ì´í„° ì¶”ì¶œ
    """
    logging.info(f"KT ë©¤ë²„ì‹­ FAQ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}")
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        response = await page.goto(url, wait_until="domcontentloaded", timeout=40000)
        await page.wait_for_timeout(3000)
        
        # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
        status_code = response.status if response else None
        if status_code:
            if status_code >= 400:
                logging.error(f"âŒ ë©¤ë²„ì‹­ FAQ ({url}): HTTP {status_code} ì˜¤ë¥˜")
            elif status_code >= 300:
                logging.warning(f"âš ï¸ ë©¤ë²„ì‹­ FAQ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
            else:
                logging.info(f"âœ… ë©¤ë²„ì‹­ FAQ ({url}): HTTP {status_code} ì„±ê³µ")
        else:
            logging.debug(f"ğŸ” ë©¤ë²„ì‹­ FAQ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")

        markdown_body = ""
        all_qa_list = []
        seen_questions = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì§ˆë¬¸ ì¶”ì 
        
        # ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ (FAQ ì œì™¸) - Playwright ì‚¬ìš©
        try:
            logging.info("ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì‹œì‘")
            from markdownify import markdownify as md
            
            # FAQ ê´€ë ¨ ìš”ì†Œë“¤ ë° ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            faq_selectors = [
                # FAQ ê´€ë ¨
                'ul#faqList', '.faqList', 
                '.accordion-area', '.accordion',
                '.faq_box', '.faq', '.faq-list', '.faq-item', 
                '.inquiry', '.answer', '.faqClass',
                'img[src*="faq"]', 'img[src*="FAQ"]',
                'a[href*="faq"]', 'a[href*="FAQ"]',
                'iframe#cpEvent',  # FAQ iframeë„ ì œê±°
                # Header/Footer/Navigation
                'header', 'footer', 
                '.header', '.footer',
                '#header', '#footer',
                '#cfmClHeader', '#cfmClFooter',
                '.inner', 'nav', '.navigation'
            ]
            
            for selector in faq_selectors:
                try:
                    elements = await page.query_selector_all(selector)
                    for element in elements:
                        await element.evaluate('element => element.remove()')
                except:
                    pass  # ì…€ë ‰í„°ê°€ ìœ íš¨í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
            
            page_html = await page.content()
            page_markdown = md(page_html) if page_html else ""
            logging.info(f"ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì™„ë£Œ: ë§ˆí¬ë‹¤ìš´ {len(page_markdown)}ì, HTML {len(page_html)}ì")
        except Exception as e:
            logging.error(f"ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            page_markdown = ""
            page_html = ""
        
        # FAQ ì¶”ì¶œì„ ìœ„í•´ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
        await page.reload()
        await page.wait_for_timeout(3000)

        # iframe ì°¾ê¸° ë° ì ‘ê·¼
        iframe_selector = "iframe#cpEvent"
        iframe_element = await page.query_selector(iframe_selector)
        
        if not iframe_element:
            logging.error("FAQ iframeì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            await browser.close()
            return {
                "markdown": "",
                "qa_list": [],
                "total_categories": 0,
                "total_qa": 0,
                "special_processed": True,
                "playwright_processed": True,
                "error": "iframeì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
            }

        # iframeì˜ ì‹¤ì œ src í™•ì¸
        iframe_src = await iframe_element.get_attribute("src")
        logging.info(f"ë°œê²¬ëœ iframe src: {iframe_src}")

        # iframeì˜ frame ê°ì²´ ê°€ì ¸ì˜¤ê¸°
        frame = await iframe_element.content_frame()
        if not frame:
            logging.error("iframeì˜ frame ì»¨í…ì¸ ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            await browser.close()
            return {
                "markdown": "",
                "qa_list": [],
                "total_categories": 0,
                "total_qa": 0,
                "special_processed": True,
                "playwright_processed": True,
                "error": "iframe frameì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŒ"
            }
        
        # iframe ë‚´ìš©ì´ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        try:
            await frame.wait_for_load_state('domcontentloaded', timeout=30000)
            await frame.wait_for_load_state('networkidle', timeout=30000)
            logging.info("iframe ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            logging.warning(f"iframe ë¡œë”© ëŒ€ê¸° ì¤‘ íƒ€ì„ì•„ì›ƒ: {str(e)}")
            # ê³„ì† ì§„í–‰

        logging.info("iframe ì ‘ê·¼ ì„±ê³µ, FAQ ë°ì´í„° ì¶”ì¶œ ì‹œì‘")
        await page.wait_for_timeout(5000)  # iframe ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¦ê°€



        await frame.wait_for_timeout(2000)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

        # í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•œ ëª¨ë“  í˜ì´ì§€ ì²˜ë¦¬
        page_num = 1
        max_pages = 100  # ì¶©ë¶„í•œ ìµœëŒ€ í˜ì´ì§€ ì œí•œ
        visited_first_questions = set()  # ìˆœí™˜ ê°ì§€ë¥¼ ìœ„í•œ ì²« ë²ˆì§¸ ì§ˆë¬¸ ì¶”ì 
        
        while page_num <= max_pages:
            logging.info(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘...")
            
            # í˜„ì¬ í˜ì´ì§€ì˜ accordion FAQ í•­ëª©ë“¤ ì¶”ì¶œ (iframe ë‚´ì—ì„œ)
            accordion_triggers = await frame.query_selector_all('.accordion-trigger')
            
            logging.info(f"í˜ì´ì§€ {page_num}ì—ì„œ {len(accordion_triggers)}ê°œ accordion FAQ í•­ëª© ë°œê²¬")
            
            if not accordion_triggers:
                logging.info("ë” ì´ìƒ FAQ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            # ìˆœí™˜ ê°ì§€: ì²« ë²ˆì§¸ ì§ˆë¬¸ìœ¼ë¡œ ì´ë¯¸ ë°©ë¬¸í•œ í˜ì´ì§€ì¸ì§€ í™•ì¸
            try:
                first_trigger = accordion_triggers[0]
                first_question_element = await first_trigger.query_selector('.qna span')
                if first_question_element:
                    first_question = await first_question_element.inner_text()
                    if first_question.strip() in visited_first_questions:
                        logging.info(f"ìˆœí™˜ ê°ì§€: ì´ë¯¸ ì²˜ë¦¬í•œ í˜ì´ì§€ (ì²« ë²ˆì§¸ ì§ˆë¬¸: '{first_question[:50]}...')")
                        break
                    visited_first_questions.add(first_question.strip())
            except Exception as e:
                logging.warning(f"ìˆœí™˜ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            

            
            # ê° accordion FAQ í•­ëª© ì²˜ë¦¬
            for idx, trigger in enumerate(accordion_triggers):
                try:
                    # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (.linked í´ë˜ìŠ¤)
                    category_element = await trigger.query_selector('.linked')
                    category = await category_element.inner_text() if category_element else "ê¸°íƒ€"
                    
                    # ì§ˆë¬¸ ì¶”ì¶œ (.qna span)
                    question_element = await trigger.query_selector('.qna span')
                    question = await question_element.inner_text() if question_element else ""
                        
                    # ë‹µë³€ ì¶”ì¶œ (accordion í´ë¦­ í•„ìš”)
                    answer = ""
                    try:
                        # accordionì„ í´ë¦­í•˜ì—¬ ë‹µë³€ ë¡œë“œ
                        await trigger.click()
                        await frame.wait_for_timeout(1000)  # ë‹µë³€ ë¡œë”© ëŒ€ê¸°
                        
                        # í•´ë‹¹í•˜ëŠ” ë‹µë³€ ìš”ì†Œ ì°¾ê¸°
                        answer_id = f"accordionsAnswer-{idx}"
                        answer_element = await frame.query_selector(f'#{answer_id}')
                        
                        if answer_element:
                            answer = await answer_element.inner_text()
                            answer = answer.strip()
                        
                    except Exception as e:
                        logging.warning(f"FAQ ë‹µë³€ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    
                    # ì¤‘ë³µ ì²´í¬ í›„ ìœ íš¨í•œ Q/Aë§Œ ì¶”ê°€
                    if question.strip() and question.strip() not in seen_questions:
                        seen_questions.add(question.strip())  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì¶”ê°€
                        
                        # êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œë§Œ ì¶”ê°€ (ë§ˆí¬ë‹¤ìš´ ì œê±°)
                        all_qa_list.append({
                            "category": category,
                            "question": question.strip(),
                            "answer": answer.strip(),
                            "page": page_num
                        })
                        
                        logging.info(f"í˜ì´ì§€ {page_num} FAQ {idx + 1} ì¶”ì¶œ ì™„ë£Œ: {question[:50]}...")
                    elif question.strip():
                        logging.info(f"í˜ì´ì§€ {page_num} ì¤‘ë³µ FAQ ê±´ë„ˆëœ€: {question[:50]}...")
                    else:
                        logging.warning(f"í˜ì´ì§€ {page_num} FAQ {idx + 1} ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŒ")
                        
                except Exception as e:
                    logging.error(f"FAQ í•­ëª© {idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    continue
            
            # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ ì‹œë„ (ë™ì  í˜ì´ì§€ë„¤ì´ì…˜)
            try:
                logging.info("ë‹¤ìŒ í˜ì´ì§€ ë§í¬ ì°¾ëŠ” ì¤‘...")
                
                # í˜„ì¬ í˜ì´ì§€ì˜ ì²« ë²ˆì§¸ ì§ˆë¬¸ì„ ê¸°ì–µ (ì´ë™ í™•ì¸ìš©)
                current_first_question = ""
                try:
                    first_trigger = await frame.query_selector('.accordion-trigger .qna span')
                    if first_trigger:
                        current_first_question = await first_trigger.inner_text()
                except:
                    pass
                
                # í˜ì´ì§€ë„¤ì´ì…˜ ì˜ì—­ì—ì„œ ëª¨ë“  ë§í¬ í™•ì¸
                pagination_links = await frame.query_selector_all('a')
                
                next_link = None
                
                # í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ íŒŒì•… ë° ë‹¤ìŒ í˜ì´ì§€ ì°¾ê¸°
                current_page_num = page_num  # í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ (ì¹´ìš´í„°)
                next_page_num = current_page_num + 1
                
                # í˜„ì¬ í˜ì´ì§€ì™€ ë‹¤ìŒ í˜ì´ì§€ ë§í¬ ì°¾ê¸°
                for link in pagination_links:
                    try:
                        link_text = (await link.inner_text()).strip()
                        if link_text.isdigit():
                            page_number = int(link_text)
                            # í˜„ì¬ í˜ì´ì§€ë³´ë‹¤ í° ì²« ë²ˆì§¸ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
                            if page_number > current_page_num and await link.is_enabled() and await link.is_visible():
                                next_link = link
                                logging.info(f"ë‹¤ìŒ í˜ì´ì§€ ë§í¬ ë°œê²¬: '{link_text}' (í˜„ì¬: {current_page_num})")
                                break
                    except Exception as e:
                        continue
                
                # ìˆ«ì í˜ì´ì§€ê°€ ì—†ìœ¼ë©´ >> (10í˜ì´ì§€ ì´ë™) ë˜ëŠ” >>| (ëìœ¼ë¡œ) ì°¾ê¸°
                if not next_link:
                    for link in pagination_links:
                        try:
                            link_text = (await link.inner_text()).strip()
                            if link_text in ['>>', '>>|', 'ë‹¤ìŒ', 'Next'] and await link.is_enabled():
                                next_link = link
                                logging.info(f"í˜ì´ì§€ ì´ë™ ë§í¬ ë°œê²¬: '{link_text}'")
                                break
                        except Exception as e:
                            continue
                
                if next_link:
                    logging.info("í˜ì´ì§€ ì´ë™ ì‹œë„")
                    await next_link.click()
                    await frame.wait_for_timeout(4000)  # ì¶©ë¶„í•œ ëŒ€ê¸° ì‹œê°„
                    
                    # í˜ì´ì§€ ì´ë™ í™•ì¸
                    page_changed = False
                    try:
                        new_first_trigger = await frame.query_selector('.accordion-trigger .qna span')
                        if new_first_trigger:
                            new_first_question = await new_first_trigger.inner_text()
                            if new_first_question != current_first_question:
                                page_changed = True
                                logging.info(f"í˜ì´ì§€ ì´ë™ í™•ì¸ë¨: '{current_first_question[:30]}...' â†’ '{new_first_question[:30]}...'")
                    except:
                        pass
                    
                    if not page_changed:
                        logging.warning("í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨ - ê°™ì€ ë‚´ìš©")
                        break
                    
                    await frame.wait_for_timeout(1000)
                    
                    page_num += 1  # í˜ì´ì§€ ë²ˆí˜¸ëŠ” ë‹¨ìˆœíˆ ì¹´ìš´í„°ë¡œë§Œ ì‚¬ìš©
                else:
                    logging.info("ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break
                    
            except Exception as e:
                logging.error(f"í˜ì´ì§€ ì´ë™ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                break

        await browser.close()
        
        logging.info(f"FAQ ì¶”ì¶œ ì™„ë£Œ: ì´ {len(all_qa_list)}ê°œ FAQ")
        logging.info(f"qa_list ì¤€ë¹„ ì™„ë£Œ: {len(all_qa_list)}ê°œ FAQ")

    return {
        "url": url,  # URL í•„ë“œ ì¶”ê°€ (url.txt ìƒì„±ìš©)
        "markdown": page_markdown,  # FAQ ì œì™¸í•œ ì¼ë°˜ í˜ì´ì§€ ë‚´ìš©
        "html": page_html,
        "qa_list": all_qa_list,
        "total_categories": 1,  # ë‹¨ì¼ í˜ì´ì§€ì—ì„œ ì¶”ì¶œ
        "total_qa": len(all_qa_list),
        "special_processed": True,
        "playwright_processed": True
    }

# í•¸ë“¤ëŸ¬ ë“±ë¡ - ë©”ì¸ ì£¼ì†Œë¡œ ë“±ë¡
register_page_handler(
    r'https?://membership\.kt\.com/guide/faq/FAQList\.do',
    handle_membership_faq_all_playwright
)

# =========================
# 8. KT ì´ë²¤íŠ¸ ê´€ë ¨ í•¸ë“¤ëŸ¬
# =========================

async def handle_kt_event_main(url: str, fclient, menu=None) -> dict:
    logging.info(f"KT ì´ë²¤íŠ¸ ë©”ì¸ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, menu={menu}")
    """
    KT ì´ë²¤íŠ¸ ë©”ì¸ í˜ì´ì§€ í•¸ë“¤ëŸ¬
    https://event.kt.com/html/event/ongoing_event_list.html
    """
    from playwright.async_api import async_playwright
    import re
    from datetime import datetime
    
    logging.info(f"ğŸ¯ KT ì´ë²¤íŠ¸ ë©”ì¸ í˜ì´ì§€ ì²˜ë¦¬: {url}")
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        
        try:
            response = await page.goto(url, wait_until='domcontentloaded', timeout=60000)
            await page.wait_for_timeout(3000)
            
            # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
            status_code = response.status if response else None
            if status_code:
                if status_code >= 400:
                    logging.error(f"âŒ KT ì´ë²¤íŠ¸ ë©”ì¸ ({url}): HTTP {status_code} ì˜¤ë¥˜")
                elif status_code >= 300:
                    logging.warning(f"âš ï¸ KT ì´ë²¤íŠ¸ ë©”ì¸ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
                else:
                    logging.info(f"âœ… KT ì´ë²¤íŠ¸ ë©”ì¸ ({url}): HTTP {status_code} ì„±ê³µ")
            else:
                logging.debug(f"ğŸ” KT ì´ë²¤íŠ¸ ë©”ì¸ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")
            
            # í˜ì´ì§€ë„¤ì´ì…˜ ì •ë³´ ì¶”ì¶œ
            pagination_info = await page.evaluate("""() => {
                const pagination = document.querySelector('.pagination');
                if (!pagination) return { total_pages: 1, current_page: 1 };
                
                const pageLinks = pagination.querySelectorAll('a[data-page]');
                let maxPage = 1;
                pageLinks.forEach(link => {
                    const pageNum = parseInt(link.getAttribute('data-page'));
                    if (pageNum > maxPage) maxPage = pageNum;
                });
                
                const currentPageElem = pagination.querySelector('span[title="í˜„ì¬ìœ„ì¹˜"], .current');
                const currentPage = currentPageElem ? parseInt(currentPageElem.textContent) : 1;
                
                return { total_pages: maxPage, current_page: currentPage };
            }""")
            
            all_events = []
            total_pages = pagination_info.get('total_pages', 1)
            
            # ëª¨ë“  í˜ì´ì§€ ìˆœíšŒ
            for page_num in range(1, total_pages + 1):
                if page_num > 1:
                    logging.info(f"ğŸ”„ í˜ì´ì§€ {page_num}ë¡œ ì´ë™ ì‹œë„ ì¤‘...")
                    
                    # í˜ì´ì§€ ì´ë™ ì‹œë„
                    await page.evaluate(f"""() => {{
                        const pageLinks = document.querySelectorAll('a[data-page="{page_num}"]');
                        if (pageLinks.length > 0) {{
                            pageLinks[0].click();
                        }}
                    }}""")
                    
                    await page.wait_for_timeout(2000)
                    
                    # í˜ì´ì§€ ì´ë™ ì„±ê³µ ì—¬ë¶€ í™•ì¸
                    current_page = await page.evaluate("""() => {
                        const pagination = document.querySelector('.pagination');
                        if (!pagination) return 1;
                        const currentPageElem = pagination.querySelector('span[title="í˜„ì¬ìœ„ì¹˜"], .current');
                        return currentPageElem ? parseInt(currentPageElem.textContent) : 1;
                    }""")
                    
                    if current_page == page_num:
                        logging.info(f"âœ… í˜ì´ì§€ {page_num}ë¡œ ì´ë™ ì„±ê³µ")
                    else:
                        logging.warning(f"âš ï¸ í˜ì´ì§€ {page_num}ë¡œ ì´ë™ ì‹¤íŒ¨ (í˜„ì¬: {current_page})")
                        
                        # ì¬ì‹œë„ ë¡œì§
                        max_retries = 3
                        for retry in range(1, max_retries + 1):
                            logging.info(f"ğŸ”„ í˜ì´ì§€ {page_num} ì´ë™ ì¬ì‹œë„ {retry}/{max_retries}")
                            
                            await page.evaluate(f"""() => {{
                                const pageLinks = document.querySelectorAll('a[data-page="{page_num}"]');
                                if (pageLinks.length > 0) {{
                                    pageLinks[0].click();
                                }}
                            }}""")
                            
                            await page.wait_for_timeout(3000)
                            
                            retry_current_page = await page.evaluate("""() => {
                                const pagination = document.querySelector('.pagination');
                                if (!pagination) return 1;
                                const currentPageElem = pagination.querySelector('span[title="í˜„ì¬ìœ„ì¹˜"], .current');
                                return currentPageElem ? parseInt(currentPageElem.textContent) : 1;
                            }""")
                            
                            if retry_current_page == page_num:
                                logging.info(f"âœ… í˜ì´ì§€ {page_num} ì´ë™ ì¬ì‹œë„ ì„±ê³µ")
                                break
                            else:
                                logging.warning(f"âš ï¸ í˜ì´ì§€ {page_num} ì´ë™ ì¬ì‹œë„ {retry} ì‹¤íŒ¨ (í˜„ì¬: {retry_current_page})")
                        
                        # ìµœì¢… í™•ì¸
                        final_page = await page.evaluate("""() => {
                            const pagination = document.querySelector('.pagination');
                            if (!pagination) return 1;
                            const currentPageElem = pagination.querySelector('span[title="í˜„ì¬ìœ„ì¹˜"], .current');
                            return currentPageElem ? parseInt(currentPageElem.textContent) : 1;
                        }""")
                        
                        if final_page != page_num:
                            logging.error(f"âŒ í˜ì´ì§€ {page_num}ë¡œ ì´ë™ ìµœì¢… ì‹¤íŒ¨ (í˜„ì¬: {final_page})")
                            continue
                else:
                    logging.info(f"ğŸ“„ ì²« ë²ˆì§¸ í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...")
                
                # í˜„ì¬ í˜ì´ì§€ì˜ ì´ë²¤íŠ¸ ì¶”ì¶œ
                page_events = await page.evaluate("""() => {
                    const events = [];
                    const eventLinks = document.querySelectorAll('a[data-pcevtno]');
                    
                    eventLinks.forEach(link => {
                        const evtNo = link.getAttribute('data-pcevtno');
                        const apctUrl = link.getAttribute('data-apcturl');
                        const linkType = link.getAttribute('data-pcevtlinktype');
                        
                        // ì¸ë„¤ì¼ ì •ë³´
                        const thumb = link.querySelector('.thumb');
                        const img = thumb ? thumb.querySelector('img') : null;
                        const dDay = thumb ? thumb.querySelector('.d-day') : null;
                        
                        // ìš”ì•½ ì •ë³´
                        const summary = link.querySelector('.summary');
                        const title = summary ? summary.querySelector('.title') : null;
                        const date = summary ? summary.querySelector('.date') : null;
                        const type = summary ? summary.querySelector('.type') : null;
                        
                        events.push({
                            evt_no: evtNo,
                            apct_url: apctUrl,
                            link_type: linkType,
                            title: title ? title.textContent.trim() : '',
                            date: date ? date.textContent.trim() : '',
                            type: type ? type.textContent.trim() : '',
                            img_src: img ? img.getAttribute('src') : '',
                            img_alt: img ? img.getAttribute('alt') : '',
                            d_day: dDay ? dDay.textContent.trim() : '',
                            full_href: link.href || ''
                        });
                    });
                    
                    return events;
                }""")
                
                all_events.extend(page_events)
                logging.info(f"ğŸ“„ í˜ì´ì§€ {page_num}/{total_pages} ì²˜ë¦¬ ì™„ë£Œ: {len(page_events)}ê°œ ì´ë²¤íŠ¸")
            
            # ì§„ì… í˜ì´ì§€(ëª©ë¡ í˜ì´ì§€) ìì²´ë„ ì¶”ì¶œ
            logging.info(f"ğŸ“„ ì§„ì… í˜ì´ì§€ ì¶”ì¶œ ì‹œì‘")
            entry_page_html = await page.content()
            
            # markdownifyë¡œ ë³€í™˜
            from markdownify import markdownify as md_convert
            entry_page_markdown = md_convert(entry_page_html, heading_style="ATX")
            
            # ì§„ì… í˜ì´ì§€ ë°ì´í„° êµ¬ì„±
            entry_page_data = {
                "markdown": entry_page_markdown,
                "html": entry_page_html,
                "url": url,
                "metadata": {
                    "title": f"{menu or 'KT ì´ë²¤íŠ¸'} ëª©ë¡",
                    "is_entry_page": True,
                    "total_events": len(all_events),
                    "total_pages": total_pages,
                    "original_url": url,
                    "special_processed": True,
                    "playwright_processed": True
                }
            }
            logging.info(f"âœ… ì§„ì… í˜ì´ì§€ ì¶”ì¶œ ì™„ë£Œ")
            
            await browser.close()
            
            # ê° ì´ë²¤íŠ¸ì˜ ìƒì„¸ í˜ì´ì§€ë¥¼ ì²˜ë¦¬í•˜ì—¬ ê°œë³„ ê²Œì‹œë¬¼ë¡œ êµ¬ì„±
            individual_posts = [entry_page_data]  # ì§„ì… í˜ì´ì§€ë¥¼ ì²« ë²ˆì§¸ë¡œ ì¶”ê°€
            logging.info(f"ì´ {len(all_events)}ê°œ ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹œì‘")
            
            for i, event in enumerate(all_events, 1):
                try:
                    logging.info(f"{i}/{len(all_events)}ë²ˆì§¸ ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹œì‘: '{event['title']}' (evt_no: {event['evt_no']})")
                    # ìƒì„¸ í˜ì´ì§€ URL êµ¬ì„±
                    detail_url = f"https://event.kt.com/html/event/ongoing_event_view.html?page=1&searchCtg=ALL&sort=&pcEvtNo={event['evt_no']}"
                    
                    # ìƒì„¸ í˜ì´ì§€ í•¸ë“¤ëŸ¬ í˜¸ì¶œ
                    detail_result = await handle_kt_event_detail(detail_url, fclient, menu)
                    
                    if detail_result and "datas" in detail_result and detail_result["datas"]:
                        # ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•œ ë°ì´í„° ì‚¬ìš©
                        individual_post = detail_result["datas"][0]
                        # ì¶”ê°€ ë©”íƒ€ë°ì´í„° ë³‘í•©
                        individual_post["metadata"].update({
                            "evt_no": event['evt_no'],
                            "original_url": url,
                            "post_index": i,
                            "total_posts": len(all_events)
                        })
                        # ìƒì„¸ íƒ€ì´í‹€ë¡œ ë³´ì • (ë©”ë‰´ëª…ì´ ... ë¡œ ì˜ë¦¬ëŠ” ë¬¸ì œ ì˜ˆë°©)
                        detail_title = individual_post["metadata"].get('title', '').strip()
                        if detail_title:
                            event['title'] = detail_title
                        logging.info(f"{i}/{len(all_events)}ë²ˆì§¸ ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì„±ê³µ: '{event['title']}'")
                    else:
                        # ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ëª©ë¡ ì •ë³´ë¡œ fallback
                        logging.warning(f"{i}/{len(all_events)}ë²ˆì§¸ ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨, ëª©ë¡ ì •ë³´ë¡œ fallback: '{event['title']}'")
                        individual_post = {
                            "markdown": f"# {event['title']}\n\n{event['evt_no']}\n{event['date']}\n{event['type']}\n{event['d_day']}\n{event['apct_url']}",
                            "html": f"<h1>{event['title']}</h1><p>{event['evt_no']}</p><p>{event['date']}</p><p>{event['type']}</p><p>{event['d_day']}</p><p><a href='{event['apct_url']}'>{event['apct_url']}</a></p>",
                            "url": detail_url,
                            "metadata": {
                                "title": event['title'],
                                "evt_no": event['evt_no'],
                                "period": event['date'],
                                "type": event['type'],
                                "d_day": event['d_day'],
                                "original_url": url,
                                "post_index": i,
                                "total_posts": len(all_events),
                                "detail_processing_failed": True
                            }
                        }
                    
                    individual_posts.append(individual_post)
                    logging.info(f"{i}/{len(all_events)}ë²ˆì§¸ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ: '{event['title']}'")
                    
                except Exception as e:
                    logging.error(f"{i}/{len(all_events)}ë²ˆì§¸ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: '{event['title']}', ì—ëŸ¬: {str(e)}")
                    # ì—ëŸ¬ ì‹œ ëª©ë¡ ì •ë³´ë¡œ fallback
                    individual_post = {
                        "markdown": f"# {event['title']}\n\n{event['evt_no']}\n{event['date']}\n{event['type']}\n{event['d_day']}\n{event['apct_url']}\n\nìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
                        "html": f"<h1>{event['title']}</h1><p>{event['evt_no']}</p><p>{event['date']}</p><p>{event['type']}</p><p>{event['d_day']}</p><p><a href='{event['apct_url']}'>{event['apct_url']}</a></p><p>ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}</p>",
                        "url": f"https://event.kt.com/html/event/ongoing_event_view.html?page=1&searchCtg=ALL&sort=&pcEvtNo={event['evt_no']}",
                        "metadata": {
                            "title": event['title'],
                            "evt_no": event['evt_no'],
                            "period": event['date'],
                            "type": event['type'],
                            "d_day": event['d_day'],
                            "original_url": url,
                            "post_index": i,
                            "total_posts": len(all_events),
                            "error": str(e)
                        }
                    }
                    individual_posts.append(individual_post)
            
            # menus ë°°ì—´ ìƒì„± (ë‹¤ë¥¸ í•¸ë“¤ëŸ¬ì™€ ë™ì¼í•œ íŒ¨í„´)
            menus = []
            
            # ì§„ì… í˜ì´ì§€ë¥¼ ì²« ë²ˆì§¸ ë©”ë‰´ë¡œ ì¶”ê°€
            menus.append({
                "menu": menu or "KT ì´ë²¤íŠ¸",
                "url": url,
                "mobile_url": url.replace('https://event.kt.com', 'https://m.kt.com')
            })
            
            def _to_m(u: str) -> str:
                import re as _re
                if not u:
                    return ""
                m = _re.search(r"pcEvtNo=(\d+)", u)
                if not m:
                    mobile = u.replace('https://event.kt.com', 'https://m.kt.com').replace('pcEvtNo=', 'mblevtno=')
                    if 'past_event_view.html' in mobile and 'rows=' not in mobile:
                        mobile += ('&' if ('?' in mobile) else '?') + 'rows=10'
                    return mobile
                pc_no = int(m.group(1))
                mb_no = pc_no + 1
                mobile = u.replace('https://event.kt.com', 'https://m.kt.com')
                mobile = _re.sub(r"pcEvtNo=\d+", f"mblevtno={mb_no}", mobile)
                if 'past_event_view.html' in mobile and 'rows=' not in mobile:
                    mobile += ('&' if ('?' in mobile) else '?') + 'rows=10'
                return mobile
            for event in all_events:
                view_url = f"https://event.kt.com/html/event/ongoing_event_view.html?page=1&searchCtg=ALL&sort=&pcEvtNo={event['evt_no']}"
                menus.append({
                    "menu": f"{menu}^{event['title']}",
                    "url": view_url,
                    "mobile_url": _to_m(view_url)
                })
            
            return {
                "datas": individual_posts,
                "menus": menus,
                "metadata": {
                    "title": "KT ì§„í–‰ì¤‘ì¸ ì´ë²¤íŠ¸",
                    "total_events": len(all_events),
                    "total_pages": total_pages,
                    "url": url,
                    "special_processed": True,
                    "playwright_processed": True
                }
            }
            
        except Exception as e:
            logging.error(f"âŒ KT ì´ë²¤íŠ¸ ë©”ì¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            await browser.close()
            return {
                "markdown": f"# KT ì´ë²¤íŠ¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨\n\nì˜¤ë¥˜: {str(e)}",
                "html": f"<h1>KT ì´ë²¤íŠ¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨</h1><p>ì˜¤ë¥˜: {str(e)}</p>",
                "datas": [],
                "error": str(e)
            }
async def handle_kt_event_detail(url: str, fclient, menu=None) -> dict:
    logging.info(f"KT ì´ë²¤íŠ¸ ìƒì„¸ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, menu={menu}")
    """
    KT ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ í•¸ë“¤ëŸ¬
    https://event.kt.com/html/event/ongoing_event_view.html?pcEvtNo=13532
    """
    from playwright.async_api import async_playwright
    import re
    
    logging.info(f"KT ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹œì‘: {url}")
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        
        try:
            logging.info(f"ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ì§„ì…: url={url}")
            response = await page.goto(url, wait_until='domcontentloaded', timeout=60000)
            await page.wait_for_timeout(3000)
            
            # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
            status_code = response.status if response else None
            if status_code:
                if status_code >= 400:
                    logging.error(f"âŒ KT ì´ë²¤íŠ¸ ìƒì„¸ ({url}): HTTP {status_code} ì˜¤ë¥˜")
                elif status_code >= 300:
                    logging.warning(f"âš ï¸ KT ì´ë²¤íŠ¸ ìƒì„¸ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
                else:
                    logging.info(f"âœ… KT ì´ë²¤íŠ¸ ìƒì„¸ ({url}): HTTP {status_code} ì„±ê³µ")
            else:
                logging.debug(f"ğŸ” KT ì´ë²¤íŠ¸ ìƒì„¸ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")
            
            # ì´ë²¤íŠ¸ ì •ë³´ ì¶”ì¶œ
            event_info = await page.evaluate("""() => {
                const info = {};
                
                // ì œëª©
                const titleElem = document.querySelector('#contents-title, .contents-title, h1, .title');
                if (titleElem) {
                    // SNS ê³µìœ  ë²„íŠ¼ ì œê±°
                    const snsButtons = titleElem.querySelectorAll('.btn-twitter, .btn-facebook, .btn-kakao, .btn-youtube, [class*="share"], [onclick*="share"], [href*="facebook"], [href*="twitter"], [href*="kakao"]');
                    snsButtons.forEach(btn => btn.remove());
                    info.title = titleElem.textContent.trim();
                } else {
                    info.title = '';
                }
                
                // ì´ë²¤íŠ¸ ì •ë³´
                const infoElem = document.querySelector('#eventInfo, .info');
                if (infoElem) {
                    const infoItems = infoElem.querySelectorAll('div');
                    infoItems.forEach(item => {
                        const text = item.textContent.trim();
                        if (text.includes('ì‘ëª¨ê¸°ê°„')) {
                            info.period = text.replace('ì‘ëª¨ê¸°ê°„ : ', '').trim();
                        } else if (text.includes('ì‘ëª¨ëŒ€ìƒ')) {
                            info.target = text.replace('ì‘ëª¨ëŒ€ìƒ : ', '').trim();
                        } else if (text.includes('ë‹¹ì²¨ìë°œí‘œ')) {
                            info.announcement = text.replace('ë‹¹ì²¨ìë°œí‘œ : ', '').trim();
                        } else if (text.includes('ì´ë²¤íŠ¸ë¬¸ì˜')) {
                            info.inquiry = text.replace('ì´ë²¤íŠ¸ë¬¸ì˜ : ', '').trim();
                        }
                    });
                }
                
                // D-Day
                const dDayElem = document.querySelector('.d-day, [class*="d-day"]');
                info.d_day = dDayElem ? dDayElem.textContent.trim() : '';
                
                // iframe ì •ë³´
                const iframe = document.querySelector('#evtThumb iframe, .thumb iframe');
                if (iframe) {
                    info.iframe_src = iframe.getAttribute('src');
                    info.iframe_width = iframe.getAttribute('width');
                    info.iframe_height = iframe.getAttribute('height');
                    info.iframe_title = iframe.getAttribute('title');
                }
                
                return info;
            }""")
            
            logging.info(f"ì´ë²¤íŠ¸ ì •ë³´ ì¶”ì¶œ ì„±ê³µ: title='{event_info.get('title', 'unknown')}', period='{event_info.get('period', 'unknown')}'")
            
            # ê¸°ê°„ íŒŒì‹± (startdate/enddate)
            def _parse_to_hyphen(s: str) -> str:
                import re as _re
                m = _re.search(r"(\d{4})[.\-/](\d{1,2})[.\-/](\d{1,2})", s or "")
                if not m:
                    return ""
                return f"{m.group(1)}-{m.group(2).zfill(2)}-{m.group(3).zfill(2)}"

            startdate = '0000-00-00'
            enddate = '9999-99-99'
            period_text = event_info.get('period') or ''
            if period_text:
                import re as _re
                parts = [_p.strip() for _p in _re.split(r"~|â€“|-|to", period_text) if _p and _p.strip()]
                if len(parts) >= 1:
                    sd = _parse_to_hyphen(parts[0])
                    if sd:
                        startdate = sd
                if len(parts) >= 2:
                    ed = _parse_to_hyphen(parts[1])
                    if ed:
                        enddate = ed
            # iframe ë‚´ìš© ì²˜ë¦¬ (ë„ë©”ì¸ ì œí•œ ì—†ì´ ë¬´ì¡°ê±´ ì¶”ì¶œ)
            iframe_content = ""
            iframe_html = ""
            if event_info.get('iframe_src'):
                try:
                    logging.info(f"ì´ë²¤íŠ¸ iframe ì²˜ë¦¬ ì‹œì‘: {event_info['iframe_src']}")
                    
                    # iframe ë‚´ë¶€ë¡œ ì´ë™
                    iframe_page = await context.new_page()
                    await iframe_page.goto(event_info['iframe_src'], wait_until='domcontentloaded', timeout=60000)
                    await iframe_page.wait_for_timeout(5000)  # iframe ë¡œë”© ëŒ€ê¸°
                    
                    # iframe ë‚´ìš© ì¶”ì¶œ
                    iframe_data = await iframe_page.evaluate("""() => {
                        // ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                        const elementsToRemove = document.querySelectorAll('script, style, noscript, .ad, .banner, .popup');
                        elementsToRemove.forEach(el => el.remove());
                        
                        // ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ ì°¾ê¸°
                        const mainContent = document.querySelector('body') || document.documentElement;
                        return {
                            html: mainContent ? mainContent.innerHTML : '',
                            title: document.title || '',
                            url: window.location.href
                        };
                    }""")
                    
                    iframe_html = iframe_data.get('html', '')
                    iframe_content = iframe_html
                    logging.info(f"ì´ë²¤íŠ¸ iframe ì²˜ë¦¬ ì„±ê³µ: ê¸¸ì´={len(iframe_html)}")
                    
                    await iframe_page.close()
                    
                except Exception as e:
                    logging.warning(f"ì´ë²¤íŠ¸ iframe ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    iframe_content = f"<p>iframe ë¡œë”© ì‹¤íŒ¨: {str(e)}</p>"
                    iframe_html = iframe_content
            else:
                logging.info("ì´ë²¤íŠ¸ iframe ì—†ìŒ")
            
            # ë§ˆí¬ë‹¤ìš´ ìƒì„±
            markdown_content = f"# {event_info.get('title', 'KT ì´ë²¤íŠ¸')}\n\n"
            
            if event_info.get('period'):
                markdown_content += f"{event_info['period']}\n"
            if event_info.get('target'):
                markdown_content += f"{event_info['target']}\n"
            if event_info.get('announcement'):
                markdown_content += f"{event_info['announcement']}\n"
            if event_info.get('inquiry'):
                markdown_content += f"{event_info['inquiry']}\n"
            if event_info.get('d_day'):
                markdown_content += f"{event_info['d_day']}\n"
            
            markdown_content += f"\n"
            
            if iframe_content:
                # iframe ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
                try:
                    from bs4 import BeautifulSoup
                    from markdownify import markdownify as md
                    
                    soup = BeautifulSoup(iframe_content, 'html.parser')
                    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                    for tag in soup(['script', 'style', 'noscript']):
                        tag.decompose()
                    # SNS ë²„íŠ¼ ì œê±°
                    for selector in ['.btn-twitter', '.btn-facebook', '.btn-kakao', '.btn-youtube']:
                        for element in soup.select(selector):
                            element.decompose()
                    
                    cleaned_html = str(soup)
                    iframe_markdown = md(cleaned_html)
                    markdown_content += iframe_markdown
                    logging.info(f"ì´ë²¤íŠ¸ iframe ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì„±ê³µ: ê¸¸ì´={len(iframe_markdown)}")
                    
                except Exception as e:
                    logging.warning(f"ì´ë²¤íŠ¸ iframe ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
                    markdown_content += f"iframe ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}\n"
            else:
                markdown_content += "ì´ë²¤íŠ¸ ìƒì„¸ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
            
            # HTML ìƒì„±
            html_content = f"<h1>{event_info.get('title', 'KT ì´ë²¤íŠ¸')}</h1>"
            if event_info.get('period'):
                html_content += f"<p>{event_info['period']}</p>"
            if event_info.get('target'):
                html_content += f"<p>{event_info['target']}</p>"
            if event_info.get('announcement'):
                html_content += f"<p>{event_info['announcement']}</p>"
            if event_info.get('inquiry'):
                html_content += f"<p>{event_info['inquiry']}</p>"
            if event_info.get('d_day'):
                html_content += f"<p>{event_info['d_day']}</p>"
            if iframe_html:
                html_content += iframe_html
            else:
                html_content += "<p>ì´ë²¤íŠ¸ ìƒì„¸ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.</p>"
            
            # ëª¨ë°”ì¼ ì´ë²¤íŠ¸ URL ìƒì„± ê·œì¹™ (ìš”ì²­: mblevtno = pcEvtNo + 1)
            def _pc_to_m_url(pc_url: str) -> str:
                import re as _re
                if not pc_url:
                    return ""
                m = _re.search(r"pcEvtNo=(\d+)", pc_url)
                if not m:
                    return pc_url.replace('https://event.kt.com', 'https://m.kt.com').replace('pcEvtNo=', 'mblevtno=')
                pc_no = int(m.group(1))
                mb_no = pc_no + 1
                mobile = pc_url.replace('https://event.kt.com', 'https://m.kt.com')
                mobile = _re.sub(r"pcEvtNo=\d+", f"mblevtno={mb_no}", mobile)
                return mobile

            mobile_url_from_detail = _pc_to_m_url(url)

            await browser.close()
            
            logging.info(f"KT ì´ë²¤íŠ¸ ìƒì„¸ ì²˜ë¦¬ ì™„ë£Œ: title='{event_info.get('title', 'unknown')}', iframe_processed={bool(iframe_content)}")
                        
            return {
                "datas": [{
                    "markdown": markdown_content,
                    "html": html_content,
                    "url": url,
                    "mobile_url": mobile_url_from_detail,
                        "murl": mobile_url_from_detail,
                    "startdate": startdate,
                    "enddate": enddate,
                    "metadata": {
                        "title": event_info.get('title', 'KT ì´ë²¤íŠ¸'),
                        "period": event_info.get('period', ''),
                        "target": event_info.get('target', ''),
                        "announcement": event_info.get('announcement', ''),
                        "inquiry": event_info.get('inquiry', ''),
                        "d_day": event_info.get('d_day', ''),
                        "iframe_src": event_info.get('iframe_src', ''),
                        "iframe_processed": bool(iframe_content)
                    }
                }],
                "menus": [{
                    "menu": f"{menu}^{event_info.get('title', 'unknown')}",
                    "url": url,
                    "murl": mobile_url_from_detail
                }],
                # ìƒìœ„(return) ë ˆë²¨ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
            }
            
        except Exception as e:
            logging.error(f"KT ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            await browser.close()
            return {
                "datas": [{
                    "markdown": f"# KT ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨\n\nì˜¤ë¥˜: {str(e)}",
                    "html": f"<h1>KT ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨</h1><p>ì˜¤ë¥˜: {str(e)}</p>",
                    "url": url,
                    "error": str(e)
                }]
            }

async def handle_kt_past_event_main(url: str, fclient, menu=None) -> dict:
    logging.info(f"KT ì§€ë‚œ ì´ë²¤íŠ¸ ë©”ì¸ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, menu={menu}")
    """
    KT ì§€ë‚œ ì´ë²¤íŠ¸ ë©”ì¸ í˜ì´ì§€ í•¸ë“¤ëŸ¬
    https://event.kt.com/html/event/past_event_list.html
    
    í˜ì´ì§€ë„¤ì´ì…˜ì„ ìˆœíšŒí•˜ë©´ì„œ ëª¨ë“  data-pcevtno ê°’ì„ ìˆ˜ì§‘í•œ í›„,
    ë³‘ë ¬ ì²˜ë¦¬ë¡œ ìƒì„¸ í˜ì´ì§€ë“¤ì„ ìŠ¤í¬ë˜í•‘
    """
    from playwright.async_api import async_playwright
    import asyncio
    import re
    from datetime import datetime
    
    logging.info(f"ğŸ¯ KT ì§€ë‚œ ì´ë²¤íŠ¸ ë©”ì¸ í˜ì´ì§€ ì²˜ë¦¬: {url}")
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        
        try:
            response = await page.goto(url, wait_until='networkidle', timeout=60000)
            await page.wait_for_timeout(5000)
            
            # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
            status_code = response.status if response else None
            if status_code:
                if status_code >= 400:
                    logging.error(f"âŒ KT ì§€ë‚œ ì´ë²¤íŠ¸ ë©”ì¸ ({url}): HTTP {status_code} ì˜¤ë¥˜")
                elif status_code >= 300:
                    logging.warning(f"âš ï¸ KT ì§€ë‚œ ì´ë²¤íŠ¸ ë©”ì¸ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
                else:
                    logging.info(f"âœ… KT ì§€ë‚œ ì´ë²¤íŠ¸ ë©”ì¸ ({url}): HTTP {status_code} ì„±ê³µ")
            else:
                logging.debug(f"ğŸ” KT ì§€ë‚œ ì´ë²¤íŠ¸ ë©”ì¸ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")
            
            # í˜ì´ì§€ë„¤ì´ì…˜ ì •ë³´ ì¶”ì¶œ
            pagination_info = await page.evaluate("""() => {
                const pagination = document.querySelector('.pagination');
                if (!pagination) return { total_pages: 1, current_page: 1 };
                
                // í˜„ì¬ í˜ì´ì§€ í™•ì¸ (title="í˜„ì¬ìœ„ì¹˜"ì¸ span ìš”ì†Œ)
                const currentPageElem = pagination.querySelector('span[title="í˜„ì¬ìœ„ì¹˜"]');
                const currentPage = currentPageElem ? parseInt(currentPageElem.textContent) : 1;
                
                // data-page ì†ì„±ì´ ìˆëŠ” ëª¨ë“  ë§í¬ì—ì„œ ìµœëŒ€ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
                const pageLinks = pagination.querySelectorAll('a[data-page]');
                let maxPage = currentPage; // í˜„ì¬ í˜ì´ì§€ë¶€í„° ì‹œì‘
                
                pageLinks.forEach(link => {
                    const dataPage = link.getAttribute('data-page');
                    if (dataPage) {
                        const pageNum = parseInt(dataPage);
                        if (!isNaN(pageNum) && pageNum > maxPage) {
                            maxPage = pageNum;
                        }
                    }
                });
                
                // ë§ˆì§€ë§‰ í˜ì´ì§€ ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ë” ë§ì€ í˜ì´ì§€ê°€ ìˆëŠ”ì§€ íŒë‹¨
                const lastLink = pagination.querySelector('a.last');
                const nextLink = pagination.querySelector('a.next');
                
                // ë‹¤ìŒ í˜ì´ì§€ë‚˜ ë§ˆì§€ë§‰ í˜ì´ì§€ ë§í¬ê°€ ìˆìœ¼ë©´ ë” ë§ì€ í˜ì´ì§€ê°€ ìˆì„ ìˆ˜ ìˆìŒ
                if (lastLink || nextLink) {
                    // ë³´ìˆ˜ì ìœ¼ë¡œ í˜„ì¬ ë³´ì´ëŠ” ìµœëŒ€ í˜ì´ì§€ë³´ë‹¤ ë” ìˆë‹¤ê³  ê°€ì •
                    // ì‹¤ì œë¡œëŠ” ë§ˆì§€ë§‰ í˜ì´ì§€ë¥¼ í´ë¦­í•´ì„œ í™•ì¸í•´ì•¼ í•˜ì§€ë§Œ, 
                    // ì¼ë‹¨ í˜„ì¬ ë³´ì´ëŠ” í˜ì´ì§€ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ í•¨
                }
                
                return { 
                    total_pages: maxPage, 
                    current_page: currentPage,
                    has_next: !!nextLink,
                    has_last: !!lastLink
                };
            }""")
            
            all_event_infos = []
            
            # ë¨¼ì € ë§ˆì§€ë§‰ í˜ì´ì§€ë¥¼ í™•ì¸í•˜ì—¬ ì •í™•í•œ ì´ í˜ì´ì§€ ìˆ˜ë¥¼ ì•Œì•„ëƒ„
            if pagination_info.get('has_last', False):
                logging.info("ğŸ” ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸í•˜ì—¬ ì •í™•í•œ ì´ í˜ì´ì§€ ìˆ˜ íŒŒì•… ì¤‘...")
                try:
                    # ë§ˆì§€ë§‰ í˜ì´ì§€ë¡œ ì´ë™
                    await page.evaluate("""() => {
                        const pagination = document.querySelector('.pagination');
                        if (pagination) {
                            const lastLink = pagination.querySelector('a.last');
                            if (lastLink) {
                                lastLink.click();
                            }
                        }
                    }""")
                    
                    await page.wait_for_timeout(3000)
                    
                    # ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ í™•ì¸
                    last_page_info = await page.evaluate("""() => {
                        const pagination = document.querySelector('.pagination');
                        if (!pagination) return { last_page: 1 };
                        
                        const currentPageElem = pagination.querySelector('span[title="í˜„ì¬ìœ„ì¹˜"]');
                        const lastPage = currentPageElem ? parseInt(currentPageElem.textContent) : 1;
                        
                        return { last_page: lastPage };
                    }""")
                    
                    total_pages = last_page_info.get('last_page', pagination_info.get('total_pages', 1))
                    logging.info(f"âœ… ì •í™•í•œ ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸: {total_pages}í˜ì´ì§€")
                    
                    # ì²« í˜ì´ì§€ë¡œ ëŒì•„ê°€ê¸°
                    await page.evaluate("""() => {
                        const pagination = document.querySelector('.pagination');
                        if (pagination) {
                            const firstLink = pagination.querySelector('a.first');
                            if (firstLink) {
                                firstLink.click();
                            }
                        }
                    }""")
                    
                    await page.wait_for_timeout(3000)
                    
                except Exception as e:
                    logging.warning(f"ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {str(e)}")
                    total_pages = pagination_info.get('total_pages', 1)
            else:
                total_pages = pagination_info.get('total_pages', 1)
            
            logging.info(f"ğŸ“„ ì´ {total_pages}ê°œ í˜ì´ì§€ì—ì„œ ì´ë²¤íŠ¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘")
            
            # ëª¨ë“  í˜ì´ì§€ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
            current_page = 1
            for page_num in range(1, total_pages + 1):
                if page_num > current_page:
                    # ë‹¤ìŒ í˜ì´ì§€ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ì´ë™
                    while current_page < page_num:
                        logging.info(f"ğŸ”„ í˜ì´ì§€ {current_page} -> {current_page + 1}ë¡œ ì´ë™ ì¤‘...")
                        
                        # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ í´ë¦­
                        page_moved = await page.evaluate("""() => {
                            const pagination = document.querySelector('.pagination');
                            if (!pagination) return false;
                            
                            const nextLink = pagination.querySelector('a.next');
                            if (nextLink) {
                                nextLink.click();
                                return true;
                            }
                            return false;
                        }""")
                        
                        if page_moved:
                            await page.wait_for_timeout(3000)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                            
                            # ì‹¤ì œ ì´ë™ëœ í˜ì´ì§€ í™•ì¸
                            actual_page = await page.evaluate("""() => {
                                const pagination = document.querySelector('.pagination');
                                if (!pagination) return 1;
                                
                                const currentPageElem = pagination.querySelector('span[title="í˜„ì¬ìœ„ì¹˜"]');
                                return currentPageElem ? parseInt(currentPageElem.textContent) : 1;
                            }""")
                            
                            if actual_page > current_page:
                                current_page = actual_page
                                logging.info(f"âœ… í˜ì´ì§€ {current_page}ë¡œ ì´ë™ ì„±ê³µ")
                            else:
                                logging.warning(f"âš ï¸ í˜ì´ì§€ ì´ë™ì´ ì˜ˆìƒê³¼ ë‹¤ë¦„: í˜„ì¬ {actual_page}")
                                break
                        else:
                            logging.warning(f"âš ï¸ ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                            break
                    
                    if current_page != page_num:
                        logging.warning(f"âš ï¸ ëª©í‘œ í˜ì´ì§€ {page_num}ì— ë„ë‹¬í•˜ì§€ ëª»í•¨ (í˜„ì¬: {current_page})")
                        continue
                else:
                    logging.info(f"ğŸ“„ ì²« ë²ˆì§¸ í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...")
                
                # í˜„ì¬ í˜ì´ì§€ì˜ ì´ë²¤íŠ¸ ì •ë³´ ì¶”ì¶œ (ì •í™•í•œ HTML êµ¬ì¡°ì— ë§ê²Œ)
                page_events = await page.evaluate(f"""() => {{
                    const events = [];
                    
                    // í…Œì´ë¸” êµ¬ì¡°: table.board tbody tr
                    const table = document.querySelector('table.board');
                    if (!table) {{
                        console.log('Table not found');
                        return events;
                    }}
                    
                    const tbody = table.querySelector('tbody');
                    if (!tbody) {{
                        console.log('Tbody not found');
                        return events;
                    }}
                    
                    const rows = tbody.querySelectorAll('tr');
                    console.log('Found rows:', rows.length);
                    
                    rows.forEach((row, index) => {{
                        const link = row.querySelector('a[data-pcevtno]');
                        if (link) {{
                            const evtNo = link.getAttribute('data-pcevtno');
                            if (evtNo) {{
                                // ì œëª©: ë§í¬ì˜ í…ìŠ¤íŠ¸ (ì˜ˆ: "KT ì¥ê¸°ê³ ê° ê°ì‚¬ ì´ë²¤íŠ¸")
                                const title = link.textContent.trim();
                                
                                // ê¸°ê°„: ë‘ ë²ˆì§¸ td (ì˜ˆ: "2025.08.28 ~ 2025.09.10")
                                const cells = row.querySelectorAll('td');
                                const period = cells.length > 1 ? cells[1].textContent.trim() : '';
                                
                                console.log(`Event ${{index + 1}}: ${{title}} (${{evtNo}})`);
                                
                                events.push({{
                                    page: {page_num},
                                    evt_no: evtNo,
                                    title: title,
                                    period: period,
                                    link_href: link.href || ''
                                }});
                            }}
                        }}
                    }});
                    
                    console.log('Total events found:', events.length);
                    return events;
                }}""")
                
                all_event_infos.extend(page_events)
                logging.info(f"ğŸ“„ í˜ì´ì§€ {page_num}/{total_pages} ì²˜ë¦¬ ì™„ë£Œ: {len(page_events)}ê°œ ì´ë²¤íŠ¸")
                
                # í˜ì´ì§€ë³„ ì´ë²¤íŠ¸ ìƒì„¸ ë¡œê¹…
                if len(page_events) == 0:
                    logging.error(f"âŒ í˜ì´ì§€ {page_num}ì—ì„œ ì´ë²¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                else:
                    evt_nos_on_page = [e['evt_no'] for e in page_events]
                    logging.info(f"ğŸ“„ í˜ì´ì§€ {page_num} ì´ë²¤íŠ¸ ë²ˆí˜¸ë“¤: {evt_nos_on_page}")
            
            # ì§„ì… í˜ì´ì§€(ëª©ë¡ í˜ì´ì§€) ìì²´ë„ ì¶”ì¶œ
            logging.info(f"ğŸ“„ ì§„ì… í˜ì´ì§€ ì¶”ì¶œ ì‹œì‘")
            entry_page_html = await page.content()
            
            # markdownifyë¡œ ë³€í™˜
            from markdownify import markdownify as md_convert
            entry_page_markdown = md_convert(entry_page_html, heading_style="ATX")
            
            # ì§„ì… í˜ì´ì§€ ë°ì´í„° êµ¬ì„±
            entry_page_data = {
                "markdown": entry_page_markdown,
                "html": entry_page_html,
                "url": url,
                "metadata": {
                    "title": f"{menu or 'KT ì§€ë‚œ ì´ë²¤íŠ¸'} ëª©ë¡",
                    "is_entry_page": True,
                    "total_events": len(all_event_infos),
                    "total_pages": total_pages,
                    "original_url": url,
                    "special_processed": True,
                    "playwright_processed": True
                }
            }
            logging.info(f"âœ… ì§„ì… í˜ì´ì§€ ì¶”ì¶œ ì™„ë£Œ")
            
            await browser.close()
            
            logging.info(f"ğŸ¯ ì´ {len(all_event_infos)}ê°œ ì§€ë‚œ ì´ë²¤íŠ¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
            
            # ì¤‘ë³µ ì œê±° (evt_no ê¸°ì¤€)
            unique_events = {}
            duplicates = []
            for event in all_event_infos:
                evt_no = event['evt_no']
                if evt_no not in unique_events:
                    unique_events[evt_no] = event
                else:
                    duplicates.append(evt_no)
            
            unique_event_list = list(unique_events.values())
            logging.info(f"ğŸ¯ ì¤‘ë³µ ì œê±° í›„: {len(unique_event_list)}ê°œ ì´ë²¤íŠ¸")
            
            if duplicates:
                duplicate_counts = {}
                for dup in duplicates:
                    duplicate_counts[dup] = duplicate_counts.get(dup, 0) + 1
                logging.warning(f"âš ï¸ ì¤‘ë³µ ë°œê²¬ëœ ì´ë²¤íŠ¸ë“¤: {dict(duplicate_counts)} (ì´ {len(duplicates)}ê°œ ì¤‘ë³µ)")
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ìƒì„¸ í˜ì´ì§€ë“¤ ìŠ¤í¬ë˜í•‘
            individual_posts = [entry_page_data]  # ì§„ì… í˜ì´ì§€ë¥¼ ì²« ë²ˆì§¸ë¡œ ì¶”ê°€
            if unique_event_list:
                logging.info(f"ğŸš€ {len(unique_event_list)}ê°œ ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘")
                
                # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„¸ë§ˆí¬ì–´ (ë™ì‹œ ì²˜ë¦¬ ê°œìˆ˜ ì œí•œ)
                semaphore = asyncio.Semaphore(15)  # ìµœëŒ€ 15ê°œ ë™ì‹œ ì²˜ë¦¬
                
                async def process_single_event(event_info, event_index):
                    async with semaphore:
                        try:
                            logging.info(f"[{event_index+1}/{len(unique_event_list)}] ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹œì‘: '{event_info['title']}' (evt_no: {event_info['evt_no']})")
                            
                            # ìƒì„¸ í˜ì´ì§€ URL êµ¬ì„±
                            detail_url = f"https://event.kt.com/html/event/past_event_view.html?page={event_info['page']}&searchCtg=ALL&pcEvtNo={event_info['evt_no']}"
                            
                            # ìƒì„¸ í˜ì´ì§€ í•¸ë“¤ëŸ¬ í˜¸ì¶œ (ì œëª© ì •ë³´ ì „ë‹¬)
                            detail_result = await handle_kt_past_event_detail(detail_url, fclient, menu, event_info)
                            
                            if detail_result and "datas" in detail_result and detail_result["datas"]:
                                # ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•œ ë°ì´í„° ì‚¬ìš©
                                individual_post = detail_result["datas"][0]
                                # ì¶”ê°€ ë©”íƒ€ë°ì´í„° ë³‘í•©
                                individual_post["metadata"].update({
                                    "evt_no": event_info['evt_no'],
                                    "original_url": url,
                                    "post_index": event_index + 1,
                                    "total_posts": len(unique_event_list),
                                    "source_page": event_info['page']
                                })
                                # ìƒì„¸ íƒ€ì´í‹€ë¡œ ë³´ì •
                                detail_title = individual_post["metadata"].get('title', '').strip()
                                if detail_title:
                                    event_info['title'] = detail_title
                                logging.info(f"[{event_index+1}/{len(unique_event_list)}] ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì„±ê³µ: '{event_info['title']}'")
                                return individual_post, event_info
                            else:
                                # ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ëª©ë¡ ì •ë³´ë¡œ fallback
                                logging.warning(f"[{event_index+1}/{len(unique_event_list)}] ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨, ëª©ë¡ ì •ë³´ë¡œ fallback: '{event_info['title']}'")
                                
                                # ì œëª©ì´ ë¹„ì–´ìˆìœ¼ë©´ ì´ë²¤íŠ¸ ë²ˆí˜¸ë¡œ ì œëª© ìƒì„±
                                display_title = event_info['title'] if event_info['title'].strip() else f"ì§€ë‚œ ì´ë²¤íŠ¸({event_info['evt_no']})"
                                
                                individual_post = {
                                    "markdown": f"# {display_title}\\n\\nì´ë²¤íŠ¸ ë²ˆí˜¸: {event_info['evt_no']}\\nê¸°ê°„: {event_info['period']}\\n\\nì´ë²¤íŠ¸ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                                    "html": f"<h1>{display_title}</h1><p>ì´ë²¤íŠ¸ ë²ˆí˜¸: {event_info['evt_no']}</p><p>ê¸°ê°„: {event_info['period']}</p><p>ì´ë²¤íŠ¸ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.</p>",
                                    "url": detail_url,
                                    "metadata": {
                                        "title": display_title,
                                        "evt_no": event_info['evt_no'],
                                        "period": event_info['period'],
                                        "status": "ì¢…ë£Œ",
                                        "original_url": url,
                                        "post_index": event_index + 1,
                                        "total_posts": len(unique_event_list),
                                        "source_page": event_info['page'],
                                        "detail_processing_failed": True,
                                        "startdate": "1900-01-01",
                                        "enddate": "2999-12-31"
                                    }
                                }
                                return individual_post, event_info
                                
                        except Exception as e:
                            logging.error(f"[{event_index+1}/{len(unique_event_list)}] ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: '{event_info['title']}', ì—ëŸ¬: {str(e)}")
                            # ì—ëŸ¬ ì‹œ ëª©ë¡ ì •ë³´ë¡œ fallback
                            
                            # ì œëª©ì´ ë¹„ì–´ìˆìœ¼ë©´ ì´ë²¤íŠ¸ ë²ˆí˜¸ë¡œ ì œëª© ìƒì„±
                            display_title = event_info['title'] if event_info['title'].strip() else f"ì§€ë‚œ ì´ë²¤íŠ¸({event_info['evt_no']})"
                            
                            individual_post = {
                                "markdown": f"# {display_title}\\n\\nì´ë²¤íŠ¸ ë²ˆí˜¸: {event_info['evt_no']}\\nê¸°ê°„: {event_info['period']}\\n\\nìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
                                "html": f"<h1>{display_title}</h1><p>ì´ë²¤íŠ¸ ë²ˆí˜¸: {event_info['evt_no']}</p><p>ê¸°ê°„: {event_info['period']}</p><p>ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}</p>",
                                "url": f"https://event.kt.com/html/event/past_event_view.html?page={event_info['page']}&searchCtg=ALL&pcEvtNo={event_info['evt_no']}",
                                "metadata": {
                                    "title": display_title,
                                    "evt_no": event_info['evt_no'],
                                    "period": event_info['period'],
                                    "status": "ì˜¤ë¥˜",
                                    "original_url": url,
                                    "post_index": event_index + 1,
                                    "total_posts": len(unique_event_list),
                                    "source_page": event_info['page'],
                                    "error": str(e),
                                    "startdate": "1900-01-01",
                                    "enddate": "2999-12-31"
                                }
                            }
                            return individual_post, event_info
                
                # ëª¨ë“  ì´ë²¤íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
                tasks = [process_single_event(event_info, i) for i, event_info in enumerate(unique_event_list)]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # ê²°ê³¼ ì •ë¦¬ ë° ëˆ„ë½ ì¶”ì 
                processed_evt_nos = set()
                failed_evt_nos = []
                
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        logging.error(f"âŒ ë³‘ë ¬ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ ({i+1}/{len(results)}): {str(result)}")
                        if i < len(unique_event_list):
                            failed_evt_no = unique_event_list[i].get('evt_no', 'unknown')
                            failed_evt_nos.append(failed_evt_no)
                            logging.error(f"âŒ ì‹¤íŒ¨í•œ ì´ë²¤íŠ¸ ë²ˆí˜¸: {failed_evt_no}")
                    else:
                        individual_post, event_info = result
                        individual_posts.append(individual_post)
                        evt_no = event_info.get('evt_no', 'unknown')
                        processed_evt_nos.add(evt_no)
                
                # ëˆ„ë½ëœ ì´ë²¤íŠ¸ í™•ì¸
                expected_evt_nos = {event['evt_no'] for event in unique_event_list}
                missing_evt_nos = expected_evt_nos - processed_evt_nos
                
                if missing_evt_nos:
                    logging.error(f"âŒ ëˆ„ë½ëœ ì´ë²¤íŠ¸ë“¤ ({len(missing_evt_nos)}ê°œ): {sorted(missing_evt_nos)}")
                    for missing_no in sorted(missing_evt_nos):
                        # ëˆ„ë½ëœ ì´ë²¤íŠ¸ ì •ë³´ ì°¾ê¸°
                        missing_event = next((e for e in unique_event_list if e['evt_no'] == missing_no), None)
                        if missing_event:
                            logging.error(f"âŒ ëˆ„ë½ ì´ë²¤íŠ¸ ìƒì„¸: ë²ˆí˜¸={missing_no}, ì œëª©='{missing_event.get('title', 'unknown')}', í˜ì´ì§€={missing_event.get('page', 'unknown')}")
                
                if failed_evt_nos:
                    logging.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨í•œ ì´ë²¤íŠ¸ë“¤ ({len(failed_evt_nos)}ê°œ): {failed_evt_nos}")
                
                logging.info(f"ğŸ“Š ì´ë²¤íŠ¸ ì²˜ë¦¬ í†µê³„: ì „ì²´={len(unique_event_list)}, ì„±ê³µ={len(processed_evt_nos)}, ì‹¤íŒ¨={len(failed_evt_nos)}, ëˆ„ë½={len(missing_evt_nos)}")
            
            # menus ë°°ì—´ ìƒì„± (individual_posts ì²˜ë¦¬ í›„ ì‹¤ì œ ì œëª©ìœ¼ë¡œ)
            menus = []
            
            # ì§„ì… í˜ì´ì§€ë¥¼ ì²« ë²ˆì§¸ ë©”ë‰´ë¡œ ì¶”ê°€
            menus.append({
                "menu": menu or "KT ì§€ë‚œ ì´ë²¤íŠ¸",
                "url": url,
                "murl": url.replace('https://event.kt.com', 'https://m.kt.com')
            })
            
            def _to_m(u: str) -> str:
                import re as _re
                if not u:
                    return ""
                m = _re.search(r"pcEvtNo=(\d+)", u)
                if not m:
                    return u.replace('https://event.kt.com', 'https://m.kt.com').replace('pcEvtNo=', 'mblevtno=')
                pc_no = int(m.group(1))
                mb_no = pc_no + 1
                mobile = u.replace('https://event.kt.com', 'https://m.kt.com')
                mobile = _re.sub(r"pcEvtNo=\d+", f"mblevtno={mb_no}", mobile)
                return mobile
            
            # ê° ê²Œì‹œë¬¼ ë°ì´í„°ì— murl ì£¼ì…
            for _post in individual_posts:
                try:
                    _u = _post.get('url', '')
                    if _u:
                        _post['murl'] = _to_m(_u)
                except Exception:
                    pass

            # individual_postsì—ì„œ ì‹¤ì œ ì²˜ë¦¬ëœ ì œëª©ê³¼ URL ì‚¬ìš©
            for post in individual_posts:
                post_metadata = post.get('metadata', {})
                post_title = post_metadata.get('title', 'ì œëª© ì—†ìŒ')
                evt_no = post_metadata.get('evt_no', 'unknown')
                post_url = post.get('url', '')
                
                # ë©”ë‰´ êµ¬ì¡°: í˜œíƒ^ì´ë²¤íŠ¸/í•«ë”œ^ì§€ë‚œ ì´ë²¤íŠ¸^{ì´ë²¤íŠ¸ëª…}({evt_no})
                # evt_noë¥¼ í¬í•¨í•˜ì—¬ ê³ ìœ ì„± ë³´ì¥
                final_menu = f"{menu}^{post_title}({evt_no})"
                
                menus.append({
                    "menu": final_menu,
                    "url": post_url,
                    "murl": _to_m(post_url)
                })
            
            return {
                "datas": individual_posts,
                "menus": menus,
                "metadata": {
                    "title": "KT ì§€ë‚œ ì´ë²¤íŠ¸",
                    "total_events": len(unique_event_list),
                    "total_pages": total_pages,
                    "url": url,
                    "special_processed": True,
                    "playwright_processed": True,
                    "parallel_processed": True
                }
            }
            
        except Exception as e:
            logging.error(f"âŒ KT ì§€ë‚œ ì´ë²¤íŠ¸ ë©”ì¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            await browser.close()
            return {
                "markdown": f"# KT ì§€ë‚œ ì´ë²¤íŠ¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨\\n\\nì˜¤ë¥˜: {str(e)}",
                "html": f"<h1>KT ì§€ë‚œ ì´ë²¤íŠ¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨</h1><p>ì˜¤ë¥˜: {str(e)}</p>",
                "datas": [],
                "error": str(e)
            }
async def handle_kt_past_event_detail(url: str, fclient, menu=None, main_event_info=None) -> dict:
    logging.info(f"KT ì§€ë‚œ ì´ë²¤íŠ¸ ìƒì„¸ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, menu={menu}")
    """
    KT ì§€ë‚œ ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ í•¸ë“¤ëŸ¬
    https://event.kt.com/html/event/past_event_view.html?page=1&searchCtg=ALL&pcEvtNo=13590
    """
    from playwright.async_api import async_playwright
    import re
    
    logging.info(f"KT ì§€ë‚œ ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹œì‘: {url}")
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        
        try:
            # ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
            max_retries = 3
            retry_count = 0
            response = None
            
            while retry_count < max_retries:
                try:
                    logging.info(f"ì§€ë‚œ ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ì§„ì… ì‹œë„ {retry_count + 1}/{max_retries}: url={url}")
                    # íƒ€ì„ì•„ì›ƒì„ 60ì´ˆë¡œ ëŠ˜ë¦¬ê³  ë” ê´€ëŒ€í•œ ë¡œë”© ì¡°ê±´ ì‚¬ìš©
                    response = await page.goto(url, wait_until='networkidle', timeout=60000)
                    await page.wait_for_timeout(5000)  # í˜ì´ì§€ ì•ˆì •í™” ëŒ€ê¸°
                    break  # ì„±ê³µí•˜ë©´ ë£¨í”„ íƒˆì¶œ
                except Exception as e:
                    retry_count += 1
                    if retry_count >= max_retries:
                        logging.error(f"ì§€ë‚œ ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨ (ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼): {str(e)}")
                        raise e
                    else:
                        logging.warning(f"ì§€ë‚œ ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨, ì¬ì‹œë„ {retry_count}/{max_retries}: {str(e)}")
                        await page.wait_for_timeout(3000)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
            
            # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
            status_code = response.status if response else None
            if status_code:
                if status_code >= 400:
                    logging.error(f"âŒ KT ì§€ë‚œ ì´ë²¤íŠ¸ ìƒì„¸ ({url}): HTTP {status_code} ì˜¤ë¥˜")
                elif status_code >= 300:
                    logging.warning(f"âš ï¸ KT ì§€ë‚œ ì´ë²¤íŠ¸ ìƒì„¸ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
                else:
                    logging.info(f"âœ… KT ì§€ë‚œ ì´ë²¤íŠ¸ ìƒì„¸ ({url}): HTTP {status_code} ì„±ê³µ")
            else:
                logging.debug(f"ğŸ” KT ì§€ë‚œ ì´ë²¤íŠ¸ ìƒì„¸ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")
            
            # í˜ì´ì§€ ì½˜í…ì¸  êµ¬ì¡° í™•ì¸ (.contents ì˜ì—­ ê¸°ì¤€)
            page_content = await page.evaluate("""() => {
                // .contents ì˜ì—­ í™•ì¸
                const contentsDiv = document.querySelector('.contents');
                if (!contentsDiv) {
                    return { empty: true, text: '', terminated: false, error: 'Contents div not found' };
                }
                
                // .box-close í´ë˜ìŠ¤ì—ì„œ ì¢…ë£Œ ë©”ì‹œì§€ í™•ì¸
                const boxClose = contentsDiv.querySelector('.box-close');
                const isTerminated = boxClose && boxClose.textContent.includes('ì´ë²¤íŠ¸ê°€ ì¢…ë£Œ');
                
                // ì œëª© í™•ì¸
                const titleElem = contentsDiv.querySelector('#contents-title, .contents-title');
                const title = titleElem ? titleElem.textContent.trim() : '';
                
                return {
                    empty: false,
                    terminated: isTerminated,
                    title: title,
                    contentsHTML: contentsDiv.outerHTML,
                    terminatedMessage: boxClose ? boxClose.textContent.trim() : ''
                };
            }""")
            
            if page_content.get('empty', False):
                logging.warning(f"ì§€ë‚œ ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì¢…ë£Œ ë©”ì‹œì§€ë§Œ ìˆìŒ: {url}")
                
                # URLì—ì„œ pcEvtNo ì¶”ì¶œ
                evt_no_match = re.search(r'pcEvtNo=(\d+)', url)
                evt_no = evt_no_match.group(1) if evt_no_match else 'unknown'
                
                await browser.close()
                return {
                    "datas": [{
                        "markdown": f"# ì¢…ë£Œëœ ì´ë²¤íŠ¸({evt_no})\\n\\nì´ë²¤íŠ¸ ë²ˆí˜¸: {evt_no}\\n\\nì´ë²¤íŠ¸ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                        "html": f"<h1>ì¢…ë£Œëœ ì´ë²¤íŠ¸({evt_no})</h1><p>ì´ë²¤íŠ¸ ë²ˆí˜¸: {evt_no}</p><p>ì´ë²¤íŠ¸ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.</p>",
                        "url": url,
                        "metadata": {
                            "title": f"ì¢…ë£Œëœ ì´ë²¤íŠ¸({evt_no})",
                            "evt_no": evt_no,
                            "period": "",
                            "status": "ì¢…ë£Œ",
                            "empty_content": True,
                            "startdate": "1900-01-01",
                            "enddate": "2999-12-31"
                        }
                    }]
                }
            
            # ì´ë²¤íŠ¸ ì •ë³´ ì¶”ì¶œ (ê°„ë‹¨í•˜ê²Œ ì œëª©ë§Œ ì¶”ì¶œ, ë‚˜ë¨¸ì§€ëŠ” .contents ì „ì²´ ì‚¬ìš©)
            event_info = await page.evaluate("""() => {
                const info = {};
                
                // .contents ì˜ì—­ì—ì„œ ì œëª©ë§Œ ì¶”ì¶œ
                const contentsDiv = document.querySelector('.contents');
                if (contentsDiv) {
                    // ì œëª©: #contents-title ë˜ëŠ” .contents-title
                    const titleElem = contentsDiv.querySelector('#contents-title, .contents-title');
                    if (titleElem) {
                        info.title = titleElem.textContent.trim();
                        console.log('Title found:', info.title);
                    }
                    
                    // iframe í™•ì¸ (í•„ìš”ì‹œ)
                    const iframe = contentsDiv.querySelector('iframe');
                    if (iframe && iframe.getAttribute('src')) {
                        info.iframe_src = iframe.getAttribute('src');
                        console.log('Iframe found:', info.iframe_src);
                    }
                } else {
                    console.log('Contents div not found');
                }
                
                return info;
            }""")
            
            # ë©”ì¸ í˜ì´ì§€ì—ì„œ ì „ë‹¬ë°›ì€ ì œëª© ìš°ì„  ì‚¬ìš©
            if main_event_info and main_event_info.get('title'):
                event_info['title'] = main_event_info['title']
            
            logging.info(f"ì§€ë‚œ ì´ë²¤íŠ¸ ì •ë³´ ì¶”ì¶œ ì„±ê³µ: title='{event_info.get('title', 'unknown')}', period='{event_info.get('period', 'unknown')}'")
            
            # ê¸°ê°„ íŒŒì‹± (startdate/enddate)
            def _parse_to_hyphen(s: str) -> str:
                import re as _re
                m = _re.search(r"(\d{4})[.\-/](\d{1,2})[.\-/](\d{1,2})", s or "")
                if not m:
                    return ""
                return f"{m.group(1)}-{m.group(2).zfill(2)}-{m.group(3).zfill(2)}"

            startdate = '1900-01-01'
            enddate = '2999-12-31'
            period_text = event_info.get('period') or ''
            if period_text:
                import re as _re
                parts = [_p.strip() for _p in _re.split(r"~|â€“|-|to", period_text) if _p and _p.strip()]
                if len(parts) >= 1:
                    sd = _parse_to_hyphen(parts[0])
                    if sd:
                        startdate = sd
                if len(parts) >= 2:
                    ed = _parse_to_hyphen(parts[1])
                    if ed:
                        enddate = ed
            
            # ì½˜í…ì¸  ì²˜ë¦¬ (.contents ì˜ì—­ ì „ì²´ ì‚¬ìš©)
            content_html = page_content.get('contentsHTML', '')
            
            # iframeì´ ìˆë‹¤ë©´ ë³„ë„ ì²˜ë¦¬
            if event_info.get('iframe_src'):
                try:
                    logging.info(f"ì§€ë‚œ ì´ë²¤íŠ¸ iframe ì²˜ë¦¬ ì‹œì‘: {event_info['iframe_src']}")
                    
                    # iframe ë‚´ë¶€ë¡œ ì´ë™
                    iframe_page = await context.new_page()
                    await iframe_page.goto(event_info['iframe_src'], wait_until='networkidle', timeout=60000)
                    await iframe_page.wait_for_timeout(8000)  # iframe ë¡œë”© ëŒ€ê¸°
                    
                    # iframe ë‚´ìš© ì¶”ì¶œ
                    iframe_data = await iframe_page.evaluate("""() => {
                        // ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                        const elementsToRemove = document.querySelectorAll('script, style, noscript, .ad, .banner, .popup');
                        elementsToRemove.forEach(el => el.remove());
                        
                        // ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ ì°¾ê¸°
                        const mainContent = document.querySelector('body') || document.documentElement;
                        return {
                            html: mainContent ? mainContent.innerHTML : '',
                            title: document.title || '',
                            url: window.location.href
                        };
                    }""")
                    
                    iframe_content = iframe_data.get('html', '')
                    # .contents HTMLê³¼ iframe ë‚´ìš© ê²°í•©
                    content_html += f"\\n\\n<div class='iframe-content'>{iframe_content}</div>"
                    logging.info(f"ì§€ë‚œ ì´ë²¤íŠ¸ iframe ì²˜ë¦¬ ì„±ê³µ: ê¸¸ì´={len(iframe_content)}")
                    
                    await iframe_page.close()
                    
                except Exception as e:
                    logging.warning(f"ì§€ë‚œ ì´ë²¤íŠ¸ iframe ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    content_html += f"\\n\\n<p>iframe ë¡œë”© ì‹¤íŒ¨: {str(e)}</p>"
            
            logging.info(f"ì§€ë‚œ ì´ë²¤íŠ¸ ì½˜í…ì¸  ì‚¬ìš©: ê¸¸ì´={len(content_html)}")
            
            # ë§ˆí¬ë‹¤ìš´ ìƒì„±
            markdown_content = f"# {event_info.get('title', 'KT ì§€ë‚œ ì´ë²¤íŠ¸')}\\n\\n"
            
            if event_info.get('period'):
                markdown_content += f"**ê¸°ê°„**: {event_info['period']}\\n\\n"
            if event_info.get('target'):
                markdown_content += f"**ëŒ€ìƒ**: {event_info['target']}\\n\\n"
            if event_info.get('announcement'):
                markdown_content += f"**ë‹¹ì²¨ìë°œí‘œ**: {event_info['announcement']}\\n\\n"
            if event_info.get('inquiry'):
                markdown_content += f"**ë¬¸ì˜**: {event_info['inquiry']}\\n\\n"
            if event_info.get('d_day'):
                markdown_content += f"**ìƒíƒœ**: {event_info['d_day']}\\n\\n"
            
            if content_html:
                # HTML ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
                try:
                    from bs4 import BeautifulSoup
                    from markdownify import markdownify as md
                    
                    soup = BeautifulSoup(content_html, 'html.parser')
                    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                    for tag in soup(['script', 'style', 'noscript']):
                        tag.decompose()
                    # SNS ë²„íŠ¼ ì œê±°
                    for selector in ['.btn-twitter', '.btn-facebook', '.btn-kakao', '.btn-youtube']:
                        for element in soup.select(selector):
                            element.decompose()
                    
                    cleaned_html = str(soup)
                    content_markdown = md(cleaned_html)
                    markdown_content += content_markdown
                    logging.info(f"ì§€ë‚œ ì´ë²¤íŠ¸ ì½˜í…ì¸  ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì„±ê³µ: ê¸¸ì´={len(content_markdown)}")
                    
                except Exception as e:
                    logging.warning(f"ì§€ë‚œ ì´ë²¤íŠ¸ ì½˜í…ì¸  ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
                    markdown_content += f"ì½˜í…ì¸ ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}\\n"
            else:
                markdown_content += "ì´ë²¤íŠ¸ ìƒì„¸ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\\n"
            
            # HTML ìƒì„±
            html_content = f"<h1>{event_info.get('title', 'KT ì§€ë‚œ ì´ë²¤íŠ¸')}</h1>"
            if event_info.get('period'):
                html_content += f"<p><strong>ê¸°ê°„</strong>: {event_info['period']}</p>"
            if event_info.get('target'):
                html_content += f"<p><strong>ëŒ€ìƒ</strong>: {event_info['target']}</p>"
            if event_info.get('announcement'):
                html_content += f"<p><strong>ë‹¹ì²¨ìë°œí‘œ</strong>: {event_info['announcement']}</p>"
            if event_info.get('inquiry'):
                html_content += f"<p><strong>ë¬¸ì˜</strong>: {event_info['inquiry']}</p>"
            if event_info.get('d_day'):
                html_content += f"<p><strong>ìƒíƒœ</strong>: {event_info['d_day']}</p>"
            
            if content_html:
                html_content += content_html
            else:
                html_content += "<p>ì´ë²¤íŠ¸ ìƒì„¸ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.</p>"
            
            # URLì—ì„œ pcEvtNo ì¶”ì¶œ
            evt_no_match = re.search(r'pcEvtNo=(\d+)', url)
            evt_no = evt_no_match.group(1) if evt_no_match else 'unknown'
            
            await browser.close()
            
            return {
                "datas": [{
                    "markdown": markdown_content,
                    "html": html_content,
                    "url": url,
                    "metadata": {
                        "title": event_info.get('title', 'KT ì§€ë‚œ ì´ë²¤íŠ¸'),
                        "evt_no": evt_no,
                        "period": event_info.get('period', ''),
                        "target": event_info.get('target', ''),
                        "announcement": event_info.get('announcement', ''),
                        "inquiry": event_info.get('inquiry', ''),
                        "d_day": event_info.get('d_day', ''),
                        "status": "ì¢…ë£Œ",
                        "startdate": startdate,
                        "enddate": enddate,
                        "iframe_src": event_info.get('iframe_src', ''),
                        "special_processed": True,
                        "playwright_processed": True
                    }
                }]
            }
            
        except Exception as e:
            logging.error(f"âŒ KT ì§€ë‚œ ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            await browser.close()
            
            # URLì—ì„œ pcEvtNo ì¶”ì¶œ
            evt_no_match = re.search(r'pcEvtNo=(\d+)', url)
            evt_no = evt_no_match.group(1) if evt_no_match else 'unknown'
            
            return {
                "datas": [{
                    "markdown": f"# KT ì§€ë‚œ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨\\n\\nì´ë²¤íŠ¸ ë²ˆí˜¸: {evt_no}\\n\\nì˜¤ë¥˜: {str(e)}",
                    "html": f"<h1>KT ì§€ë‚œ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨</h1><p>ì´ë²¤íŠ¸ ë²ˆí˜¸: {evt_no}</p><p>ì˜¤ë¥˜: {str(e)}</p>",
                    "url": url,
                    "metadata": {
                        "title": "KT ì§€ë‚œ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨",
                        "evt_no": evt_no,
                        "error": str(e),
                        "status": "ì˜¤ë¥˜"
                    }
                }]
            }


# KT ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ë“±ë¡
register_page_handler(
    r'https?://event\.kt\.com/html/event/ongoing_event_list\.html',
    handle_kt_event_main
)

register_page_handler(
    r'https?://event\.kt\.com/html/event/ongoing_event_view\.html\?.*pcEvtNo=\d+',
    handle_kt_event_detail
)
# # KT ì§€ë‚œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ë“±ë¡
# register_page_handler(
#     r'https?://event\.kt\.com/html/event/past_event_list\.html',
#     handle_kt_past_event_main
# )

# register_page_handler(
#     r'https?://event\.kt\.com/html/event/past_event_view\.html\?.*pcEvtNo=\d+',
#     handle_kt_past_event_detail
# )


# =========================
# 10-F. KT Shop ì•¡ì„¸ì„œë¦¬ ëª©ë¡/ìƒì„¸ í•¸ë“¤ëŸ¬
# =========================
async def handle_accessory_detail(url: str, fclient, context=None) -> Optional[Dict[str, Any]]:
    from markdownify import markdownify as md

    logging.info(f"ì•¡ì„¸ì„œë¦¬ ìƒì„¸ í•¸ë“¤ëŸ¬ ì‹œì‘: url={url}")

    async def _process_detail(ctx) -> Optional[Dict[str, Any]]:
        page = await ctx.new_page()
        status_detail = None
        try:
            try:
                response_detail = await page.goto(url, wait_until='networkidle', timeout=60000)
            except AsyncTimeoutError as te:
                logging.warning(f"ì•¡ì„¸ì„œë¦¬ ìƒì„¸ í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ(networkidle) - ì¬ì‹œë„ ì‹œë„: {te}")
                try:
                    response_detail = await page.goto(url, wait_until='load', timeout=45000)
                except AsyncTimeoutError as te2:
                    logging.warning(f"ì•¡ì„¸ì„œë¦¬ ìƒì„¸ í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ(load) - ìµœì¢… ì¬ì‹œë„(domcontentloaded): {te2}")
                    try:
                        response_detail = await page.goto(url, wait_until='domcontentloaded', timeout=30000)
                    except AsyncTimeoutError as te3:
                        logging.error(f"ì•¡ì„¸ì„œë¦¬ ìƒì„¸ í˜ì´ì§€ ë¡œë“œ ìµœì¢… íƒ€ì„ì•„ì›ƒ(domcontentloaded): {te3}")
                        # Playwright ë¡œë”©ì´ ëª¨ë‘ ì‹¤íŒ¨í•œ ê²½ìš°: fclient í´ë°± ìˆ˜í–‰
                        if fclient:
                            try:
                                logging.info("ì•¡ì„¸ì„œë¦¬ ìƒì„¸: Playwright íƒ€ì„ì•„ì›ƒìœ¼ë¡œ fclient í´ë°± ìˆ˜í–‰")
                                fallback = await fclient.scrape_single_url(url)
                                combined_html = fallback.get('html', '')
                                markdown = fallback.get('markdown', '')
                                result = {
                                    'url': url,
                                    'murl': to_mshop_url(url),
                                    'title': '',
                                    'html': combined_html,
                                    'markdown': markdown,
                                    'status_code': None,
                                    'recommendations': [],
                                    'special_processed': True,
                                    'playwright_processed': False
                                }
                                return result
                            except Exception as fallback_exc:
                                logging.warning(f"ì•¡ì„¸ì„œë¦¬ ìƒì„¸ fclient í´ë°± ì‹¤íŒ¨: {fallback_exc}")
                        return None
            status_detail = response_detail.status if response_detail else None

            if status_detail and status_detail >= 400:
                logging.error(f"âŒ ì•¡ì„¸ì„œë¦¬ ìƒì„¸ ({url}): HTTP {status_detail} ì˜¤ë¥˜")
            else:
                logging.info(f"âœ… ì•¡ì„¸ì„œë¦¬ ìƒì„¸ ({url}): HTTP {status_detail or 'unknown'}")

            await page.wait_for_timeout(1500)

            title = await page.evaluate("document.querySelector('.ui-prd_tit')?.textContent?.trim() || ''")
            info_html = await page.evaluate("document.querySelector('.ui-view-info')?.outerHTML || ''")
            tab_html = await page.evaluate("document.querySelector('.ui-prdView-tab')?.outerHTML || ''")
            recommend_html = await page.evaluate("document.querySelector('.ui-viewPrd-cont.ui-best-cont')?.outerHTML || ''")

            combined_html_parts = [part for part in [info_html, tab_html] if part]
            combined_html = "\n".join(combined_html_parts)
            markdown = md(combined_html) if combined_html else ''

            if (not combined_html or not markdown) and fclient:
                try:
                    logging.info("ì•¡ì„¸ì„œë¦¬ ìƒì„¸ ê¸°ë³¸ ì¶”ì¶œ ì‹¤íŒ¨, fclient fallback ì‹œë„")
                    fallback = await fclient.scrape_single_url(url)
                    combined_html = fallback.get('html', combined_html)
                    markdown = fallback.get('markdown', markdown)
                except Exception as fallback_exc:
                    logging.warning(f"ì•¡ì„¸ì„œë¦¬ ìƒì„¸ fallback ì‹¤íŒ¨: {fallback_exc}")

            recommendations = await page.evaluate("""() => {
                const results = [];
                const seen = new Set();
                const container = document.querySelector('.ui-viewPrd-cont.ui-best-cont');
                if (!container) {
                    return results;
                }
                container.querySelectorAll('li a').forEach(a => {
                    const href = a.getAttribute('href') || '';
                    let abs = '';
                    if (href) {
                        try {
                            abs = new URL(href, window.location.href).href;
                        } catch (err) {
                            abs = href;
                        }
                    }
                    if (!abs || seen.has(abs)) {
                        return;
                    }
                    seen.add(abs);
                    const name = (a.querySelector('.prd-tit')?.textContent || a.textContent || '').trim();
                    const desc = (a.querySelector('.total-price em, .price em, .prd-price em')?.textContent || '').trim();
                    const image = a.querySelector('img')?.src || '';
                    if (!name && !desc) {
                        return;
                    }
                    results.push({
                        kind: 'best',
                        name,
                        desc,
                        url: abs,
                        image
                    });
                });
                return results;
            }""")

            result = {
                'url': url,
                'murl': to_mshop_url(url),
                'title': title,
                'html': combined_html,
                'markdown': markdown,
                'status_code': status_detail,
                'recommendations': recommendations or [],
                'special_processed': True,
                'playwright_processed': True
            }

            if recommend_html:
                result['recommendations_html'] = recommend_html

            return result
        except Exception as exc:
            logging.warning(f"ì•¡ì„¸ì„œë¦¬ ìƒì„¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {exc}")
            return None
        finally:
            try:
                await page.close()
            except Exception:
                pass

    if context is not None:
        return await _process_detail(context)

    from playwright.async_api import async_playwright

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context_local = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'
        )
        try:
            return await _process_detail(context_local)
        finally:
            await browser.close()


async def handle_accessory_display_list(url: str, fclient, menu: str = None) -> dict:
    from markdownify import markdownify as md

    logging.info(f"ì•¡ì„¸ì„œë¦¬ display í•¸ë“¤ëŸ¬ ì‹œì‘: url={url}, menu={menu}")

    menus: List[Dict[str, Any]] = []
    datas: List[Dict[str, Any]] = []
    seen_prodnos: Set[str] = set()

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        response = await page.goto(url, wait_until='networkidle', timeout=60000)
        status_code = response.status if response else None

        if status_code and status_code >= 400:
            logging.error(f"âŒ ì•¡ì„¸ì„œë¦¬ ëª©ë¡ ({url}): HTTP {status_code} ì˜¤ë¥˜")
        else:
            logging.info(f"âœ… ì•¡ì„¸ì„œë¦¬ ëª©ë¡ ({url}): HTTP {status_code or 'unknown'}")

        await page.wait_for_timeout(1500)

        async def extract_items() -> List[Dict[str, Any]]:
            return await page.evaluate("""() => {
                return Array.from(document.querySelectorAll('ul.ui-access-prdLst li a.ui-btn-access')).map((a, index) => ({
                    prodNo: a.getAttribute('prodno') || '',
                    onclick: a.getAttribute('onclick') || '',
                    title: (a.querySelector('.prd-tit')?.textContent || a.textContent || '').trim(),
                    price: (a.querySelector('.total-price em, .price-txt em, .payment em')?.textContent || '').replace(/[^0-9]/g, ''),
                    image: a.querySelector('img')?.src || '',
                    index
                }));
            }""")

        async def get_current_page() -> int:
            try:
                current = await page.evaluate("""() => {
                    const strong = document.querySelector('.pageWrap strong');
                    return strong ? strong.textContent.trim() : '';
                }""")
                return int(current or '1')
            except Exception:
                return 1

        async def goto_page(target: int) -> bool:
            locator = page.locator('.pageWrap a', has_text=str(target))
            if await locator.count() > 0:
                try:
                    await locator.first.click()
                    await page.wait_for_load_state('networkidle')
                    await page.wait_for_timeout(800)
                    return True
                except Exception as exc:
                    logging.debug(f"í˜ì´ì§€ ë²ˆí˜¸ {target} ì´ë™ ì‹¤íŒ¨: {exc}")
            arrow_locator = page.locator('.pageWrap a')
            count = await arrow_locator.count()
            for idx in range(count):
                try:
                    text = (await arrow_locator.nth(idx).inner_text()).strip()
                except Exception:
                    continue
                if text == '>':
                    try:
                        await arrow_locator.nth(idx).click()
                        await page.wait_for_load_state('networkidle')
                        await page.wait_for_timeout(800)
                        return True
                    except Exception as exc:
                        logging.debug(f"ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {exc}")
            return False

        async def fetch_detail(prod_no: str, title_hint: str) -> Optional[Dict[str, Any]]:
            detail_url = f"https://shop.kt.com/accessory/accsProductView.do?prodNo={prod_no}"
            detail = await handle_accessory_detail(detail_url, fclient, context)
            if not detail:
                return None
            if not detail.get('title') and title_hint:
                detail['title'] = title_hint
            return detail

        current_page = await get_current_page()

        while True:
            items = await extract_items()
            logging.info(f"í˜ì´ì§€ {current_page}: {len(items)}ê°œ ìƒí’ˆ ë°œê²¬")

            for item in items:
                prod_no = (item.get('prodNo') or '').strip()
                title_hint = (item.get('title') or '').strip()
                if not prod_no or prod_no in seen_prodnos:
                    continue
                seen_prodnos.add(prod_no)

                detail = await fetch_detail(prod_no, title_hint)
                if not detail:
                    continue

                base_menu = (menu or '').strip()
                menu_name = f"{base_menu}^{detail['title']}" if base_menu else f"Shop^ì•¡ì„¸ì„œë¦¬ êµ¬ë§¤^{detail['title']}"

                menus.append({'menu': menu_name, 'url': detail['url'], 'murl': detail.get('murl')})
                datas.append(detail)

            next_target = current_page + 1
            moved = await goto_page(next_target)
            if not moved:
                break

            new_page = await get_current_page()
            if new_page == current_page:
                break
            current_page = new_page

        await browser.close()

    logging.info(f"ì´ {len(datas)}ê°œ ì•¡ì„¸ì„œë¦¬ ìƒì„¸ ì²˜ë¦¬ ì™„ë£Œ")
    return {
        'menus': menus,
        'datas': datas,
        'total_processed': len(datas),
        'status': 'completed',
        'special_processed': True,
        'playwright_processed': True
    }


register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR042901',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR042902',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR042903',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR043002',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR043004',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR043005',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR043006',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR043007',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR043101',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR043102',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR043103',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR043104',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR043105',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR043401',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR043402',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR043501',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR043502',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR043503',
    handle_accessory_display_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR043504',
    handle_accessory_display_list
)
# =========================
# 9. ì˜í™”ì˜ˆë§¤ ê³ ê°ì„¼í„° FAQ í•¸ë“¤ëŸ¬
# =========================

async def handle_movie_customer_center_faq_playwright(url: str, fclient, menu=None) -> dict:
    """
    ì˜í™”ì˜ˆë§¤ ê³ ê°ì„¼í„° FAQ í˜ì´ì§€ ì²˜ë¦¬ í•¸ë“¤ëŸ¬
    - iframe ë‚´ë¶€ì˜ ì‹¤ì œ FAQ URLì„ ì²˜ë¦¬í•˜ì—¬ êµ¬ì¡°í™”ëœ FAQë¡œ ë³€í™˜
    - ëª¨ë“  ì¹´í…Œê³ ë¦¬ì™€ í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
    - FAQ ì™¸ì˜ ì¼ë°˜ í˜ì´ì§€ ë‚´ìš©ë„ í•¨ê»˜ ì¶”ì¶œ
    """
    logging.info(f"ì˜í™”ì˜ˆë§¤ ê³ ê°ì„¼í„° FAQ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}")
    
    # ì¹´í…Œê³ ë¦¬ ì •ì˜
    categories = [
        {"id": "7", "name": "ì‹ ê·œì´ìš©ì"},
        {"id": "10", "name": "ì˜ˆë§¤ ê´€ë ¨"},
        {"id": "12", "name": "ê²°ì œ ê´€ë ¨"},
        {"id": "13", "name": "ì˜ˆë§¤ ì·¨ì†Œ"}
    ]
    
    all_qa_list = []
    page_content = ""
    page_html = ""
    
    # ë¨¼ì € ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ (FAQ ì œì™¸) - Playwright ì‚¬ìš©
    try:
        logging.info("ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì‹œì‘")
        from playwright.async_api import async_playwright
        from markdownify import markdownify as md
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            await page.goto(url, wait_until='domcontentloaded')
            await page.wait_for_timeout(3000)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # iframe ì²˜ë¦¬ - FAQ ë‚´ìš©ì´ iframe ì•ˆì— ìˆìœ¼ë¯€ë¡œ iframeìœ¼ë¡œ ì´ë™
            iframe_element = await page.query_selector('#iFrmMileage')
            if iframe_element:
                logging.info("iframe ë°œê²¬, iframe ë‚´ë¶€ë¡œ ì´ë™")
                frame = await iframe_element.content_frame()
                
                if frame:
                    # iframe ë‚´ë¶€ì—ì„œ content-box ìš”ì†Œ ì°¾ê¸°
                    content_element = await frame.query_selector('div.content-box')
                    if content_element:
                        # FAQ ê´€ë ¨ ìš”ì†Œë“¤ ë° ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                        faq_selectors = [
                            # FAQ ê´€ë ¨
                            '.faq_box', '.faq', '.faq-list', '.faq-item', '.inquiry', '.answer', '.faqClass',
                            'img[src*="faq"]', 'img[src*="FAQ"]',
                            'a[href*="faq"]', 'a[href*="FAQ"]',
                            'p:has-text("FAQ")', 'p:has-text("faq")',
                            'div:has-text("FAQ")', 'div:has-text("faq")',
                            'span:has-text("FAQ")', 'span:has-text("faq")',
                            'li:has-text("FAQ")', 'li:has-text("faq")',
                            # Header/Footer/Navigation
                            'header', 'footer', 
                            '.header', '.footer',
                            '#header', '#footer',
                            '#cfmClHeader', '#cfmClFooter',
                            '.inner', 'nav', '.navigation'
                        ]
                        
                        for selector in faq_selectors:
                            try:
                                elements = await content_element.query_selector_all(selector)
                                for element in elements:
                                    await element.evaluate('element => element.remove()')
                            except:
                                pass  # ì…€ë ‰í„°ê°€ ìœ íš¨í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
                        
                        page_html = await content_element.inner_html()
                        page_markdown = md(page_html) if page_html else ""
                        logging.info(f"iframe ë‚´ë¶€ì—ì„œ ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì™„ë£Œ: ë§ˆí¬ë‹¤ìš´ {len(page_markdown)}ì, HTML {len(page_html)}ì")
                    else:
                        logging.warning("iframe ë‚´ë¶€ì—ì„œ content-box ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        page_markdown = ""
                        page_html = ""
                else:
                    logging.warning("iframeì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (cross-origin)")
                    page_markdown = ""
                    page_html = ""
            else:
                # iframeì´ ì—†ìœ¼ë©´ cfmClContentsì—ì„œ ì¶”ì¶œ
                content_element = await page.query_selector('#cfmClContents')
                if content_element:
                    page_html = await content_element.inner_html()
                    page_markdown = md(page_html) if page_html else ""
                    logging.info(f"cfmClContentsì—ì„œ ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì™„ë£Œ: ë§ˆí¬ë‹¤ìš´ {len(page_markdown)}ì, HTML {len(page_html)}ì")
                else:
                    logging.warning("cfmClContents ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    page_markdown = ""
                    page_html = ""
            
            await browser.close()
        
    except Exception as e:
        logging.error(f"ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        page_markdown = ""
        page_html = ""
    
    # FAQ ì¶”ì¶œ
    logging.info("FAQ ì¶”ì¶œ ì‹œì‘")
    
    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ FAQ ì¶”ì¶œ
    for category in categories:
        logging.info(f"ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹œì‘: {category['name']}")
        
        page_num = 1
        category_qa_count = 0
        
        while True:
            # ì¹´í…Œê³ ë¦¬ë³„ í˜ì´ì§€ URL êµ¬ì„±
            category_url = f"https://showmovie.mobile.kt.com/Customer/FaqList.aspx?qIdx={category['id']}&Page={page_num}"
            logging.info(f"  í˜ì´ì§€ {page_num} ì²˜ë¦¬: {category_url}")
            
            try:
                # HTTP ìš”ì²­ìœ¼ë¡œ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ë” ë¹ ë¦„)
                import aiohttp
                async with aiohttp.ClientSession() as session:
                    async with session.get(category_url) as response:
                        # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
                        if response.status >= 400:
                            logging.error(f"âŒ ì˜í™”ì˜ˆë§¤ FAQ ({category_url}): HTTP {response.status} ì˜¤ë¥˜")
                        elif response.status >= 300:
                            logging.warning(f"âš ï¸ ì˜í™”ì˜ˆë§¤ FAQ ({category_url}): HTTP {response.status} ë¦¬ë‹¤ì´ë ‰íŠ¸")
                        else:
                            logging.info(f"âœ… ì˜í™”ì˜ˆë§¤ FAQ ({category_url}): HTTP {response.status} ì„±ê³µ")
                        
                        if response.status == 200:
                            page_content = await response.text()
                        else:
                            logging.warning(f"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {response.status}")
                            break
                
                # HTMLì—ì„œ FAQ íŒŒì‹±
                page_faqs = parse_movie_faq_from_html_content(page_content, category['name'])
                
                if not page_faqs:
                    logging.info(f"  í˜ì´ì§€ {page_num}ì— FAQê°€ ì—†ìŒ. ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¢…ë£Œ")
                    break
                
                # ê²°ê³¼ì— ì¶”ê°€
                for faq in page_faqs:
                    faq['page'] = page_num
                    all_qa_list.append(faq)
                    category_qa_count += 1
                
                logging.info(f"  í˜ì´ì§€ {page_num} ì™„ë£Œ: {len(page_faqs)}ê°œ FAQ ì¶”ì¶œ")
                
                # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸ (í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸)
                if f'Page={page_num + 1}' in page_content:
                    page_num += 1
                else:
                    logging.info(f"  ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ìŒ. ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¢…ë£Œ")
                    break
                    
            except Exception as e:
                logging.error(f"ì¹´í…Œê³ ë¦¬ {category['name']} í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                break
        
        logging.info(f"ì¹´í…Œê³ ë¦¬ {category['name']} ì²˜ë¦¬ ì™„ë£Œ: ì´ {category_qa_count}ê°œ FAQ")
    
    logging.info(f"ì˜í™”ì˜ˆë§¤ ê³ ê°ì„¼í„° FAQ ì „ì²´ ì¶”ì¶œ ì™„ë£Œ: ì´ {len(all_qa_list)}ê°œ FAQ")
    
    # crawl4aiì—ì„œ ì´ë¯¸ ë§ˆí¬ë‹¤ìš´ì„ ë°›ì•˜ìœ¼ë¯€ë¡œ ì¶”ê°€ ë³€í™˜ ë¶ˆí•„ìš”
    
    logging.info(f"qa_list ì¤€ë¹„ ì™„ë£Œ: {len(all_qa_list)}ê°œ FAQ")
    
    return {
        "url": url,  # URL í•„ë“œ ì¶”ê°€ (url.txt ìƒì„±ìš©)
        "markdown": page_markdown,  # FAQ ì œì™¸í•œ ì¼ë°˜ í˜ì´ì§€ ë‚´ìš©ë§Œ
        "html": page_html,  # HTMLì€ ìœ ì§€
        "qa_list": all_qa_list,  # FAQ ë°ì´í„°ë§Œ ë³„ë„ ì €ì¥
        "total_categories": len(categories),
        "total_qa": len(all_qa_list),
        "special_processed": True,
        "playwright_processed": True
    }

def parse_movie_faq_from_html_content(html_content: str, category_name: str) -> list:
    """HTML ë‚´ìš©ì—ì„œ FAQ ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ë°˜í™˜"""
    try:
        import re
        
        faqs = []
        
        # ì •ê·œì‹ìœ¼ë¡œ FAQ íŒ¨í„´ ì°¾ê¸°
        faq_pattern = r'<a href="#" class="inquiry"><em class="icon_q">Q ì§ˆë¬¸</em><span>([^<]+)</span></a>\s*<div class="answer">.*?<div class="answer-inner">\s*<p>(.*?)</p>'
        matches = re.findall(faq_pattern, html_content, re.DOTALL)
        
        logging.info(f"  {category_name} ì¹´í…Œê³ ë¦¬ì—ì„œ ë°œê²¬ëœ FAQ: {len(matches)}ê°œ")
        
        for idx, (question, answer) in enumerate(matches):
            try:
                question = question.strip()
                answer = answer.strip()
                
                # HTML ì—”í‹°í‹° ë””ì½”ë”© ë° ì •ë¦¬
                question = question.replace('&gt;', '>').replace('&lt;', '<').replace('&amp;', '&')
                answer = answer.replace('&gt;', '>').replace('&lt;', '<').replace('&amp;', '&')
                answer = answer.replace('<br/>', '\n').replace('<br>', '\n')
                
                if question and answer:
                    faqs.append({
                        "category": category_name,
                        "question": question,
                        "answer": answer
                    })
                    logging.info(f"    FAQ {idx+1} íŒŒì‹± ì™„ë£Œ: {question[:50]}...")
            
            except Exception as e:
                logging.error(f"    FAQ í•­ëª© {idx+1} íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                continue
        
        return faqs
        
    except Exception as e:
        logging.error(f"HTML íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        return []
def parse_movie_faq_from_html(html_content: str) -> dict:
    """HTML ë‚´ìš©ì—ì„œ FAQë¥¼ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜"""
    try:
        import re
        
        markdown_body = ""
        all_qa_list = []
        
        logging.info(f"HTML íŒŒì‹± ì‹œì‘: ë‚´ìš© ê¸¸ì´ {len(html_content)} ë¬¸ì")
        
        # ì •ê·œì‹ìœ¼ë¡œ FAQ íŒ¨í„´ ì°¾ê¸°
        # <span>ì§ˆë¬¸</span> ë‹¤ìŒì— ì˜¤ëŠ” <div class="answer-inner"><p>ë‹µë³€</p></div> íŒ¨í„´
        faq_pattern = r'<span>([^<]+)</span>.*?<div class="answer-inner">\s*<p>(.*?)</p>'
        matches = re.findall(faq_pattern, html_content, re.DOTALL)
        
        logging.info(f"ì •ê·œì‹ìœ¼ë¡œ ë°œê²¬ëœ FAQ íŒ¨í„´: {len(matches)}ê°œ")
        
        if len(matches) < 5:  # ì˜ˆìƒë³´ë‹¤ ì ìœ¼ë©´ ë‹¤ë¥¸ íŒ¨í„´ ì‹œë„
            # ë” í¬ê´„ì ì¸ íŒ¨í„´ìœ¼ë¡œ ì¬ì‹œë„
            faq_pattern2 = r'<a href="#" class="inquiry"><em class="icon_q">Q ì§ˆë¬¸</em><span>([^<]+)</span></a>\s*<div class="answer">.*?<div class="answer-inner">\s*<p>(.*?)</p>'
            matches2 = re.findall(faq_pattern2, html_content, re.DOTALL)
            logging.info(f"í¬ê´„ì  íŒ¨í„´ìœ¼ë¡œ ë°œê²¬ëœ FAQ: {len(matches2)}ê°œ")
            if len(matches2) > len(matches):
                matches = matches2
        
        for idx, (question, answer) in enumerate(matches):
            try:
                question = question.strip()
                answer = answer.strip()
                
                # HTML ì—”í‹°í‹° ë””ì½”ë”©
                question = question.replace('&gt;', '>').replace('&lt;', '<').replace('&amp;', '&')
                answer = answer.replace('&gt;', '>').replace('&lt;', '<').replace('&amp;', '&')
                
                if question and answer:
                    category_name = "ì‹ ê·œì´ìš©ì"  # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬
                    all_qa_list.append({
                        "category": category_name,
                        "question": question,
                        "answer": answer,
                        "page": 1
                    })
                    logging.info(f"ì •ê·œì‹ FAQ íŒŒì‹± ì™„ë£Œ {idx+1}: {question[:50]}...")
            
            except Exception as e:
                logging.error(f"FAQ í•­ëª© {idx+1} íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                continue
        
        logging.info(f"HTML íŒŒì‹± ì™„ë£Œ: ì´ {len(all_qa_list)}ê°œ FAQ")
        
        # qa_listëŠ” app.pyì—ì„œ ì €ì¥í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë°˜í™˜ë§Œ í•¨
        return {
            "markdown": "",  # FAQ ë‚´ìš© ì œê±°
            "qa_list": all_qa_list,
            "total_categories": 1,
            "total_qa": len(all_qa_list),
            "special_processed": True,
            "playwright_processed": True
        }
        
    except Exception as e:
        logging.error(f"HTML íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        return {
            "markdown": "",
            "qa_list": [],
            "total_categories": 0,
            "total_qa": 0,
            "special_processed": True,
            "playwright_processed": True
        }

def parse_movie_faq_from_markdown(markdown_content: str) -> dict:
    """ë§ˆí¬ë‹¤ìš´ ë‚´ìš©ì—ì„œ FAQë¥¼ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜"""
    try:
        import re
        
        markdown_body = ""
        all_qa_list = []
        
        # FAQ íŒ¨í„´ ì°¾ê¸°
        # "Q ì§ˆë¬¸" ë‹¤ìŒì— ì˜¤ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì§ˆë¬¸ìœ¼ë¡œ, ê·¸ ë‹¤ìŒ ë¸”ë¡ì„ ë‹µë³€ìœ¼ë¡œ ì¶”ì¶œ
        faq_pattern = r'\*Q ì§ˆë¬¸\*([^\n]+)\n\n\s*\*Q\*\n\n(.+?)(?=\n\n\s*ìƒì„¸ë³´ê¸° ë‹«í˜|\n\n\*Q ì§ˆë¬¸\*|$)'
        matches = re.findall(faq_pattern, markdown_content, re.DOTALL)
        
        logging.info(f"ë§ˆí¬ë‹¤ìš´ì—ì„œ ë°œê²¬ëœ FAQ íŒ¨í„´: {len(matches)}ê°œ")
        
        for idx, (question, answer) in enumerate(matches):
            question = question.strip()
            answer = answer.strip()
            
            if question and answer:
                category_name = "ì‹ ê·œì´ìš©ì"  # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬
                all_qa_list.append({
                    "category": category_name,
                    "question": question,
                    "answer": answer,
                    "page": 1
                })
                logging.info(f"ë§ˆí¬ë‹¤ìš´ FAQ íŒŒì‹± ì™„ë£Œ {idx+1}: {question[:50]}...")
        
        logging.info(f"ë§ˆí¬ë‹¤ìš´ íŒŒì‹± ì™„ë£Œ: ì´ {len(all_qa_list)}ê°œ FAQ")
        
        return {
            "markdown": "",  # FAQ ë‚´ìš© ì œê±°
            "qa_list": all_qa_list,
            "total_categories": 1,
            "total_qa": len(all_qa_list),
            "special_processed": True,
            "playwright_processed": True
        }
        
    except Exception as e:
        logging.error(f"ë§ˆí¬ë‹¤ìš´ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        return {
            "markdown": "",
            "qa_list": [],
            "total_categories": 0,
            "total_qa": 0,
            "special_processed": True,
            "playwright_processed": True
        }



# ì˜í™”ì˜ˆë§¤ ê³ ê°ì„¼í„° FAQ í•¸ë“¤ëŸ¬ ë“±ë¡
register_page_handler(
    r'https?://membership\.kt\.com/culture/movie/CustomerCenterInfo\.do',
    handle_movie_customer_center_faq_playwright
)
async def handle_ermsweb_faq_all_playwright(url: str, fclient, menu=None) -> dict:
    """
    ëª¨ë“  ì¹´í…Œê³ ë¦¬, ëª¨ë“  í˜ì´ì§€, ëª¨ë“  Q/Aë¥¼ gigagenie handler í¬ë§·ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” handler
    - íƒ€ì„ì•„ì›ƒ ê°œì„  ë° ì•ˆì •ì„± í–¥ìƒ
    """
    logging.info(f"ERMS FAQ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}")
    
    # ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ì„¤ì •
    max_retries = 2
    base_timeout = 60000  # 60ì´ˆ ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ
    
    for attempt in range(max_retries):
        try:
            logging.info(f"ERMS FAQ í˜ì´ì§€ ì§„ì… ì‹œë„ {attempt + 1}/{max_retries}: url={url}")
            
            # ì‹œë„ë³„ë¡œ ë‹¤ë¥¸ ë¡œë”© ì „ëµ ì ìš©
            if attempt == 0:
                wait_until = "domcontentloaded"
                timeout = 50000
                extra_wait = 5000
            else:
                wait_until = "networkidle"
                timeout = base_timeout
                extra_wait = 8000
            
            # ì™„ì „íˆ ìƒˆë¡œìš´ ë¸Œë¼ìš°ì € ì„¸ì…˜ìœ¼ë¡œ ì‹œì‘
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                
                # í˜ì´ì§€ ë¡œë“œ
                response = await page.goto(url, wait_until=wait_until, timeout=timeout)
                await page.wait_for_timeout(extra_wait)
                
                # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸ ë° ë¡œê¹…
                status_code = response.status if response else None
                if status_code:
                    if status_code >= 400:
                        logging.error(f"âŒ ERMS FAQ ({url}): HTTP {status_code} ì˜¤ë¥˜")
                    elif status_code >= 300:
                        logging.warning(f"âš ï¸ ERMS FAQ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
                    else:
                        logging.info(f"âœ… ERMS FAQ ({url}): HTTP {status_code} ì„±ê³µ")
                else:
                    logging.debug(f"ğŸ” ERMS FAQ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")
                
                # í˜ì´ì§€ ë¡œë”© ìƒíƒœ í™•ì¸
                try:
                    await page.wait_for_selector("ul#tab-slide-menu li a", timeout=30000)
                    logging.info("ERMS FAQ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
                except Exception as e:
                    logging.warning(f"ERMS FAQ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹¤íŒ¨: {e}, ê³„ì† ì§„í–‰")

                markdown_body = ""
                all_qa_list = []
                page_html = ""
                
                # ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ (FAQ ì œì™¸) - Playwright ì‚¬ìš©
                try:
                    logging.info("ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì‹œì‘")
                    from markdownify import markdownify as md
                    
                    # FAQ ê´€ë ¨ ìš”ì†Œë“¤ ë° ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    faq_selectors = [
                        # FAQ ê´€ë ¨
                        'ul#faqList', '.faqList', 
                        '.accordion-area', '.accordion',
                        '.faq_box', '.faq', '.faq-list', '.faq-item', 
                        '.inquiry', '.answer', '.faqClass',
                        'img[src*="faq"]', 'img[src*="FAQ"]',
                        'a[href*="faq"]', 'a[href*="FAQ"]',
                        'ul.accordions',  # ERMS FAQ ë¦¬ìŠ¤íŠ¸
                        # Header/Footer/Navigation
                        'header', 'footer', 
                        '.header', '.footer',
                        '#header', '#footer',
                        '#cfmClHeader', '#cfmClFooter',
                        '.inner', 'nav', '.navigation'
                    ]
                    
                    for selector in faq_selectors:
                        try:
                            elements = await page.query_selector_all(selector)
                            for element in elements:
                                await element.evaluate('element => element.remove()')
                        except:
                            pass  # ì…€ë ‰í„°ê°€ ìœ íš¨í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
                    
                    page_html = await page.content()
                    page_markdown = md(page_html) if page_html else ""
                    logging.info(f"ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì™„ë£Œ: ë§ˆí¬ë‹¤ìš´ {len(page_markdown)}ì, HTML {len(page_html)}ì")
                except Exception as e:
                    logging.error(f"ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                    page_markdown = ""
                    page_html = ""
                
                # FAQ ì¶”ì¶œì„ ìœ„í•´ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
                await page.reload()
                await page.wait_for_timeout(3000)

                # 1. ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                categories = await page.query_selector_all("ul#tab-slide-menu li a")
                category_info = []
                for a in categories:
                    nodeid = await a.get_attribute("data-nodeid")
                    nodename = await a.get_attribute("data-nodename") or (await a.inner_text()).replace('\n', ' ').strip()
                    category_info.append({"nodeid": nodeid, "nodename": nodename, "element": a})
                
                logging.info(f"ì´ {len(category_info)}ê°œ ì¹´í…Œê³ ë¦¬ ë°œê²¬: {[cat['nodename'] for cat in category_info]}")

                for cat_idx, cat in enumerate(category_info):
                    # ì¹´í…Œê³ ë¦¬ í´ë¦­
                    await cat["element"].click()
                    await page.wait_for_timeout(1500)
                    faq_category = cat["nodename"]
                    logging.info(f"ì¹´í…Œê³ ë¦¬ {cat_idx + 1}/{len(category_info)} ì²˜ë¦¬ ì‹œì‘: {faq_category}")

                    # ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                    last_page = 1
                    page_links = await page.query_selector_all('.pagination .scope a')
                    for a in page_links:
                        txt = (await a.inner_text()).strip()
                        if txt.isdigit():
                            last_page = max(last_page, int(txt))
                        if 'last' in (await a.get_attribute('class') or ''):
                            href = await a.get_attribute('href')
                            m = re.search(r'gotoAjax\((\d+),', href or '')
                            if m:
                                last_page = max(last_page, int(m.group(1)))
                    
                    logging.info(f"ì¹´í…Œê³ ë¦¬ '{faq_category}': ì´ {last_page}ê°œ í˜ì´ì§€ ë°œê²¬")

                    category_qa_count = 0
                    for page_num in range(1, last_page + 1):
                        # í˜ì´ì§€ ì´ë™ (1í˜ì´ì§€ëŠ” ì´ë¯¸ ë¡œë”©ë¨)
                        if page_num > 1:
                            page_btns = await page.query_selector_all('.pagination .scope a')
                            for btn in page_btns:
                                if (await btn.inner_text()).strip() == str(page_num):
                                    await btn.click()
                                    await page.wait_for_timeout(1500)
                                    break

                        # Q/A ì¶”ì¶œ
                        faq_items = await page.query_selector_all('ul.accordions > li.liWrap')
                        logging.info(f"í˜ì´ì§€ {page_num}/{last_page}: {len(faq_items)}ê°œ FAQ í•­ëª© ë°œê²¬")
                        for li in faq_items:
                            try:
                                category_elem = await li.query_selector('.linked')
                                category = await category_elem.inner_text() if category_elem else ""
                                question_elem = await li.query_selector('.qna span')
                                if question_elem:
                                    question = await question_elem.inner_text()
                                else:
                                    qna_elem = await li.query_selector('.qna')
                                    question = await qna_elem.inner_text() if qna_elem else ""
                                trigger = await li.query_selector('.accordion-trigger')
                                answerDiv = await li.query_selector('.accordion-contents')
                                if answerDiv and (await answerDiv.evaluate('el => getComputedStyle(el).display')) == 'none':
                                    if trigger:
                                        await trigger.click()
                                        await page.wait_for_timeout(500)
                                answer_elem = await li.query_selector('.accordion-contents .faqClass')
                                answer = await answer_elem.inner_text() if answer_elem else ""
                                # ë§ˆí¬ë‹¤ìš´/êµ¬ì¡°í™”
                                markdown_body += f"{category}\n{question}\n{answer}\n***\n"
                                all_qa_list.append({
                                    "category": category,
                                    "question": question,
                                    "answer": answer,
                                    "page": page_num
                                })
                                category_qa_count += 1
                            except Exception as e:
                                logging.warning(f"FAQ í•­ëª© ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                                continue
                    
                    logging.info(f"ì¹´í…Œê³ ë¦¬ '{faq_category}' ì²˜ë¦¬ ì™„ë£Œ: ì´ {category_qa_count}ê°œ FAQ")

                await browser.close()
                
                # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ê²½ìš° ê²°ê³¼ ë°˜í™˜
                logging.info(f"ğŸ‰ ERMS FAQ ì „ì²´ ì¶”ì¶œ ì™„ë£Œ: ì´ ì¹´í…Œê³ ë¦¬ {len(category_info)}ê°œ, ì´ FAQ {len(all_qa_list)}ê°œ")
                logging.info(f"qa_list ì¤€ë¹„ ì™„ë£Œ: {len(all_qa_list)}ê°œ FAQ")
                
                return {
                    "url": url,  # URL í•„ë“œ ì¶”ê°€ (url.txt ìƒì„±ìš©)
                    "markdown": page_markdown,  # FAQ ì œì™¸í•œ ì¼ë°˜ í˜ì´ì§€ ë‚´ìš©
                    "html": page_html,
                    "qa_list": all_qa_list,
                    "total_categories": len(category_info),
                    "total_qa": len(all_qa_list),
                    "special_processed": True,
                    "playwright_processed": True
                }
                
        except Exception as e:
            if attempt < max_retries - 1:
                logging.warning(f"ERMS FAQ í˜ì´ì§€ ì²˜ë¦¬ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {str(e)} - ì¬ì‹œë„ ì¤‘...")
                await asyncio.sleep(5)  # ì¬ì‹œë„ ì „ 5ì´ˆ ëŒ€ê¸°
                continue
            else:
                logging.error(f"ERMS FAQ í˜ì´ì§€ ì²˜ë¦¬ ìµœì¢… ì‹¤íŒ¨: {str(e)}")
                return {
                    "markdown": f"ERMS FAQ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
                    "html": f"<p>ERMS FAQ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}</p>",
                    "qa_list": [],
                    "total_categories": 0,
                    "total_qa": 0,
                    "special_processed": True,
                    "playwright_processed": True,
                    "error": str(e)
                }


# ermsweb FAQ í•¸ë“¤ëŸ¬ ë“±ë¡
register_page_handler(
    r'https?://ermsweb\.kt\.com/pc/faq/faqList\.do',
    handle_ermsweb_faq_all_playwright
)

# =========================
# 10-B. KT Shop íŒì—… ì²˜ë¦¬ ê³µí†µ í•¸ë“¤ëŸ¬ (layerOpen/hash, void(0)+.plus)
# =========================
async def handle_ktshop_popup_extractor(url: str, fclient, menu=None) -> dict:
    """
    KT Shop í˜ì´ì§€ì—ì„œ ë‹¤ìŒ ì¡°ê±´ì˜ íŠ¸ë¦¬ê±°ë¥¼ ëª¨ë‘ ìˆœíšŒí•˜ì—¬ íŒì—… ë‚´ìš©ì„ ì¶”ì¶œí•˜ê³ ,
    íŠ¸ë¦¬ê±° ìœ„ì¹˜ì— íŒì—… ë‚´ìš©ì„ ì‚½ì…í•œ ë’¤ article íƒœê·¸ì˜ ì›ë³¸ íŒì—… ì˜ì—­ì€ ì œì™¸í•˜ì—¬ ìµœì¢… ì½˜í…ì¸ ë¥¼ êµ¬ì„±í•œë‹¤.

    - í•´ì‹œ ëŒ€ìƒ + layerOpen('#id', this)
    - href="javascript:void(0)" ì´ê³  classì— 'plus' í¬í•¨

    ê·œì¹™:
    1) ê¸°ë³¸ í˜ì´ì§€ ë‚´ìš©ì€ ìœ ì§€í•˜ë˜, íŒì—… ë‚´ìš©ë§Œ íŠ¸ë¦¬ê±° ìœ„ì¹˜ì— ì¸ë¼ì¸ ì‚½ì…
    2) íŒì—… ì¶”ì¶œ í›„ ì˜¤ë²„ë ˆì´ëŠ” ë‹«ê±°ë‚˜ ë¬´ì‹œí•˜ë„ë¡ ì²˜ë¦¬
    3) íŒì—… ì™¸ ì›ë³¸ article íƒœê·¸ëŠ” ìµœì¢… HTMLì—ì„œ ì œê±°
    """
    import re
    import time
    logging.info(f"KT Shop íŒì—… ì²˜ë¦¬ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, menu={menu}")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        )
        page = await context.new_page()

        try:
            response = await page.goto(url, wait_until='domcontentloaded', timeout=60000)
            await page.wait_for_timeout(2500)

            status_code = response.status if response else None
            if status_code:
                if status_code >= 400:
                    logging.error(f"âŒ KT Shop íŒì—… ({url}): HTTP {status_code} ì˜¤ë¥˜")
                elif status_code >= 300:
                    logging.warning(f"âš ï¸ KT Shop íŒì—… ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
                else:
                    logging.info(f"âœ… KT Shop íŒì—… ({url}): HTTP {status_code} ì„±ê³µ")
            else:
                logging.debug(f"ğŸ” KT Shop íŒì—… ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")

            # íŠ¸ë¦¬ê±° ìˆ˜ì§‘: layerOpen('#id') í˜•íƒœ (ëª¨ë“  íƒœê·¸ ëŒ€ìƒ)
            hash_triggers = await page.query_selector_all("*[onclick*='layerOpen(']")
            # íŠ¸ë¦¬ê±° ìˆ˜ì§‘: javascript:void(0) + class í¬í•¨ 'plus', ë˜ëŠ” showDeviceModel/showDeviceInfo í˜¸ì¶œ ìš”ì†Œ
            plus_triggers = await page.query_selector_all(
                "a[href^='javascript:void(0)'].plus, .plus[href^='javascript:void(0)'], *[onclick*='showDeviceModel('], *[onclick*='showDeviceInfo(']"
            )

            logging.info(f"ë°œê²¬ëœ íŠ¸ë¦¬ê±°: layerOpen={len(hash_triggers)}, plus={len(plus_triggers)}")

            async def _hide_overlays():
                try:
                    await page.evaluate("""
                        () => {
                            const selectors = ['.layerPop', '.modal', '.overlay', '.dim', '.dimmed', '.popup', '.opener'];
                            selectors.forEach(sel => {
                                document.querySelectorAll(sel).forEach(el => {
                                    el.style.display = 'none';
                                    el.style.visibility = 'hidden';
                                    el.style.pointerEvents = 'none';
                                });
                            });
                            // body ìŠ¤í¬ë¡¤ ì œí•œ í•´ì œ
                            document.body.style.overflow = 'auto';
                        }
                    """)
                except Exception as e:
                    logging.debug(f"ì˜¤ë²„ë ˆì´ ìˆ¨ê¹€ ì‹¤íŒ¨(ë¬´ì‹œ): {str(e)}")

            async def _wait_for_visible_popup_html(timeout_ms=5000, preferred_selectors=None):
                """í´ë¦­ í›„ ì§€ì • ì‹œê°„ ë™ì•ˆ ë°˜ë³µì ìœ¼ë¡œ ê°€ì‹œ íŒì—…ì„ íƒì§€í•˜ì—¬ HTMLì„ ë°˜í™˜"""
                base_candidates = [
                    '.layerPop', '.modal', '.popup', '[role="dialog"]',
                    '#esim-phone-model', '#phone-check-information', '#dual-sim-phone', '#dual-sim-word', '#dualNumber-setting'
                ]
                candidates = list(preferred_selectors or []) + base_candidates
                attempts = max(1, int(timeout_ms / 250))
                for _ in range(attempts):
                    for sel in candidates:
                        try:
                            el = await page.query_selector(sel)
                            if el:
                                visible = await el.evaluate("""
                                    (node) => {
                                        const cs = window.getComputedStyle(node);
                                        const rect = node.getBoundingClientRect();
                                        return cs && cs.display !== 'none' && cs.visibility !== 'hidden' && rect.width > 0 && rect.height > 0;
                                    }
                                """)
                                if visible:
                                    try:
                                        html = await el.inner_html()
                                        return html
                                    except Exception:
                                        pass
                        except Exception:
                            continue
                    await page.wait_for_timeout(250)
                return ""

            async def _insert_after_trigger(trigger_handle, html_content):
                try:
                    await trigger_handle.evaluate(
                        """
                        (el, html) => {
                            const container = document.createElement('div');
                            container.className = 'ai-popup-extracted';
                            container.innerHTML = html || '';
                            if (el && el.parentNode) {
                                if (el.nextSibling) {
                                    el.parentNode.insertBefore(container, el.nextSibling);
                                } else {
                                    el.parentNode.appendChild(container);
                                }
                            }
                        }
                        """,
                        html_content
                    )
                except Exception as e:
                    logging.warning(f"íŠ¸ë¦¬ê±° ë’¤ ì‚½ì… ì‹¤íŒ¨: {str(e)}")

            # 1) layerOpen('#id') íŠ¸ë¦¬ê±° ì²˜ë¦¬
            for idx, a in enumerate(hash_triggers, 1):
                try:
                    onclick_text = await a.get_attribute('onclick')
                    target_id = None
                    if onclick_text:
                        m = re.search(r"layerOpen\(\s*['\"](#[^'\"]+)['\"]", onclick_text)
                        if m:
                            target_id = m.group(1)
                    logging.info(f"[layerOpen] íŠ¸ë¦¬ê±° {idx}/{len(hash_triggers)} ì²˜ë¦¬: target={target_id}")
                    try:
                        await a.click()
                    except Exception:
                        # click ë§‰í ê²½ìš° JSë¡œ ì§ì ‘ í˜¸ì¶œ ì‹œë„
                        await page.evaluate("el => el.click()", a)
                    await page.wait_for_timeout(900)

                    popup_html = ""
                    if target_id:
                        try:
                            target_el = await page.query_selector(target_id)
                            if target_el and await target_el.is_visible():
                                popup_html = await target_el.inner_html()
                            elif target_el:
                                # ë³´ì´ì§€ ì•Šì•„ë„ ê°•ì œë¡œ í‘œì‹œ í›„ ì¶”ì¶œ
                                await page.evaluate("sel => { const el = document.querySelector(sel); if (el){ el.style.display='block'; el.style.visibility='visible'; el.style.opacity='1'; } }", target_id)
                                await page.wait_for_timeout(200)
                                popup_html = await target_el.inner_html()
                        except Exception as e:
                            logging.warning(f"target ì—˜ë¦¬ë¨¼íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    if not popup_html:
                        # ê°€ì‹œ íŒì—…ì„ ìµœëŒ€ 5ì´ˆê¹Œì§€ íƒì§€
                        popup_html = await _wait_for_visible_popup_html(5000)

                    if popup_html:
                        await _insert_after_trigger(a, popup_html)
                    else:
                        logging.info("íŒì—… ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ")

                    # ì˜¤ë²„ë ˆì´ ë‹«ê¸°/ë¬´ì‹œ ì²˜ë¦¬
                    await _hide_overlays()
                    # ESC ì‹œë„ (ì¼ë¶€ ë ˆì´ì–´)
                    try:
                        await page.keyboard.press('Escape')
                    except Exception:
                        pass
                    await page.wait_for_timeout(200)
                except Exception as e:
                    logging.warning(f"layerOpen íŠ¸ë¦¬ê±° ì²˜ë¦¬ ì‹¤íŒ¨ {idx}: {str(e)}")

            # 2) javascript:void(0) + .plus íŠ¸ë¦¬ê±° ì²˜ë¦¬
            for idx, a in enumerate(plus_triggers, 1):
                try:
                    onclick_text = (await a.get_attribute('onclick')) or ''
                    logging.info(f"[plus] íŠ¸ë¦¬ê±° {idx}/{len(plus_triggers)} ì²˜ë¦¬ onclick='{onclick_text[:60]}'")
                    preferred_selectors = []
                    if 'showDeviceModel' in onclick_text:
                        preferred_selectors = ['#esim-phone-model']
                    elif 'showDeviceInfo' in onclick_text:
                        preferred_selectors = ['#phone-check-information']
                    try:
                        await a.click()
                    except Exception:
                        await page.evaluate("el => el.click()", a)
                    await page.wait_for_timeout(900)

                    # ê°€ì‹œ íŒì—…ì„ ìµœëŒ€ 5ì´ˆê¹Œì§€ íƒì§€ (ìš°ì„  ì„ íƒì ë¨¼ì €)
                    popup_html = await _wait_for_visible_popup_html(5000, preferred_selectors)

                    if popup_html:
                        await _insert_after_trigger(a, popup_html)
                    else:
                        logging.info("plus íŠ¸ë¦¬ê±° íŒì—… ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ")

                    await _hide_overlays()
                    try:
                        await page.keyboard.press('Escape')
                    except Exception:
                        pass
                    await page.wait_for_timeout(200)
                except Exception as e:
                    logging.warning(f"plus íŠ¸ë¦¬ê±° ì²˜ë¦¬ ì‹¤íŒ¨ {idx}: {str(e)}")

            # íŒì—… ì›ë³¸ article ë‚´ìš© ì œê±° (ì‚½ì…ë³¸ë§Œ ìœ ì§€)
            try:
                await page.evaluate("""
                    () => {
                        // article íƒœê·¸ ë‚´ë¶€ë¥¼ ë¹„ìš°ë˜, ai-popup-extracted ë§Œ ë³´ì¡´
                        document.querySelectorAll('article').forEach(article => {
                            const keeps = Array.from(article.querySelectorAll('.ai-popup-extracted'));
                            // ê¸°ì¡´ ë‚´ìš© ì œê±°
                            while (article.firstChild) article.removeChild(article.firstChild);
                            // ë³´ì¡´ ìš”ì†Œ ì¬ì‚½ì…
                            keeps.forEach(node => {
                                // ì›ë˜ ë…¸ë“œë¥¼ ì´ë™ì‹œí‚¤ë©´ ì›ë³¸ ìœ„ì¹˜ì—ì„œ ë¹ ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ ë³µì œë³¸ì„ ì‚¬ìš©
                                const clone = node.cloneNode(true);
                                article.appendChild(clone);
                            });
                        });
                        // ê³µí†µ ë¶ˆí•„ìš” ìš”ì†Œ ì œê±°
                        const removeSelectors = [
                            '#cfmClHeader', '#cfmClFooter', '#cfmClSkip',
                            '.location', '.sns-area', '.opener',
                            '.swiper-controls-wrapper', '.opage-hashtag-arrow', '.swiper-button-next', '.swiper-button-prev',
                            '.icon.kakao', '.icon.facebook', '.icon.twitter', '.icon.youtube',
                            '.btn-twitter', '.btn-facebook', '.btn-kakao', '.btn-youtube'
                        ];
                        removeSelectors.forEach(sel => {
                            document.querySelectorAll(sel).forEach(e => e.remove());
                        });
                        // ìˆ¨ê¹€ ìš”ì†Œ ì œê±°
                        document.querySelectorAll('[style*="display:none"]').forEach(e => e.remove());
                        document.querySelectorAll('.invisible').forEach(e => e.remove());
                    }
                """)
            except Exception as e:
                logging.debug(f"article ì œê±°/ì •ë¦¬ ì‹¤íŒ¨(ë¬´ì‹œ): {str(e)}")

            # ìµœì¢… HTML ìˆ˜ì§‘ (#cfmClContents ìš°ì„ )
            try:
                html_content = await page.eval_on_selector("#cfmClContents", "el => el.outerHTML")
            except Exception:
                html_content = await page.content()

            title = await page.title()
            await browser.close()

        except Exception as e:
            logging.error(f"âŒ KT Shop íŒì—… ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            try:
                await browser.close()
            except Exception:
                pass
            return {
                "url": url,
                "title": "KT Shop íŒì—… ì²˜ë¦¬ ì‹¤íŒ¨",
                "markdown": f"# ì²˜ë¦¬ ì‹¤íŒ¨\n\nì˜¤ë¥˜: {str(e)}",
                "html": f"<h1>ì²˜ë¦¬ ì‹¤íŒ¨</h1><p>{str(e)}</p>",
                "status_code": None,
                "special_processed": True,
                "playwright_processed": True,
                "error": str(e)
            }

    # HTML â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ (ì¶”ê°€ í¬ë§· ì—†ì´ ë³¸ë¬¸ë§Œ)
    try:
        from markdownify import markdownify as md
        markdown_content = md(html_content, heading_style="ATX")
    except Exception as e:
        logging.warning(f"ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
        markdown_content = ""

    logging.info("ğŸ‰ KT Shop íŒì—… ì²˜ë¦¬ ì™„ë£Œ")

    return {
        "url": url,
        "murl": to_mshop_url(url),
        "title": title,
        "markdown": markdown_content,
        "html": html_content,
        "status_code": status_code,
        "special_processed": True,
        "playwright_processed": True
    }

# í•¸ë“¤ëŸ¬ ë“±ë¡ - ì§€ì • URLë“¤ (ê¸°ì¡´ ì „ìš© í•¸ë“¤ëŸ¬ ë¯¸ì‚¬ìš© ëŒ€ìƒ)
register_page_handler(
    r'https?://shop\.kt\.com/direct/directEsim\.do',
    handle_ktshop_popup_extractor
)

register_page_handler(
    r'https?://shop\.kt\.com/direct/directUsim\.do',
    handle_ktshop_popup_extractor
)

register_page_handler(
    r'https?://shop\.kt\.com/direct/quickUsim\.do',
    handle_ktshop_popup_extractor
)

register_page_handler(
    r'https?://shop\.kt\.com/direct/directChangeRate\.do',
    handle_ktshop_popup_extractor
)
register_page_handler(
    r'https?://shop\.kt\.com/direct/directSharing\.do',
    handle_ktshop_popup_extractor
)

# ë“€ì–¼ë²ˆí˜¸ê°€ì…
register_page_handler(
    r'https?://shop\.kt\.com/direct/directDual\.do',
    handle_ktshop_popup_extractor
)

# ì„ ë¶ˆUSIMêµ¬ë§¤ì¶©ì „
register_page_handler(
    r'https?://shop\.kt\.com/unify/mobile\.do\?.*category=usim',
    handle_ktshop_popup_extractor
)

# ìŠ¤ë§ˆíŠ¸ê¸°ê¸°ìš”ê¸ˆì œê°€ì…
register_page_handler(
    r'https?://shop\.kt\.com/direct/directSmart\.do',
    handle_ktshop_popup_extractor
)

# eSIMì´ë™
register_page_handler(
    r'https?://shop\.kt\.com/direct/directEsimMove\.do',
    handle_ktshop_popup_extractor
)
# =========================
# 10-C. ëª¨ë°”ì¼ ì œí’ˆ ë¦¬ìŠ¤íŠ¸ í•¸ë“¤ëŸ¬ (products.do?category=*)
# =========================
async def handle_mobile_products_list(url: str, fclient, menu=None) -> dict:
    """
    ëª¨ë°”ì¼ ì œí’ˆ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ì²˜ë¦¬ í•¸ë“¤ëŸ¬
    - ë¦¬ìŠ¤íŠ¸ì—ì„œ prodnm(ì œí’ˆëª…) ë° ìƒì„¸ ì§„ì… ì •ë³´ ìˆ˜ì§‘
    - ê° ì œí’ˆ ìƒì„¸ì—ì„œ 'ì œí’ˆ íŠ¹ì§•', 'ìœ ì˜ì‚¬í•­' ì¶”ì¶œ
    - ë©”ë‰´ëª…: Shop^ëª¨ë°”ì¼ ê°€ì…^í•¸ë“œí°^{prodnm}
    """
    from markdownify import markdownify as md
    logging.info(f"ëª¨ë°”ì¼ ì œí’ˆ ë¦¬ìŠ¤íŠ¸ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, menu={menu}")
    menus, datas = [], []
    base_menu = (menu or '').strip()
    base_title = base_menu.split('^')[-1].strip() if base_menu else 'ëª¨ë°”ì¼ ì œí’ˆ ë¦¬ìŠ¤íŠ¸'
    base_title = sanitize_filename(base_title)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        response = await page.goto(url, wait_until='domcontentloaded', timeout=60000)
        await page.wait_for_timeout(2500)

        try:
            await page.wait_for_function(
                "document.querySelectorAll('.nwProdList input[name=\"prodAttr\"]').length > 0",
                timeout=20000,
            )
        except Exception:
            logging.warning(
                "âš ï¸ ëª¨ë°”ì¼ ì œí’ˆ ë¦¬ìŠ¤íŠ¸: prodAttr ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ í˜ì´ì§€ ìº¡ì²˜ë§Œ ì§„í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )

        status_code = response.status if response else None
        if status_code:
            if status_code >= 400:
                logging.error(f"âŒ ëª¨ë°”ì¼ ì œí’ˆ ë¦¬ìŠ¤íŠ¸ ({url}): HTTP {status_code} ì˜¤ë¥˜")
            elif status_code >= 300:
                logging.warning(f"âš ï¸ ëª¨ë°”ì¼ ì œí’ˆ ë¦¬ìŠ¤íŠ¸ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
            else:
                logging.info(f"âœ… ëª¨ë°”ì¼ ì œí’ˆ ë¦¬ìŠ¤íŠ¸ ({url}): HTTP {status_code} ì„±ê³µ")
        else:
            logging.debug(f"ğŸ” ëª¨ë°”ì¼ ì œí’ˆ ë¦¬ìŠ¤íŠ¸ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")

        # ë©”ì¸ í˜ì´ì§€ ì½˜í…ì¸  ì¶”ì¶œ (ë©”ë‰´ ê¸°ë³¸ í˜ì´ì§€ ì €ì¥)
        try:
            main_html = await page.evaluate("""
                () => {
                    const selectors = ['.nwListArea.inner', '.nwWrap', '#cfmClContents'];
                    for (const sel of selectors) {
                        const el = document.querySelector(sel);
                        if (el && el.innerHTML && el.innerHTML.trim().length > 0) {
                            return el.innerHTML;
                        }
                    }
                    return document.body ? document.body.innerHTML : '';
                }
            """)
        except Exception:
            logging.warning("âš ï¸ ëª¨ë°”ì¼ ì œí’ˆ ë¦¬ìŠ¤íŠ¸: ë©”ì¸ ì˜ì—­ HTML ì¶”ì¶œ ì‹¤íŒ¨, body ì „ì²´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            main_html = await page.content()

        if main_html:
            main_markdown = md(main_html)
            if base_menu:
                menus.append({'menu': base_menu, 'url': url, 'murl': to_mshop_url(url)})
            datas.append({
                'url': url,
                'title': base_title,
                'markdown': main_markdown,
                'html': main_html,
                'special_processed': True,
                'playwright_processed': True,
                'murl': to_mshop_url(url)
            })

        # 1) ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œí’ˆëª…/ìƒì„¸ ì§„ì… ì •ë³´ ìˆ˜ì§‘
        product_items = await page.evaluate(r"""
            () => {
                const results = [];
                // ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ í•œì •
                const roots = Array.from(document.querySelectorAll('.nwProdList'));
                for (const root of roots){
                    // ìˆ¨ê²¨ì§„ input[name=prodAttr]ì—ì„œ ë©”íƒ€ ìˆ˜ì§‘ (ìƒì„¸ view.do êµ¬ì„±ìš© íŒŒë¼ë¯¸í„° í¬í•¨)
                    root.querySelectorAll('input[name="prodAttr"]').forEach((inp) => {
                        const prodnm = inp.getAttribute('prodnm') || '';
                        const prodno = inp.getAttribute('prodno') || '';
                        const imageurl = inp.getAttribute('imageurl') || '';
                        const sntyno = inp.getAttribute('sntyno') || '';
                        const pplid = inp.getAttribute('pplid') || '';
                        const svcengtmonstypecd = inp.getAttribute('svcengtmonstypecd') || '';
                        const supporttype = inp.getAttribute('supporttype') || '';
                        if (prodnm) {
                            results.push({ prodnm, prodno, imageurl, sntyno, pplid, svcengtmonstypecd, supporttype });
                        }
                    });
                    // ì•µì»¤ ê¸°ë°˜ ìƒì„¸ ê²½ë¡œ ì¶”ì • (ê°€ëŠ¥í•˜ë©´) â€” ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ë¡œ ì œí•œ
                    root.querySelectorAll('a[href]').forEach(a => {
                        const href = a.getAttribute('href')||'';
                        const title = (a.textContent||'').trim();
                        if (/productDetail\.do\?ItemCode=/.test(href) || /mobile\/view\.do\?/.test(href)){
                            try {
                                const abs = new URL(href, location.href).href;
                                results.push({ anchorHref: abs, title });
                            } catch(e) {}
                        }
                    });
                }
                return results;
            }
        """)

        logging.info(f"ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘: {len(product_items)}ê°œ ì›ì‹œ í•­ëª©")

        # 2) ì œí’ˆë³„ ëŒ€í‘œ ì •ë³´ ì •ë¦¬ ë° ìƒì„¸ ì¶”ì¶œ
        normalized = []
        seen_names = set()

        for item in product_items:
            try:
                # ì œí’ˆëª…ì€ prodnm(ìˆ¨ê²¨ì§„ input ê¸°ë°˜)ë§Œ ì¸ì •
                prodnm = (item.get('prodnm') or '').strip()
            except Exception:
                prodnm = ''
            if not prodnm or prodnm in seen_names:
                continue
            seen_names.add(prodnm)
            # ìƒì„¸ URL êµ¬ì„±: ìš°ì„  anchorHref, ì—†ìœ¼ë©´ view.do ì¡°í•© ì‹œë„, ìµœì¢… í´ë°±ì€ ë¦¬ìŠ¤íŠ¸ URL
            detail_url = item.get('anchorHref')
            if not detail_url:
                prodno = (item.get('prodno') or '').strip()
                sntyno = (item.get('sntyno') or '').strip()
                pplid = (item.get('pplid') or '').strip()
                svc_value = item.get('svcengtmonstypecd') or ''
                svc = svc_value.strip()
                support = (item.get('supporttype') or '').strip()
                if prodno:
                    const_params = []
                    if sntyno:
                        const_params.append(f"sntyNo={sntyno}")
                    if pplid:
                        const_params.append(f"pplId={pplid}")
                    if svc:
                        const_params.append(f"svcEngtMonsTypeCd={svc}")
                    if support:
                        const_params.append(f"supportType={support}")
                    qp = ("&".join(const_params))
                    base = f"https://shop.kt.com/mobile/view.do?prodNo={prodno}"
                    detail_url = base + (f"&{qp}" if qp else '')
            normalized.append({ 'name': prodnm, 'url': detail_url or url, 'prodno': item.get('prodno','') })

        logging.info(f"ì •ê·œí™”ëœ ì œí’ˆ: {len(normalized)}ê°œ")

        # 3) ê° ì œí’ˆ ìƒì„¸ì—ì„œ 'ì œí’ˆ íŠ¹ì§•', 'ìœ ì˜ì‚¬í•­' ì¶”ì¶œ
        for idx, prod in enumerate(normalized, 1):
            try:
                logging.info(f"[{idx}/{len(normalized)}] ìƒì„¸ ì¶”ì¶œ: {prod['name']}")
                # ì œí’ˆ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
                if prod.get('url') and prod['url'] != url:
                    try:
                        await page.goto(prod['url'], wait_until='domcontentloaded', timeout=60000)
                        await page.wait_for_timeout(1200)
                    except Exception as _e:
                        logging.warning(f"ìƒì„¸ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨(ë¬´ì‹œí•˜ê³  ê³„ì†): {prod['url']} -> {_e}")

                # ìƒì„¸ ì»¨í…Œì´ë„ˆ í•œì • ì¶”ì¶œ + íƒ­ í´ë¦­ ì‹œë„(ìœ ì˜ì‚¬í•­/ìƒí’ˆì •ë³´)
                await page.evaluate(r"""
                    () => {
                        // ìƒì„¸ ì˜ì—­ ê·¼ì²˜ë§Œ ëŒ€ìƒìœ¼ë¡œ íƒ­ í´ë¦­ì„ ì‹œë„í•œë‹¤
                        const roots = ['#cfmClContents', '.nwViewProdDetail', '.prodDetailWrap', '.prodDetail'];
                        const within = [];
                        for (const sel of roots){
                            const el = document.querySelector(sel);
                            if (el) within.push(el);
                        }
                        const clickByText = (root, text) => {
                            const cands = root.querySelectorAll('a,button,[role="tab"],li');
                            for (const el of cands){
                                const t = (el.innerText||'').replace(/\s+/g,' ').trim();
                                if (!t) continue;
                                if (t === text || t.includes(text)){
                                    if (el.classList && el.classList.contains('nwWindowPop')) continue; // ì™¸ë¶€ íŒì—… ë°°ì œ
                                    try { el.click(); return true; } catch(e) {}
                                }
                            }
                            return false;
                        };
                        const clickBySelector = (sel) => {
                            const el = document.querySelector(sel);
                            if (!el) return false;
                            try { el.click(); return true; } catch(e) { return false; }
                        };
                        for (const root of within){
                            // 1) ëª…ì‹œì  ID/ì†ì„± ìš°ì„ : prodDetailTab, [nw-target="#view-1"]
                            if (clickBySelector('#prodDetailTab')) continue;
                            if (clickBySelector('[nw-target="#view-1"]')) continue;
                            // 2) í…ìŠ¤íŠ¸ í´ë°±
                            clickByText(root, 'ìƒí’ˆì •ë³´');
                        }
                    }
                """)
                await page.wait_for_timeout(300)
                # display:none íŒ¨ë„ ê°•ì œ í‘œì‹œ í›„, ìƒì„¸ ë£¨íŠ¸ ì „ì²´ ì¶”ì¶œ ìš°ì„ 
                await page.evaluate("""
                    () => {
                        const root = document.querySelector('.nwViewProdDetail')
                          || document.querySelector('#cfmClContents')
                          || document.querySelector('.prodDetailWrap')
                          || document.querySelector('.prodDetail');
                        if (!root) return;
                        const show = (el) => {
                            if (!el) return;
                            try {
                                el.style.display = 'block';
                                el.style.visibility = 'visible';
                                el.style.opacity = '1';
                                el.style.height = 'auto';
                                el.style.maxHeight = 'none';
                            } catch (e) {}
                        };
                        ['#view-1', '#view-4'].forEach(sel => show(root.querySelector(sel)));
                        root.querySelectorAll('[nw-tab]').forEach(show);
                    }
                """)
                await page.wait_for_timeout(120)

                detail_html = await page.evaluate("""
                    () => {
                        // ê°€ëŠ¥í•˜ë©´ ìƒì„¸ ë£¨íŠ¸(.nwViewProdDetail) ì „ì²´ë¥¼ ìš°ì„ 
                        const containers = ['.nwViewProdDetail', '#cfmClContents', '.prodDetailWrap', '.prodDetail', '#view-1'];
                        let targetEl = null;
                        for (const sel of containers){
                            const el = document.querySelector(sel);
                            if (el && el.innerHTML && el.innerHTML.trim().length>0) {
                                targetEl = el;
                                break;
                            }
                        }
                        if (!targetEl) {
                            targetEl = document.body;
                        }
                        
                        // ìœ ì˜ì‚¬í•­, êµ¬ë§¤í›„ê¸°, ì „ë¬¸ìƒë‹´ ì˜ì—­ ì œê±° (ì •í™•í•œ ì…€ë ‰í„° ì‚¬ìš©)
                        if (targetEl) {
                            const clone = targetEl.cloneNode(true);
                            
                            // ì •í™•í•œ ì…€ë ‰í„°ë¡œ ì œê±°
                            const removeSelectors = [
                                '#noteArea',            // ìœ ì˜ì‚¬í•­ ì»¨í…ì¸  ì˜ì—­
                                '#noteTab',             // ìœ ì˜ì‚¬í•­ ì»¨í…ì¸  ì˜ì—­ (ë‹¤ë¥¸ êµ¬ì¡°)
                                '#view-4',              // ìœ ì˜ì‚¬í•­ íƒ­ íŒ¨ë„
                                'button[nw-target="#noteTab"]',     // ìœ ì˜ì‚¬í•­ íƒ­ ë²„íŠ¼
                                'button[nw-target="#view-4"]',      // ìœ ì˜ì‚¬í•­ íƒ­ ë²„íŠ¼ (ë‹¤ë¥¸ êµ¬ì¡°)
                                '[nw-tab="#view-4"]',   // ìœ ì˜ì‚¬í•­ íƒ­ íŒ¨ë„
                                '#reviewTab',           // êµ¬ë§¤í›„ê¸° ì»¨í…ì¸  ì˜ì—­
                                '#prodReviewTab',       // êµ¬ë§¤í›„ê¸° íƒ­ ë²„íŠ¼
                                '#counselTab',          // ì „ë¬¸ìƒë‹´ ì»¨í…ì¸  ì˜ì—­
                                'button[nw-target="#counselTab"]'  // ì „ë¬¸ìƒë‹´ íƒ­ ë²„íŠ¼
                            ];
                            
                            removeSelectors.forEach(selector => {
                                try {
                                    const el = clone.querySelector(selector);
                                    if (el) {
                                        el.remove();
                                    }
                                } catch(e) {
                                    // ì…€ë ‰í„° ì˜¤ë¥˜ ë¬´ì‹œ
                                }
                            });
                            
                            // "ìœ ì˜ì‚¬í•­" í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ íƒ­ ë²„íŠ¼ê³¼ ê·¸ ì—°ê²°ëœ ì»¨í…ì¸  ì œê±°
                            try {
                                const allButtons = clone.querySelectorAll('button, a, [role="tab"]');
                                allButtons.forEach(btn => {
                                    const text = (btn.textContent || '').trim();
                                    if (text === 'ìœ ì˜ì‚¬í•­' || text.includes('ìœ ì˜ì‚¬í•­')) {
                                        // ë²„íŠ¼ì´ ê°€ë¦¬í‚¤ëŠ” íƒ€ê²Ÿë„ ì œê±°
                                        const target = btn.getAttribute('nw-target');
                                        if (target) {
                                            const targetEl = clone.querySelector(target);
                                            if (targetEl) targetEl.remove();
                                        }
                                        // ë²„íŠ¼ ìì²´ë„ ì œê±°
                                        btn.remove();
                                    }
                                });
                            } catch(e) {
                                // ì˜¤ë¥˜ ë¬´ì‹œ
                            }
                            
                            return clone.innerHTML || '';
                        }
                        return '';
                    }
                """)
                
                # "ë‹¤ìŒë‚´ìš©ì°¸ì¡°" altë¥¼ ê°€ì§„ ì´ë¯¸ì§€ë¥¼ GPT-4Vë¡œ ì²˜ë¦¬
                try:
                    import requests
                    from io import BytesIO
                    import base64
                    from openai import OpenAI
                    import os
                    
                    soup = BeautifulSoup(detail_html, 'html.parser')
                    openai_client = None
                    
                    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒë§Œ)
                    if 'OPENAI_API_KEY' in os.environ:
                        openai_client = OpenAI()
                        logging.info("ğŸ¤– GPT-4V OCR ì¤€ë¹„ ì™„ë£Œ")
                    
                    # "ë‹¤ìŒë‚´ìš©ì°¸ì¡°" altë¥¼ ê°€ì§„ ì´ë¯¸ì§€ ì°¾ê¸°
                    images = soup.find_all('img', alt='ë‹¤ìŒë‚´ìš©ì°¸ì¡°')
                    if images:
                        logging.info(f"ğŸ” 'ë‹¤ìŒë‚´ìš©ì°¸ì¡°' ì´ë¯¸ì§€ {len(images)}ê°œ ë°œê²¬, GPT-4V OCR ì²˜ë¦¬ ì‹œì‘...")
                        
                        for img in images:
                            try:
                                img_url = img.get('src', '')
                                if not img_url:
                                    continue
                                
                                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                                if img_url.startswith('//'):
                                    img_url = 'https:' + img_url
                                elif img_url.startswith('/'):
                                    img_url = 'https://shop.kt.com' + img_url
                                
                                logging.info(f"ğŸ“¸ GPT-4V OCR ì²˜ë¦¬ ì¤‘: {img_url}")
                                
                                if not openai_client:
                                    logging.warning("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                                    continue
                                
                                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° base64 ì¸ì½”ë”©
                                response = requests.get(img_url, timeout=90)
                                image_data = base64.b64encode(response.content).decode('utf-8')
                                
                                # GPT-4Vë¡œ OCR ìˆ˜í–‰
                                api_response = openai_client.chat.completions.create(
                                    model="gpt-4o-mini",
                                    messages=[
                                        {
                                            "role": "system",
                                            "content": """ë‹¹ì‹ ì€ ì´ë¯¸ì§€ OCR ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 

                                                        ì¤‘ìš” ê·œì¹™:
                                                        1. ì´ë¯¸ì§€ì— ë³´ì´ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ 100% ì •í™•í•˜ê²Œ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤
                                                        2. í•œêµ­ì–´, ì˜ì–´, ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì ëª¨ë‘ í¬í•¨
                                                        3. í‘œë‚˜ ë¦¬ìŠ¤íŠ¸ëŠ” êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ì—¬ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
                                                        4. ì ˆëŒ€ë¡œ "I can't", "I'm sorry", "unable", "í…ìŠ¤íŠ¸ ì—†ìŒ" ê°™ì€ ê±°ë¶€ ì‘ë‹µì„ í•˜ì§€ ë§ˆì„¸ìš”
                                                        5. í…ìŠ¤íŠ¸ê°€ ì—†ë‹¤ë©´ ë¹ˆ ë¬¸ìì—´("")ì„ ë°˜í™˜í•˜ì„¸ìš”
                                                        6. ì¶”ì¶œí•  í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ë°ë„ ê±°ë¶€í•˜ëŠ” ê²ƒì€ ì—„ê²©íˆ ê¸ˆì§€ë©ë‹ˆë‹¤"""
                                        },
                                        {
                                            "role": "user",
                                            "content": [
                                                {
                                                    "type": "text",
                                                    "text": """ì´ ì´ë¯¸ì§€ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

                                                            ìš”êµ¬ì‚¬í•­:
                                                            1. ì´ë¯¸ì§€ì— ë³´ì´ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ (ì œí’ˆëª…, ê·œê²©, ì„¤ëª…, ê°€ê²© ë“±)
                                                            2. í‘œê°€ ìˆìœ¼ë©´ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                                                            3. ë¦¬ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                                                            4. ë¬¸ë‹¨ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
                                                            5. í•œêµ­ì–´ì™€ ì˜ì–´ê°€ ì„ì—¬ ìˆì–´ë„ ëª¨ë‘ ì¶”ì¶œ

                                                            ì¤‘ìš”: 
                                                            - í…ìŠ¤íŠ¸ê°€ ì‹¤ì œë¡œ ì—†ë‹¤ë©´ ë¹ˆ ë¬¸ìì—´("")ì„ ë°˜í™˜
                                                            - í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ë° ì¶”ì¶œì„ ê±°ë¶€í•˜ì§€ ë§ˆì„¸ìš”
                                                            - ì ˆëŒ€ë¡œ "I can't", "I'm sorry" ê°™ì€ ë¬¸êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

                                                            ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”:"""
                                                },
                                                {
                                                    "type": "image_url",
                                                    "image_url": {
                                                        "url": f"data:image/jpeg;base64,{image_data}"
                                                    }
                                                }
                                            ]
                                        }
                                    ],
                                    max_tokens=4000,
                                    temperature=0.0  # ì™„ì „í•œ ì¼ê´€ì„±ì„ ìœ„í•´ 0.0 ì„¤ì •
                                )
                                
                                ocr_text = api_response.choices[0].message.content.strip()
                                
                                # ì˜ëª»ëœ ì‘ë‹µ í•„í„°ë§ - ë” ê°•ë ¥í•˜ê²Œ
                                invalid_responses = [
                                    "i'm sorry", "i can't", "cannot", "unable", "can't help",
                                    "i don't", "i cannot", "not able", "no text", "í…ìŠ¤íŠ¸ ì—†ìŒ",
                                    "ë¹ˆ ì´ë¯¸ì§€", "no content", "empty"
                                ]
                                is_invalid = any(phrase.lower() in ocr_text.lower() for phrase in invalid_responses)
                                
                                # ì‘ë‹µì´ ë„ˆë¬´ ì§§ê±°ë‚˜ íŠ¹ì • íŒ¨í„´ì„ í¬í•¨í•˜ë©´ ë¬´íš¨
                                if len(ocr_text) < 10 and ocr_text.lower() not in ["", "na"]:
                                    is_invalid = True
                                
                                if ocr_text and not is_invalid:
                                    logging.info(f"âœ… OCR ê²°ê³¼: {len(ocr_text)}ì ì¶”ì¶œë¨")
                                    # ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´
                                    new_tag = soup.new_tag('div')
                                    new_tag.string = f'\n{ocr_text}\n'
                                    img.replace_with(new_tag)
                                else:
                                    logging.warning(f"âš ï¸ OCR ê²°ê³¼ ì—†ìŒ ë˜ëŠ” ì˜ëª»ëœ ì‘ë‹µ: {img_url}")
                                    # í…ìŠ¤íŠ¸ë¥¼ íŠ¹ë³„ í‘œì‹œ
                                    new_tag = soup.new_tag('div')
                                    new_tag.string = '\n[ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨]\n'
                                    img.replace_with(new_tag)
                                    
                            except Exception as ocr_error:
                                logging.warning(f"âš ï¸ GPT-4V OCR ì²˜ë¦¬ ì‹¤íŒ¨: {img_url} - {str(ocr_error)}")
                                continue
                        
                        # ìˆ˜ì •ëœ HTMLë¡œ ì—…ë°ì´íŠ¸
                        detail_html = str(soup)
                        logging.info("âœ… GPT-4V OCR ì²˜ë¦¬ ì™„ë£Œ")
                    
                except ImportError as ie:
                    logging.warning(f"âš ï¸ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ: {str(ie)}")
                except Exception as e:
                    logging.warning(f"âš ï¸ GPT-4V OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

                md_all = md(detail_html)
                # ê°„ê²°í™”: ìƒì„¸ ë£¨íŠ¸ ì „ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                content = md_all

                # ì…ë ¥ menuë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë©”ë‰´ëª… êµ¬ì„± (í•˜ë“œì½”ë”© ì œê±°)
                base_menu = (menu or '').strip()
                menu_name = f"{base_menu}^{prod['name']}" if base_menu else f"Shop^{prod['name']}"
                menus.append({ 'menu': menu_name, 'url': prod['url'], 'murl': to_mshop_url(prod['url']) })
                datas.append({
                    'url': prod['url'],
                    'title': prod['name'],
                    'markdown': content,
                    'html': detail_html,
                    'special_processed': True,
                    'playwright_processed': True,
                    'murl': to_mshop_url(prod['url'])
                })
            except Exception as e:
                logging.warning(f"ìƒì„¸ ì¶”ì¶œ ì‹¤íŒ¨: {prod.get('name','unknown')}: {str(e)}")
                continue

        await browser.close()

    return {
        'menus': menus,
        'datas': datas,
        'total_processed': len(datas),
        'status': 'completed',
        'message': f"ì´ {len(datas)}ê°œ ëª¨ë°”ì¼ ì œí’ˆ ì²˜ë¦¬ ì™„ë£Œ"
    }

register_page_handler(
    r'https?://shop\.kt\.com/mobile/products\.do\?category=.*',
    handle_mobile_products_list
)
# =========================
# 10-D. êµ¿ë°”ì´ phoneView.do ì „ìš© í•¸ë“¤ëŸ¬ (display:none ëª¨ë‘ í‘œì‹œ í›„ ì „ì²´ ì¶”ì¶œ)
# =========================
async def handle_goodbye_phoneview(url: str, fclient, menu=None) -> dict:
    logging.info(f"êµ¿ë°”ì´ phoneView í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, menu={menu}")
    menus, datas = [], []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        response = await page.goto(url, wait_until='domcontentloaded', timeout=60000)
        await page.wait_for_timeout(600)

        status_code = response.status if response else None
        if status_code and status_code >= 400:
            logging.error(f"âŒ phoneView ({url}): HTTP {status_code} ì˜¤ë¥˜")
        else:
            logging.info(f"âœ… phoneView ({url}): HTTP {status_code or 'unknown'}")

        # 1) display:none/hidden ìš”ì†Œ ê°•ì œ í‘œì‹œ
        await page.evaluate("""
            () => {
                const show = (el) => {
                    if (!el) return;
                    try {
                        el.style.display = 'block';
                        el.style.visibility = 'visible';
                        el.style.opacity = '1';
                        el.style.height = 'auto';
                        el.style.maxHeight = 'none';
                    } catch(e) {}
                };
                // ì „ì—­ì ìœ¼ë¡œ ìˆ¨ê¹€ ì œê±°
                document.querySelectorAll('[hidden], .hidden, .is-hidden').forEach(n => {
                    n.removeAttribute('hidden');
                    show(n);
                });
                document.querySelectorAll('*').forEach(n => {
                    const st = (n.getAttribute('style')||'').toLowerCase();
                    if (st.includes('display:none')) show(n);
                    if (st.includes('visibility:hidden')) show(n);
                });
                // ì£¼ìš” íƒ­/íŒ¨ë„ í›„ë³´ë“¤
                ['.nwViewProdDetail', '#cfmClContents', '.prodDetailWrap', '.prodDetail', '#view-1', '#view-4']
                  .forEach(sel => show(document.querySelector(sel)));
            }
        """)
        await page.wait_for_timeout(200)

        # 2) ì»¨í…Œì´ë„ˆ ìš°ì„  ìˆœìœ„ë¡œ ì „ì²´ HTML íšë“
        detail_html = await page.evaluate("""
            () => {
                const containers = ['.nwViewProdDetail', '#cfmClContents', '.prodDetailWrap', '.prodDetail', '#content', 'main'];
                for (const sel of containers){
                    const el = document.querySelector(sel);
                    if (el && el.innerHTML && el.innerHTML.trim().length>0) return el.innerHTML;
                }
                return document.body ? document.body.innerHTML : '';
            }
        """)

        from markdownify import markdownify as md
        md_all = md(detail_html)

        # ë©”ë‰´/íƒ€ì´í‹€: ì…ë ¥ menuë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³ , íƒ€ì´í‹€ì€ h1/ë¬¸ì„œì œëª©ì—ì„œ ì¶”ì¶œ
        base_menu_in = (menu or '').strip()
        try:
            title_text = (await page.evaluate("""
                () => {
                    const pick = (sel) => {
                        const el = document.querySelector(sel);
                        return el ? (el.innerText||'').trim() : '';
                    };
                    return pick('h1') || pick('.title') || pick('.tit') || document.title || '';
                }
            """)) or 'êµ¿ë°”ì´ ì¤‘ê³ í° ë³´ìƒ'
        except Exception:
            title_text = 'êµ¿ë°”ì´ ì¤‘ê³ í° ë³´ìƒ'

        if base_menu_in:
            mobile_url = url if '/m/' in url else to_mshop_url(url)
            menus.append({ 'menu': base_menu_in, 'url': url, 'murl': mobile_url })
        datas.append({
            'url': url,
            'title': title_text,
            'markdown': md_all,
            'html': detail_html,
            'special_processed': True,
            'playwright_processed': True,
            'murl': url if '/m/' in url else to_mshop_url(url)
        })

        await browser.close()

    return {
        'menus': menus,
        'datas': datas,
        'total_processed': len(datas),
        'status': 'completed',
        'message': f"phoneView ì²˜ë¦¬ ì™„ë£Œ ({len(datas)}ê°œ)"
    }

register_page_handler(
    r'https?://shop\.kt\.com/goodbye/phoneView\.do.*',
    handle_goodbye_phoneview
)


# =========================
# 10-E. ê¸°íšì „ ëª©ë¡/ìƒì„¸ í•¸ë“¤ëŸ¬ (olhsStore.do â†’ olhsPlan.do)
# - iframe ë‚´ë¶€ ëª©ë¡ + í˜ì´ì§€ë„¤ì´ì…˜ ìˆœíšŒ
# - ì œëª©(plan_tit) â†’ ë©”ë‰´ëª… suffix, ì „ì‹œê¸°ê°„ â†’ startdate
# =========================
async def handle_store_plans_list(url: str, fclient, menu=None) -> dict:
    import re
    from markdownify import markdownify as md
    logging.info(f"ê¸°íšì „ ëª©ë¡ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, menu={menu}")
    menus, datas = [], []

    def _norm_date(dtxt: str) -> str:
        # ì˜ˆ: 2025.9.10 ~ â†’ 2025-09-10
        try:
            m = re.search(r'(20\d{2})[\.-]\s*(\d{1,2})[\.-]\s*(\d{1,2})', dtxt)
            if not m:
                return ''
            y, mo, dy = int(m.group(1)), int(m.group(2)), int(m.group(3))
            return f"{y:04d}-{mo:02d}-{dy:02d}"
        except Exception:
            return ''

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        response = await page.goto(url, wait_until='domcontentloaded', timeout=90000)
        await page.wait_for_timeout(800)

        status_code = response.status if response else None
        if status_code and status_code >= 400:
            logging.error(f"âŒ ê¸°íšì „ ëª©ë¡ ({url}): HTTP {status_code} ì˜¤ë¥˜")
        else:
            logging.info(f"âœ… ê¸°íšì „ ëª©ë¡ ({url}): HTTP {status_code or 'unknown'}")

        # iframe íƒìƒ‰: ì²« ê°€ì‹œ í”„ë ˆì„ ë˜ëŠ” .plan_titë¥¼ í¬í•¨í•˜ëŠ” í”„ë ˆì„
        await page.wait_for_selector('iframe', timeout=10000)
        target_frame = None
        for fr in page.frames:
            if fr == page.main_frame:
                continue
            try:
                if await fr.query_selector('.plan_tit'):
                    target_frame = fr
                    break
            except Exception:
                continue
        if not target_frame:
            # ê°€ì‹œ í”„ë ˆì„ ì¤‘ ì²« ë²ˆì§¸
            for fr in page.frames:
                if fr != page.main_frame:
                    target_frame = fr
                    break

        if not target_frame:
            await browser.close()
            return { 'menus': [], 'datas': [], 'total_processed': 0, 'status': 'completed', 'message': 'í”„ë ˆì„ ë¯¸íƒì§€' }

        # ì´ í˜ì´ì§€ ìˆ˜ ì¶”ì •
        try:
            total_pages = await target_frame.evaluate("""
                () => {
                    const pg = document.querySelector('.pageWrap.ui-paging');
                    if (!pg) return 1;
                    let max = 1;
                    pg.querySelectorAll('[pageno]').forEach(a => {
                        const n = parseInt(a.getAttribute('pageno')||'1');
                        if (!isNaN(n) && n>max) max = n;
                    });
                    return max || 1;
                }
            """)
        except Exception:
            total_pages = 1

        logging.info(f"ê¸°íšì „ ëª©ë¡ ì´ í˜ì´ì§€: {total_pages}")

        collected = []

        async def extract_page_items() -> list:
            try:
                return await target_frame.evaluate(r"""
                    () => {
                        const items = [];
                        document.querySelectorAll('.plan_tit').forEach(t => {
                            const title = (t.innerText||'').replace(/\s+/g,' ').trim();
                            let href = '';
                            // íƒ€ì´í‹€ ì£¼ë³€ a[href] - ì¡°ë¶€ëª¨ê¹Œì§€ í™•ì¸
                            let a = t.closest('a');
                            if (!a || !a.getAttribute('href')){
                                // ë¶€ëª¨ì—ì„œ ì°¾ê¸°
                                const parent = t.parentElement;
                                if (parent) {
                                    a = parent.querySelector('a[href]');
                                }
                                // ì¡°ë¶€ëª¨ì—ì„œ ì°¾ê¸°
                                if ((!a || !a.getAttribute('href')) && parent) {
                                    const grandParent = parent.parentElement;
                                    if (grandParent) {
                                        a = grandParent.querySelector('a[href]');
                                    }
                                }
                            }
                            if (a && a.getAttribute('href')){
                                href = a.href || a.getAttribute('href') || '';
                            }
                            // ì „ì‹œê¸°ê°„ í…ìŠ¤íŠ¸ ì¶”ì •
                            let period = '';
                            const root = t.closest('li') || t.closest('div') || document;
                            const blindSpans = root.querySelectorAll('span.blind');
                            for (const sp of blindSpans){
                                if ((sp.innerText||'').includes('ì „ì‹œê¸°ê°„')){
                                    const par = sp.parentElement;
                                    if (par){ period = par.innerText.replace(/\s+/g,' ').trim(); break; }
                                }
                            }
                            if (title){ items.push({ title, href, period }); }
                        });
                        return items;
                    }
                """)
            except Exception:
                return []

        for pno in range(1, (total_pages or 1)+1):
            try:
                if pno > 1:
                    try:
                        await target_frame.click(f'a[pageno="{pno}"]', timeout=8000)
                        await page.wait_for_timeout(600)
                    except Exception:
                        # ëŒ€ì²´ í´ë¦­: evaluateë¡œ í´ë¦­
                        try:
                            await target_frame.evaluate("""
                                (n) => {
                                    const el = document.querySelector(`a[pageno="${n}"]`);
                                    if (el) el.click();
                                }
                            """, pno)
                            await page.wait_for_timeout(600)
                        except Exception:
                            pass
                rows = await extract_page_items()
                logging.info(f"í˜ì´ì§€ {pno}: {len(rows)}ê±´ ìˆ˜ì§‘")
                for r in rows:
                    if any(x.get('href') == r.get('href') and x.get('title') == r.get('title') for x in collected):
                        continue
                    collected.append(r)
            except Exception as e:
                logging.warning(f"í˜ì´ì§€ {pno} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

        logging.info(f"ì´ ìˆ˜ì§‘ í•­ëª©: {len(collected)}")

        # ìƒì„¸ í˜ì´ì§€ ìˆœíšŒ
        for idx, row in enumerate(collected, 1):
            title = (row.get('title') or '').strip()
            href = row.get('href') or ''
            period = row.get('period') or ''
            startdate = _norm_date(period)
            # ìƒì„¸ URL ì ˆëŒ€ê²½ë¡œ ë³´ì •
            from urllib.parse import urljoin
            detail_url = ''
            if href:
                if not href.lower().startswith('javascript'):
                    detail_url = urljoin('https://shop.kt.com', href)
            logging.info(f"[{idx}/{len(collected)}] ìƒì„¸ ì¶”ì¶œ: {title}")
            logging.info(f"[DEBUG] ìƒì„¸ URL: {detail_url}")
            detail_html = ''
            try:
                if detail_url:
                    # Crawl4AI ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ ì§ì ‘ í˜¸ì¶œ (route_url ìš°íšŒ)
                    try:
                        if hasattr(fclient, 'crawler') and fclient.crawler:
                            from crawl4ai.async_configs import CrawlerRunConfig, CacheMode
                            
                            # ê¸°ë³¸ ìŠ¤í¬ë˜í•‘ ì„¤ì •
                            run_config = CrawlerRunConfig(
                                verbose=False,
                                word_count_threshold=10,
                                exclude_external_links=True,
                                remove_overlay_elements=False,
                                process_iframes=True,
                                ignore_body_visibility=True,
                                js_only=False,
                                cache_mode=CacheMode.BYPASS,
                                excluded_tags=['form', 'header', 'footer', 'nav'],
                                excluded_selector="#cfmClHeader, #cfmClFooter, #cfmClSkip, .location, .sns-area",
                                wait_until="networkidle",
                                delay_before_return_html=6,
                                simulate_user=True,
                                override_navigator=True,
                                page_timeout=120000,
                            )
                            
                            crawl_result = await fclient.crawler.arun(url=detail_url, config=run_config)
                            if crawl_result and crawl_result.success:
                                html_content = crawl_result.html or ''
                                md_content = crawl_result.markdown or ''
                                logging.info(f"[DEBUG] í¬ë¡¤ë§ ì„±ê³µ: HTML={len(html_content)}ì, MD={len(md_content)}ì")
                                c4_result = {
                                    'html': html_content,
                                    'markdown': md_content,
                                    'status_code': getattr(crawl_result, 'status_code', None)
                                }
                                # html/markdown ìš°ì„  ì‚¬ìš©
                                detail_html = html_content.strip()
                                pre_md_text = md_content.strip()
                                # ì•„ë˜ md_text ìƒì„± ì „ì— ìš°ì„ ê°’ìœ¼ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•´ localsì— ì €ì¥
                                if pre_md_text:
                                    md_text = pre_md_text
                            else:
                                logging.warning(f"Crawl4AI í¬ë¡¤ë§ ì‹¤íŒ¨: {detail_url} - success={getattr(crawl_result, 'success', None)}")
                        else:
                            logging.warning("fclient.crawler ì‚¬ìš© ë¶ˆê°€")
                    except Exception as ce:
                        logging.warning(f"Crawl4AI ì§ì ‘ í˜¸ì¶œ ì˜¤ë¥˜(ë¬´ì‹œ): {str(ce)}")
            except Exception as e:
                logging.warning(f"ìƒì„¸ ì´ë™ ì‹¤íŒ¨(ë¬´ì‹œ): {str(e)}")

            # ìœ„ì—ì„œ c4 ê²°ê³¼ë¡œ md_textê°€ ì´ë¯¸ ì±„ì›Œì¡Œë‹¤ë©´ ìœ ì§€, ì•„ë‹ˆë©´ ë³€í™˜
            try:
                md_text
            except NameError:
                md_text = ''
            if not md_text:
                md_text = md(detail_html, heading_style="ATX") if detail_html else ''

            base_menu = (menu or '').strip()
            menu_name = f"{base_menu}^{title}" if base_menu else f"Shop^í•«ë”œ/ê¸°íšì „^ê¸°íšì „^í†µì‹ ìƒí’ˆ^{title}"
            menus.append({ 'menu': menu_name, 'url': detail_url or url, 'murl': to_mshop_url(detail_url or url) })
            datas.append({
                'url': detail_url or url,
                'title': title,
                'markdown': md_text or '',
                'html': detail_html or '',
                'special_processed': True,
                'playwright_processed': True,
                'startdate': startdate or '',
                'murl': to_mshop_url(detail_url or url)
            })

        await browser.close()

    return {
        'menus': menus,
        'datas': datas,
        'total_processed': len(datas),
        'status': 'completed',
        'message': f"ì´ {len(datas)}ê°œ ê¸°íšì „ ì²˜ë¦¬ ì™„ë£Œ"
    }

register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR05&subDispNo=STOR0501.*',
    handle_store_plans_list
)
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR05&subDispNo=STOR0503.*',
    handle_store_plans_list
)

# =========================
# ì§€ë‹ˆ TV ì±„ë„ í¸ì„±í‘œ í•¸ë“¤ëŸ¬
# =========================
async def handle_whygenietv_channel_schedule(url: str, fclient, menu: Optional[str] = None) -> dict:
    """
    ì§€ë‹ˆ TV(WhyGenieTV) ì±„ë„ í¸ì„±í‘œ ì¶”ì¶œ í•¸ë“¤ëŸ¬

    ìš”êµ¬ì‚¬í•­:
    - ul.channel_select.tv_live ì´í•˜ íƒ­ ì •ë³´ë¥¼ ì´ìš©í•´ ê° ìƒí’ˆ í”Œëœì„ ìˆœíšŒ
    - ê° ì±„ë„ í•­ëª©ì˜ ë²ˆí˜¸, ëª…ì¹­, alt(ë¹„ê³ ) ì •ë³´ë¥¼ ìˆ˜ì§‘
    - ë§ˆí¬ë‹¤ìš´ í‘œ í˜•íƒœ(| ì±„ë„ ë²ˆí˜¸ | ì±„ë„ëª… | ë¹„ê³  |)ë¡œ ì €ì¥
    - ë©”ë‰´ ê²½ë¡œëŠ” {menu ë˜ëŠ” ê¸°ë³¸ê°’}^{í”Œëœëª…} í˜•íƒœë¡œ êµ¬ì„±
    """
    import requests

    logging.info(f"ğŸ¬ ì§€ë‹ˆ TV ì±„ë„ í¸ì„±í‘œ ì²˜ë¦¬ ì‹œì‘: url={url}")

    base_menu = (menu or "ìƒí’ˆ^WhyGenieTV^ì±„ë„ í¸ì„±í‘œ").strip()
    menus: List[Dict[str, Any]] = []
    datas: List[Dict[str, Any]] = []

    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Referer": url
    })

    try:
        response = await asyncio.to_thread(session.get, url, timeout=30)
    except Exception as e:
        session.close()
        logging.error(f"âŒ ì§€ë‹ˆ TV ì±„ë„ í¸ì„±í‘œ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return {
            "menus": [],
            "datas": [],
            "total_processed": 0,
            "status": "failed",
            "message": f"ì§€ë‹ˆ TV ì±„ë„ í¸ì„±í‘œ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}"
        }

    status_code = getattr(response, "status_code", None)
    if not response or not getattr(response, "text", ""):
        session.close()
        logging.error("âŒ ì§€ë‹ˆ TV ì±„ë„ í¸ì„±í‘œ ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return {
            "menus": [],
            "datas": [],
            "total_processed": 0,
            "status": "failed",
            "status_code": status_code,
            "message": "ì§€ë‹ˆ TV ì±„ë„ í¸ì„±í‘œ ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."
        }

    response.encoding = "euc-kr"
    soup = BeautifulSoup(response.text, "html.parser")

    channel_guide_el = soup.select_one("div.channel_guide")
    noti_desc_el = soup.select_one("div.noti_desc")

    def normalize_multiline(text: str) -> str:
        if not text:
            return ""
        # ì—°ì† ê³µë°±ì„ ì¤„ì´ê³ , ì¤„ë°”ê¿ˆ ì‚¬ì´ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        lines = [line.strip() for line in text.splitlines()]
        cleaned = "\n".join(line for line in lines if line)
        return cleaned.strip()

    channel_guide_text = normalize_multiline(channel_guide_el.get_text("\n", strip=True)) if channel_guide_el else ""
    noti_desc_text = normalize_multiline(noti_desc_el.get_text("\n", strip=True)) if noti_desc_el else ""
    channel_guide_html = str(channel_guide_el) if channel_guide_el else ""
    noti_desc_html = str(noti_desc_el) if noti_desc_el else ""

    super_tab_pattern = re.compile(r"fnSearchChannel\((?P<ch_type>[^,]+),'(?P<prod>[^']*)',\s*(?P<mid>[^)]+)\)")
    plan_pattern = re.compile(r"fnSearchChannelNoSubmit\('(?P<ch_type>[^']*)','(?P<product_cd>[^']*)',\s*(?P<mid>[^)]+)\)")

    super_tabs: List[Dict[str, Any]] = []
    for anchor in soup.select(".channel_content .sub-tabs-1st .sub-trigger"):
        tab_name = (anchor.get_text(" ", strip=True) or "").replace("\xa0", " ").strip()
        href = (anchor.get("href") or "").strip()
        if not tab_name or not href:
            continue
        match = super_tab_pattern.search(anchor.get("onclick") or "")
        if not match:
            continue
        ch_type = match.group("ch_type").strip() or "3"
        target = soup.select_one(href)
        if not target:
            continue
        plan_ul = target.select_one("ul.channel_select")
        if not plan_ul:
            continue
        super_tabs.append({
            "name": tab_name,
            "ch_type": ch_type,
            "plan_ul": plan_ul
        })

    if not super_tabs:
        # fallback: ê¸°ì¡´ ë°©ì‹ (ì§€ë‹ˆ TV ê¸°ë³¸ íƒ­ë§Œ)
        plan_container = soup.select_one("div#trigger2-1-1 ul.channel_select.tv_live") or soup.select_one("ul.channel_select.tv_live")
        if plan_container:
            super_tabs.append({
                "name": "ì§€ë‹ˆ TV",
                "ch_type": "3",
                "plan_ul": plan_container
            })

    if not super_tabs:
        session.close()
        logging.error("âŒ ì§€ë‹ˆ TV íƒ­ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {
            "menus": [],
            "datas": [],
            "total_processed": 0,
            "status": "failed",
            "status_code": status_code,
            "message": "ì§€ë‹ˆ TV íƒ­ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }

    channel_cache: Dict[Tuple[str, str, str], Tuple[int, str]] = {}

    def parse_channel_html(html_text: str) -> List[Dict[str, str]]:
        if not html_text:
            return []
        inner_soup = BeautifulSoup(html_text, "html.parser")
        channels: List[Dict[str, str]] = []
        for anchor in inner_soup.select("ul.channel li a"):
            span = anchor.select_one("span.ch")
            if not span:
                continue

            text_parts: List[str] = []
            for node in span.contents:
                if isinstance(node, NavigableString):
                    value = str(node).strip()
                    if value:
                        text_parts.append(value)

            channel_text = " ".join(text_parts).replace("\xa0", " ")
            channel_text = re.sub(r"\s+", " ", channel_text).strip()
            if not channel_text:
                continue

            channel_text = html.unescape(unquote(channel_text))

            number = channel_text
            name = ""
            number_match = re.match(r"^(\S+)\s+(.*)$", channel_text)
            if number_match:
                number = number_match.group(1).strip()
                name = number_match.group(2).strip()

            alt_text = html.unescape(unquote((anchor.get("alt") or "").strip()))

            channels.append({
                "channel_number": number,
                "channel_name": name,
                "note": alt_text
            })
        return channels

    async def fetch_channels(ch_type: str, product_cd: str, parent_menu_id: str) -> Tuple[int, List[Dict[str, str]]]:
        cache_key = (ch_type, product_cd or "", parent_menu_id or "0")
        if cache_key in channel_cache:
            cached_status, cached_html = channel_cache[cache_key]
            return cached_status, parse_channel_html(cached_html)

        data = {
            "ch_type": ch_type,
            "parent_menu_id": parent_menu_id or "0",
            "product_cd": product_cd or "",
            "option_cd_list": ""
        }

        try:
            resp = await asyncio.to_thread(session.post, "https://tv.kt.com/tv/channel/pChList.asp", data=data, timeout=30)
        except Exception as e:
            logging.error(f"âŒ ì±„ë„ ëª©ë¡ ìš”ì²­ ì‹¤íŒ¨ (product_cd={product_cd}): {e}")
            return None, []

        resp.encoding = "euc-kr"
        channel_cache[cache_key] = (resp.status_code, resp.text)
        return resp.status_code, parse_channel_html(resp.text)

    def escape_md(value: str) -> str:
        if not value:
            return ""
        return value.replace("|", "\\|")

    total_plans_processed = 0

    for super_tab in super_tabs:
        super_name = super_tab["name"]
        super_ch_type = super_tab["ch_type"]
        plan_ul = super_tab["plan_ul"]

        seen_codes: Set[str] = set()
        plan_entries: List[Dict[str, str]] = []

        for anchor in plan_ul.select("li a"):
            onclick = anchor.get("onclick") or ""
            match = plan_pattern.search(onclick)
            if not match:
                continue

            product_cd = match.group("product_cd").strip()
            parent_menu_id = match.group("mid").strip().strip(";") or "0"

            span = anchor.select_one("span")
            raw_title = (span.get_text(" ", strip=True) if span else "").replace("\xa0", " ").strip()
            clean_title = re.sub(r"\([^)]*\)", "", raw_title).strip()

            if not raw_title or not product_cd:
                continue
            if not clean_title or clean_title in ("ì „ì²´",):
                continue
            if "ì„ íƒí˜•" in clean_title:
                continue
            if product_cd in seen_codes:
                continue

            seen_codes.add(product_cd)
            plan_entries.append({
                "title": clean_title,
                "raw_title": raw_title,
                "ch_type": super_ch_type,
                "product_cd": product_cd,
                "parent_menu_id": parent_menu_id
            })

        if not plan_entries:
            logging.warning(f"âš ï¸ '{super_name}' í”Œëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            continue

        for plan in plan_entries:
            plan_title = plan["title"]
            plan_code = plan["product_cd"]
            plan_ch_type = plan["ch_type"]
            parent_menu_id = plan["parent_menu_id"]

            channel_status, channels = await fetch_channels(plan_ch_type, plan_code, parent_menu_id)
            channel_count = len(channels)

            markdown_lines = [
                "| ì±„ë„ ë²ˆí˜¸ | ì±„ë„ëª… | ë¹„ê³  |",
                "| --- | --- | --- |"
            ]
            for channel in channels:
                markdown_lines.append(
                    f"| {escape_md(channel['channel_number'])} | {escape_md(channel['channel_name'])} | {escape_md(channel['note'])} |"
                )
            markdown_table = "\n".join(markdown_lines)

            markdown_sections: List[str] = []
            markdown_sections.append(f"# {super_name} - {plan_title}")
            markdown_sections.append(markdown_table)
            if channel_guide_text:
                markdown_sections.append(channel_guide_text)
            if noti_desc_text:
                markdown_sections.append(noti_desc_text)
            full_markdown = "\n\n".join(markdown_sections)

            menu_path = f"{base_menu}^{super_name}^{plan_title}" if base_menu else f"{super_name}^{plan_title}"
            menus.append({
                "menu": menu_path,
                "url": url
            })
            datas.append({
                "menu": menu_path,
                "title": plan_title,
                "parent_tab": super_name,
                "url": url,
                "plan_code": plan_code,
                "ch_type": plan_ch_type,
                "parent_menu_id": parent_menu_id,
                "channel_count": channel_count,
                "channels": channels,
                "channel_guide_text": channel_guide_text,
                "channel_guide_html": channel_guide_html,
                "noti_desc_text": noti_desc_text,
                "noti_desc_html": noti_desc_html,
                "markdown": full_markdown,
                "status_code": channel_status
            })

            total_plans_processed += 1
            logging.info(f"âœ… ì§€ë‹ˆ TV ì±„ë„ í”Œëœ ì²˜ë¦¬ ì™„ë£Œ: parent='{super_name}', plan='{plan_title}', channel_count={channel_count}")

    session.close()

    return {
        "menus": menus,
        "datas": datas,
        "total_processed": total_plans_processed,
        "status": "completed",
        "status_code": status_code,
        "message": f"ì§€ë‹ˆ TV ì±„ë„ í¸ì„±í‘œ í”Œëœ {total_plans_processed}ê±´ ì²˜ë¦¬ ì™„ë£Œ"
    }

register_page_handler(
    r'https?://tv\.kt\.com/tv/channel/pChInfo\.asp.*',
    handle_whygenietv_channel_schedule
)

# =========================
# ë‹¹ì²¨ìë°œí‘œ ì²˜ë¦¬ í•¸ë“¤ëŸ¬
# =========================
async def handle_event_winner_announcements(url: str, fclient, menu=None) -> dict:
    """
    KT Shop ë‹¹ì²¨ìë°œí‘œ í˜ì´ì§€ ì²˜ë¦¬
    
    Args:
        url: ë‹¹ì²¨ìë°œí‘œ ëª©ë¡ í˜ì´ì§€ URL
        fclient: Firecrawl í´ë¼ì´ì–¸íŠ¸
        menu: ë©”ë‰´ ì •ë³´
    
    Returns:
        dict: ì¶”ì¶œëœ ë°ì´í„°
    """
    try:
        logging.info(f"ğŸ¯ ë‹¹ì²¨ìë°œí‘œ í˜ì´ì§€ ì²˜ë¦¬ ì‹œì‘: {url}")
        
        # Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ ì ‘ê·¼
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context()
            page = await context.new_page()
            
            # í˜ì´ì§€ ë¡œë“œ
            await page.goto(url, wait_until='domcontentloaded', timeout=60000)
            await page.wait_for_timeout(3000)  # ì¶”ê°€ ë¡œë”© ëŒ€ê¸°
            
            # iframeìœ¼ë¡œ ì´ë™
            iframe = await page.query_selector('iframe[src*="planDispEvent.do"]')
            if not iframe:
                raise Exception("ë‹¹ì²¨ìë°œí‘œ iframeì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            frame = await iframe.content_frame()
            if not frame:
                raise Exception("iframe ë‚´ë¶€ë¡œ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # 1ë‹¨ê³„: í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•´ ëª¨ë“  ê²Œì‹œë¬¼ ë§í¬ ìˆ˜ì§‘
            all_posts = []
            current_page = 1
            max_pages = 20  # ì•ˆì „ì¥ì¹˜
            no_new_posts_count = 0  # ì—°ì†ìœ¼ë¡œ ìƒˆ ê²Œì‹œë¬¼ì´ ì—†ëŠ” íšŸìˆ˜
            
            while current_page <= max_pages:
                logging.info(f"ğŸ“„ í˜ì´ì§€ {current_page} ì²˜ë¦¬ ì¤‘...")
                
                # í˜„ì¬ í˜ì´ì§€ì˜ ê²Œì‹œë¬¼ë“¤ ìˆ˜ì§‘ (iframe ë‚´ë¶€ì—ì„œ)
                page_posts = await frame.evaluate("""
                    () => {
                        const allTable = document.querySelector('#tabCont01 table.board_list');
                        if (!allTable) return [];
                        
                        const rows = allTable.querySelectorAll('tbody tr');
                        return Array.from(rows).map((row, index) => {
                            const cells = row.querySelectorAll('td');
                            const link = row.querySelector('a');
                            
                            if (!link || !link.onclick) return null;
                            
                            // onclickì—ì„œ ID ì¶”ì¶œ
                            const onclickStr = link.onclick.toString();
                            const eventListViewMatch = onclickStr.match(/eventListView\\((\\d+),'(\\d+)','(\\d+)'\\)/);
                            
                            if (!eventListViewMatch) return null;
                            
                            // ì´ë²¤íŠ¸ ê¸°ê°„ì„ startdate, enddateë¡œ ë¶„ë¦¬
                            const periodText = cells[2]?.textContent?.trim() || '';
                            const periodMatch = periodText.match(/(\\d{4})\\.(\\d{2})\\.(\\d{2})\\s*~\\s*(\\d{4})\\.(\\d{2})\\.(\\d{2})/);
                            const startdate = periodMatch ? `${periodMatch[1]}-${periodMatch[2]}-${periodMatch[3]}` : '';
                            const enddate = periodMatch ? `${periodMatch[4]}-${periodMatch[5]}-${periodMatch[6]}` : '';
                            
                            return {
                                index: index + 1,
                                number: cells[0]?.textContent?.trim() || '',
                                eventName: cells[1]?.textContent?.trim() || '',
                                period: periodText,
                                startdate: startdate,
                                enddate: enddate,
                                announcementDate: cells[3]?.textContent?.trim() || '',
                                eventId1: eventListViewMatch[1],
                                eventId2: eventListViewMatch[2],
                                eventId3: eventListViewMatch[3],
                                uniqueId: `${eventListViewMatch[1]}_${eventListViewMatch[3]}`,
                                filePath: `Shop^í•«ë”œ/ê¸°íšì „^ê¸°íšì „^ë‹¹ì²¨ìë°œí‘œ^${cells[1]?.textContent?.trim() || ''}`
                            };
                        }).filter(post => post !== null);
                    }
                """)
                
                if not page_posts:
                    logging.info(f"ğŸ“„ í˜ì´ì§€ {current_page}ì—ì„œ ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ìˆ˜ì§‘ ì™„ë£Œ.")
                    break
                
                # ì¤‘ë³µ ê²Œì‹œë¬¼ ì²´í¬ (ê°™ì€ uniqueIdê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸)
                new_posts = []
                existing_ids = {post['uniqueId'] for post in all_posts}
                
                for post in page_posts:
                    if post['uniqueId'] not in existing_ids:
                        new_posts.append(post)
                
                if not new_posts:
                    no_new_posts_count += 1
                    logging.info(f"ğŸ“„ í˜ì´ì§€ {current_page}: ìƒˆë¡œìš´ ê²Œì‹œë¬¼ ì—†ìŒ ({no_new_posts_count}/3)")
                    
                    if no_new_posts_count >= 3:  # ì—°ì† 3ë²ˆ ìƒˆ ê²Œì‹œë¬¼ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
                        logging.info("ğŸ“„ ì—°ì†ìœ¼ë¡œ ìƒˆë¡œìš´ ê²Œì‹œë¬¼ì´ ì—†ì–´ ìˆ˜ì§‘ ì™„ë£Œ.")
                        break
                else:
                    no_new_posts_count = 0  # ìƒˆ ê²Œì‹œë¬¼ì´ ìˆìœ¼ë©´ ì¹´ìš´í„° ë¦¬ì…‹
                
                all_posts.extend(new_posts)
                logging.info(f"ğŸ“„ í˜ì´ì§€ {current_page}: {len(new_posts)}ê°œ ìƒˆ ê²Œì‹œë¬¼ ìˆ˜ì§‘ (ì´ {len(all_posts)}ê°œ)")
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ ì‹œë„ (allListClick í•¨ìˆ˜ ì‚¬ìš©)
                try:
                    next_page = current_page + 1
                    await frame.evaluate(f"allListClick({next_page})")
                    await page.wait_for_timeout(2000)
                    current_page = next_page
                except Exception as e:
                    logging.info(f"ğŸ“„ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}. ìˆ˜ì§‘ ì™„ë£Œ.")
                    break
            
            await browser.close()
            
            logging.info(f"âœ… ì´ {len(all_posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì™„ë£Œ")
            
            # 2ë‹¨ê³„: ë³‘ë ¬ë¡œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            from markdownify import markdownify as md
            
            menus = []
            datas = []
            
            # ë©”ì¸ í˜ì´ì§€ë„ ì €ì¥
            base_menu = (menu or '').strip()
            menus.append({'menu': base_menu, 'url': url, 'murl': to_mshop_url(url)})
            datas.append({
                'url': url,
                'murl': to_mshop_url(url),
                'title': 'ë‹¹ì²¨ìë°œí‘œ ëª©ë¡',
                'markdown': f"# ë‹¹ì²¨ìë°œí‘œ ëª©ë¡\n\nì´ {len(all_posts)}ê°œ ì´ë²¤íŠ¸ ë‹¹ì²¨ìë°œí‘œ",
                'html': f"<h1>ë‹¹ì²¨ìë°œí‘œ ëª©ë¡</h1><p>ì´ {len(all_posts)}ê°œ ì´ë²¤íŠ¸ ë‹¹ì²¨ìë°œí‘œ</p>",
                'special_processed': True,
                'playwright_processed': True
            })
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„¸ë§ˆí¬ì–´ (ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ)
            semaphore = asyncio.Semaphore(5)  # ìµœëŒ€ 5ê°œ ë™ì‹œ ìš”ì²­
            
            async def extract_post_detail(post):
                async with semaphore:
                    try:
                        logging.info(f"ğŸ” ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì¤‘: {post['eventName']}")
                        
                        # ìƒˆë¡œìš´ ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤ë¡œ ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼
                        async with async_playwright() as p:
                            browser = await p.chromium.launch(headless=True)
                            context = await browser.new_context()
                            detail_page = await context.new_page()
                            
                            # ëª©ë¡ í˜ì´ì§€ë¡œ ì´ë™
                            await detail_page.goto("https://shop.kt.com/plan/planDispEvent.do", wait_until='networkidle', timeout=60000)
                            
                            # eventListView í•¨ìˆ˜ ì‹¤í–‰í•˜ì—¬ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
                            await detail_page.evaluate(f"""
                                eventListView({post['eventId1']}, '{post['eventId2']}', '{post['eventId3']}');
                            """)
                            
                            await detail_page.wait_for_timeout(3000)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                            
                            # ìƒì„¸ HTML ì¶”ì¶œ
                            detail_html = await detail_page.evaluate("""
                                () => {
                                    // ìƒì„¸ ì˜ì—­ ì¶”ì¶œ
                                    const boardView = document.querySelector('table.board_view');
                                    if (boardView) {
                                        return boardView.outerHTML;
                                    }
                                    
                                    // board_viewê°€ ì—†ìœ¼ë©´ ë³¸ë¬¸ ì „ì²´
                                    const content = document.querySelector('.content, #content, .board_content');
                                    if (content) {
                                        return content.outerHTML;
                                    }
                                    
                                    return document.body ? document.body.innerHTML : '';
                                }
                            """)
                            
                            await browser.close()
                            
                            # ë§ˆí¬ë‹¤ìš´ ë³€í™˜
                            detail_markdown = md(detail_html) if detail_html else ''
                            
                            # ìƒì„¸ URL êµ¬ì„±
                            detail_url = f"https://shop.kt.com/plan/planDispEvent.do?eventId={post['eventId1']}&eventId2={post['eventId2']}&eventId3={post['eventId3']}"
                            
                            return {
                                'success': True,
                                'post': post,
                                'html': detail_html,
                                'markdown': detail_markdown,
                                'url': detail_url
                            }
                            
                    except Exception as e:
                        logging.error(f"âŒ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ ({post['eventName']}): {e}")
                        return {
                            'success': False,
                            'post': post,
                            'error': str(e)
                        }
            
            # ë³‘ë ¬ë¡œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            detailed_results = await asyncio.gather(*[extract_post_detail(post) for post in all_posts])
            
            # 3ë‹¨ê³„: ê²°ê³¼ë¥¼ menusì™€ datasì— ì¶”ê°€
            success_count = 0
            for result in detailed_results:
                post = result.get('post', {})
                event_name = post.get('eventName', 'ì•Œ ìˆ˜ ì—†ìŒ')
                
                # ë©”ë‰´ êµ¬ì„±
                menu_name = f"{base_menu}^{event_name}" if base_menu else f"ë‹¹ì²¨ìë°œí‘œ^{event_name}"
                detail_url = result.get('url', url)
                
                menus.append({
                    'menu': menu_name,
                    'url': detail_url,
                    'murl': to_mshop_url(detail_url)
                })
                
                if result.get('success'):
                    # ì„±ê³µí•œ ê²½ìš°
                    markdown_content = f"# {event_name}\n\n"
                    markdown_content += f"**ì´ë²¤íŠ¸ ê¸°ê°„**: {post.get('period', '')}\n\n"
                    markdown_content += f"**ë‹¹ì²¨ì ë°œí‘œì¼**: {post.get('announcementDate', '')}\n\n"
                    markdown_content += "---\n\n"
                    markdown_content += result.get('markdown', '')
                    
                    datas.append({
                        'url': detail_url,
                        'murl': to_mshop_url(detail_url),
                        'title': event_name,
                        'markdown': markdown_content,
                        'html': result.get('html', ''),
                        'startdate': post.get('startdate', ''),
                        'enddate': post.get('enddate', ''),
                        'special_processed': True,
                        'playwright_processed': True
                    })
                    success_count += 1
                else:
                    # ì‹¤íŒ¨í•œ ê²½ìš°
                    datas.append({
                        'url': detail_url,
                        'murl': to_mshop_url(detail_url),
                        'title': event_name,
                        'markdown': f"# {event_name}\n\nìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}",
                        'html': f"<h1>{event_name}</h1><p>ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨</p>",
                        'startdate': post.get('startdate', ''),
                        'enddate': post.get('enddate', ''),
                        'special_processed': True,
                        'playwright_processed': True,
                        'error': result.get('error', '')
                    })
            
            logging.info(f"âœ… ë‹¹ì²¨ìë°œí‘œ ì²˜ë¦¬ ì™„ë£Œ: {success_count}/{len(all_posts)}ê°œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì„±ê³µ")
            
            return {
                'menus': menus,
                'datas': datas,
                'total_processed': len(datas),
                'status': 'completed'
            }
            
    except Exception as e:
        logging.error(f"âŒ ë‹¹ì²¨ìë°œí‘œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        base_menu = (menu or '').strip()
        return {
            'menus': [{'menu': base_menu, 'url': url, 'murl': to_mshop_url(url)}],
            'datas': [{
                'url': url,
                'murl': to_mshop_url(url),
                'title': 'ë‹¹ì²¨ìë°œí‘œ',
                'markdown': f"# ë‹¹ì²¨ìë°œí‘œ ì²˜ë¦¬ ì‹¤íŒ¨\n\nì˜¤ë¥˜: {str(e)}",
                'html': f"<h1>ë‹¹ì²¨ìë°œí‘œ ì²˜ë¦¬ ì‹¤íŒ¨</h1><p>ì˜¤ë¥˜: {str(e)}</p>",
                'error': str(e),
                'special_processed': True,
                'playwright_processed': True
            }],
            'total_processed': 0,
            'status': 'failed',
            'error': str(e)
        }

# ë‹¹ì²¨ìë°œí‘œ í•¸ë“¤ëŸ¬ ë“±ë¡
register_page_handler(
    r'https?://shop\.kt\.com/display/olhsStore\.do\?dispNo=STOR05&subDispNo=STOR0506.*',
    handle_event_winner_announcements
)

# =========================
# Webzine ë¦¬ìŠ¤íŠ¸ í•¸ë“¤ëŸ¬ (webzineList.do)
# =========================
async def handle_webzine_list(url: str, fclient, menu=None) -> dict:
    """
    Webzine ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ì²˜ë¦¬ í•¸ë“¤ëŸ¬
    - ë¦¬ìŠ¤íŠ¸ì—ì„œ ul.webzine_list ì•„ë˜ì˜ a href ìˆ˜ì§‘
    - ê° ìƒì„¸ í˜ì´ì§€ì—ì„œ div.webzine_content ì¶”ì¶œ
    """
    from markdownify import markdownify as md
    logging.info(f"Webzine ë¦¬ìŠ¤íŠ¸ í•¸ë“¤ëŸ¬ ì§„ì…: url={url}, menu={menu}")
    menus, datas = [], []
    base_menu = (menu or '').strip()
    base_title = base_menu.split('^')[-1].strip() if base_menu else 'Webzine ë¦¬ìŠ¤íŠ¸'
    base_title = sanitize_filename(base_title)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        )
        page = await context.new_page()
        response = await page.goto(url, wait_until='domcontentloaded', timeout=60000)
        await page.wait_for_timeout(2500)

        try:
            await page.wait_for_selector('ul.webzine_list', timeout=20000)
        except Exception:
            logging.warning(
                "âš ï¸ Webzine ë¦¬ìŠ¤íŠ¸: ul.webzine_list ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            )

        status_code = response.status if response else None
        if status_code:
            if status_code >= 400:
                logging.error(f"âŒ Webzine ë¦¬ìŠ¤íŠ¸ ({url}): HTTP {status_code} ì˜¤ë¥˜")
            elif status_code >= 300:
                logging.warning(f"âš ï¸ Webzine ë¦¬ìŠ¤íŠ¸ ({url}): HTTP {status_code} ë¦¬ë‹¤ì´ë ‰íŠ¸")
            else:
                logging.info(f"âœ… Webzine ë¦¬ìŠ¤íŠ¸ ({url}): HTTP {status_code} ì„±ê³µ")
        else:
            logging.debug(f"ğŸ” Webzine ë¦¬ìŠ¤íŠ¸ ({url}): ìƒíƒœ ì½”ë“œ ì •ë³´ ì—†ìŒ")

        # ë©”ì¸ í˜ì´ì§€ ì½˜í…ì¸  ì¶”ì¶œ (ë©”ë‰´ ê¸°ë³¸ í˜ì´ì§€ ì €ì¥)
        try:
            main_html = await page.evaluate("""
                () => {
                    const selectors = ['ul.webzine_list', '.webzine_list', '#cfmClContents'];
                    for (const sel of selectors) {
                        const el = document.querySelector(sel);
                        if (el && el.innerHTML && el.innerHTML.trim().length > 0) {
                            return el.innerHTML;
                        }
                    }
                    return document.body ? document.body.innerHTML : '';
                }
            """)
        except Exception:
            logging.warning("âš ï¸ Webzine ë¦¬ìŠ¤íŠ¸: ë©”ì¸ ì˜ì—­ HTML ì¶”ì¶œ ì‹¤íŒ¨, body ì „ì²´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            main_html = await page.content()

        if main_html:
            main_markdown = md(main_html)
            if base_menu:
                menus.append({'menu': base_menu, 'url': url, 'murl': to_mshop_url(url)})
            datas.append({
                'url': url,
                'title': base_title,
                'markdown': main_markdown,
                'html': main_html,
                'special_processed': True,
                'playwright_processed': True,
                'murl': to_mshop_url(url)
            })

        # 1) ë¦¬ìŠ¤íŠ¸ì—ì„œ a href ìˆ˜ì§‘
        webzine_items = await page.evaluate("""
            () => {
                const results = [];
                const webzineList = document.querySelector('ul.webzine_list');
                if (!webzineList) {
                    return results;
                }
                
                // ul.webzine_list ì•„ë˜ì˜ ëª¨ë“  a íƒœê·¸ ìˆ˜ì§‘
                const links = webzineList.querySelectorAll('a[href]');
                links.forEach(a => {
                    const href = a.getAttribute('href') || '';
                    const fullText = (a.textContent || '').trim();
                    
                    if (href) {
                        try {
                            // ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                            const absUrl = new URL(href, location.href).href;
                            
                            // í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œì™€ ì¹´í…Œê³ ë¦¬ ì œê±°, ì œëª©ë§Œ ì¶”ì¶œ
                            let cleanTitle = fullText;
                            
                            // ë‚ ì§œ íŒ¨í„´ ì œê±° (YYYY. MM ë˜ëŠ” YYYY.MM)
                            cleanTitle = cleanTitle.replace(/^\\d{4}\\.\\s*\\d{1,2}\\s*/, '');
                            
                            // ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ë§ˆì§€ë§‰ ì¤„(ì¹´í…Œê³ ë¦¬) ì œê±°
                            const lines = cleanTitle.split(/\\n/).map(l => l.trim()).filter(l => l.length > 0);
                            if (lines.length > 1) {
                                // ì²« ë²ˆì§¸ ì¤„ì´ ì œëª©, ë§ˆì§€ë§‰ ì¤„ì´ ì¹´í…Œê³ ë¦¬ë¡œ ì¶”ì •
                                cleanTitle = lines[0];
                            } else if (lines.length === 1) {
                                cleanTitle = lines[0];
                            }
                            
                            // ì—°ì†ëœ ê³µë°± ì œê±°
                            cleanTitle = cleanTitle.replace(/\\s+/g, ' ').trim();
                            
                            // ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (YYYY. MM í˜•ì‹)
                            const dateMatch = fullText.match(/(\\d{4})\\.\\s*(\\d{1,2})/);
                            let year = null;
                            let month = null;
                            if (dateMatch) {
                                year = dateMatch[1];
                                month = dateMatch[2];
                            }
                            
                            results.push({
                                url: absUrl,
                                title: cleanTitle || 'ì œëª© ì—†ìŒ',
                                fullText: fullText,
                                year: year,
                                month: month
                            });
                        } catch(e) {
                            // URL ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ href ì‚¬ìš©
                            results.push({
                                url: href,
                                title: fullText || 'ì œëª© ì—†ìŒ',
                                fullText: fullText,
                                year: null,
                                month: null
                            });
                        }
                    }
                });
                
                return results;
            }
        """)

        logging.info(f"Webzine ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘: {len(webzine_items)}ê°œ í•­ëª©")

        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        normalized = []
        for item in webzine_items:
            item_url = item.get('url', '').strip()
            if item_url and item_url not in seen_urls:
                seen_urls.add(item_url)
                
                # URL íŒŒë¼ë¯¸í„°ì—ì„œ year, month ì¶”ì¶œ ì‹œë„
                try:
                    from urllib.parse import urlparse, parse_qs
                    parsed = urlparse(item_url)
                    params = parse_qs(parsed.query)
                    
                    # URL íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                    url_year = params.get('year', [None])[0]
                    url_month = params.get('month', [None])[0]
                    
                    if url_year:
                        # year=24 -> 2024ë¡œ ë³€í™˜
                        year_val = int(url_year)
                        if year_val < 100:
                            year_val = 2000 + year_val
                        item['year'] = str(year_val)
                    if url_month:
                        item['month'] = url_month.zfill(2)
                except Exception:
                    pass
                
                normalized.append({
                    'url': item_url,
                    'title': item.get('title', '').strip() or 'ì œëª© ì—†ìŒ',
                    'year': item.get('year'),
                    'month': item.get('month')
                })

        logging.info(f"ì •ê·œí™”ëœ Webzine í•­ëª©: {len(normalized)}ê°œ")

        # 2) ê° ìƒì„¸ í˜ì´ì§€ì—ì„œ div.webzine_content ì¶”ì¶œ
        for idx, item in enumerate(normalized, 1):
            try:
                logging.info(f"[{idx}/{len(normalized)}] ìƒì„¸ ì¶”ì¶œ: {item['title']}")
                
                # ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
                try:
                    await page.goto(item['url'], wait_until='domcontentloaded', timeout=60000)
                    await page.wait_for_timeout(1200)
                except Exception as _e:
                    logging.warning(f"ìƒì„¸ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨(ë¬´ì‹œí•˜ê³  ê³„ì†): {item['url']} -> {_e}")
                    continue

                # div.webzine_content ì¶”ì¶œ
                detail_html = await page.evaluate("""
                    () => {
                        const contentEl = document.querySelector('div.webzine_content');
                        if (contentEl && contentEl.innerHTML && contentEl.innerHTML.trim().length > 0) {
                            return contentEl.innerHTML;
                        }
                        
                        // í´ë°±: ì „ì²´ ì»¨í…ì¸  ì˜ì—­ ì¶”ì¶œ
                        const fallbackSelectors = ['#cfmClContents', '.content', '.main-content', 'main'];
                        for (const sel of fallbackSelectors) {
                            const el = document.querySelector(sel);
                            if (el && el.innerHTML && el.innerHTML.trim().length > 0) {
                                return el.innerHTML;
                            }
                        }
                        
                        return document.body ? document.body.innerHTML : '';
                    }
                """)

                if not detail_html or len(detail_html.strip()) < 50:
                    logging.warning(f"âš ï¸ {item['title']}: ì»¨í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë¹„ì–´ìˆìŒ")
                    continue

                md_content = md(detail_html)
                
                # ë‚ ì§œ íŒŒì‹± (startdate ì„¤ì •)
                startdate = "1900-01-01"
                if item.get('year') and item.get('month'):
                    year = item['year']
                    month = item['month'].zfill(2)
                    startdate = f"{year}-{month}-01"
                
                # ë©”ë‰´ëª… êµ¬ì„± (ì œëª©ë§Œ ì‚¬ìš©)
                menu_name = f"{base_menu}^{item['title']}" if base_menu else f"Shop^{item['title']}"
                menus.append({ 'menu': menu_name, 'url': item['url'], 'murl': to_mshop_url(item['url']) })
                datas.append({
                    'url': item['url'],
                    'title': item['title'],
                    'markdown': md_content,
                    'html': detail_html,
                    'startdate': startdate,
                    'enddate': '2999-12-31',
                    'special_processed': True,
                    'playwright_processed': True,
                    'murl': to_mshop_url(item['url'])
                })
            except Exception as e:
                logging.warning(f"ìƒì„¸ ì¶”ì¶œ ì‹¤íŒ¨: {item.get('title', 'unknown')}: {str(e)}")
                continue

        await browser.close()

    return {
        'menus': menus,
        'datas': datas,
        'total_processed': len(datas),
        'status': 'completed',
        'message': f"ì´ {len(datas)}ê°œ Webzine í•­ëª© ì²˜ë¦¬ ì™„ë£Œ"
    }

register_page_handler(
    r'https?://shop\.kt\.com/unify/webzineList\.do.*',
    handle_webzine_list
)